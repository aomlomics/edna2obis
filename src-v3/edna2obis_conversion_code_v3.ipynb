{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Darwin Core Conversion of eDNA Sequence Data From the FAIRe (NOAA Version) metadata template \n",
    "\n",
    "**Version:** 3.0\n",
    "\n",
    "**Authors:** Katherine Silliman, Bayden Willms\n",
    "\n",
    "**Last Updated:** 9-June-2025\n",
    "\n",
    "This notebook is for converting a [FAIR-eDNA](https://fair-edna.github.io/index.html)-based data sheet to DarwinCore for submission to OBIS. It has been testing on a Windows 11 laptop, with Python 3.11. \n",
    "\n",
    "To generate the input files for edna2obis, please run [FAIRe2NODE](https://github.com/aomlomics/FAIReSheets/tree/FAIRe2NODE) to generate your own FAIR-eDNA (NOAA) template. Once you've filled in your data, you are ready to begin.\n",
    "\n",
    "This newest version of edna2obis takes the same input files as the ODE, the [Ocean DNA Explorer](https://www.oceandnaexplorer.org/). Explore your data (publically or privately) with visualizations, API capabilities, and more through ODE. \n",
    "\n",
    "[FAIR-eDNA NOAA Google Sheet](https://docs.google.com/spreadsheets/d/1mkjfUQW3gTn3ezhMQmFDQn4EBoQ2Xv4SZeSd9sqagoU/edit?gid=0#gid=0)\n",
    "\n",
    "**Requirements:**\n",
    "- Python 3\n",
    "- Python 3 packages:\n",
    "    - os\n",
    "- External packages:\n",
    "    - Bio.Entrez from biopython\n",
    "    - numpy\n",
    "    - pandas\n",
    "    - openpyxl\n",
    "    - pyworms\n",
    "    - multiprocess\n",
    "- Custom modules:\n",
    "    - WoRMS_matching\n",
    "    - analysis_helpers\n",
    "\n",
    "**Resources:**\n",
    "- Abarenkov K, Andersson AF, Bissett A, Finstad AG, Fossøy F, Grosjean M, Hope M, Jeppesen TS, Kõljalg U, Lundin D, Nilsson RN, Prager M, Provoost P, Schigel D, Suominen S, Svenningsen C & Frøslev TG (2023) Publishing DNA-derived data through biodiversity data platforms, v1.3. Copenhagen: GBIF Secretariat. https://doi.org/10.35035/doc-vf1a-nr22.https://doi.org/10.35035/doc-vf1a-nr22.\n",
    "- [OBIS manual](https://manual.obis.org/dna_data.html)\n",
    "- [TDWG Darwin Core Occurrence Core](https://dwc.tdwg.org/terms/#occurrence)\n",
    "- [GBIF DNA Derived Data Extension](https://tools.gbif.org/dwca-validator/extension.do?id=http://rs.gbif.org/terms/1.0/DNADerivedData)\n",
    "- https://github.com/iobis/dataset-edna\n",
    "\n",
    "**Citation**  \n",
    "Silliman K, Anderson S, Storo R, Thompson L (2023) A Case Study in Sharing Marine eDNA Metabarcoding Data to OBIS. Biodiversity Information Science and Standards 7: e111048. https://doi.org/10.3897/biss.7.111048\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation  \n",
    "\n",
    "```bash\n",
    "conda create -n edna2obis\n",
    "conda activate edna2obis\n",
    "conda install -c conda-forge notebook\n",
    "conda install -c conda-forge nb_conda_kernels\n",
    "\n",
    "conda install -c conda-forge numpy pandas\n",
    "conda install -c conda-forge openpyxl\n",
    "\n",
    "#worms conversion\n",
    "conda install -c conda-forge pyworms\n",
    "conda install -c conda-forge multiprocess\n",
    "conda install -c conda-forge biopython\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import WoRMS_v3_matching # new custom functions for querying WoRMS API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jupyter notebook parameters\n",
    "pd.set_option('display.max_colwidth', 150)\n",
    "pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in a Jupyter Notebook, the current directory is always where the .ipynb file is being run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prepare Input Data \n",
    "\n",
    "**Project data and metadata**  \n",
    "This workflow assumes that you have your project metadata in an Excel sheet formatted like the FAIR-eDNA template located **TODO NEED TO ADD CORRECT LINK**[here](https://docs.google.com/spreadsheets/d/1YBXFU9PuMqm7IT1tp0LTxQ1v2j0tlCWFnhSpy-EBwPw/edit?usp=drive_link). Instructions for filling out the metadata template are located in the 'README' sheet and at the [documentation website](https://noaa-omics-templates.readthedocs.io/en/latest/).\n",
    "\n",
    "**eDNA and taxonomy data**  \n",
    "The eDNA data and assigned taxonomy should be in a specific tab-delimited format. ![asv_table format](../images/asv_table.png)\n",
    "\n",
    "This file is generated automatically by [Tourmaline 2](https://github.com/aomlomics/tourmaline/tree/develop), in X location. If your data was generated with Qiime2 or a previous version of Tourmaline, you can convert the `table.qza`, `taxonomy.qza`, and `repseqs.qza` outputs to the correct format using the `create_asv_seq_taxa_obis.sh` shell script.\n",
    "\n",
    "Example:  \n",
    "\n",
    "``` bash\n",
    "#Run this with a qiime2 environment. \n",
    "bash create_asv_seq_taxa_obis.sh -f \\\n",
    "../gomecc_v2_raw/table-16S-merge.qza -t ../gomecc_v2_raw/taxonomy-16S-merge.qza -r ../gomecc_v2_raw/repseqs-16S-merge.qza \\\n",
    "-o ../gomecc_v2_raw/gomecc-16S-asv.tsv\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Set configs  \n",
    "\n",
    "Below you can set definitions for parameters used in the code. \n",
    "\n",
    "| Parameter           | Description                                                                                                       | Example                                                                                              |\n",
    "|---------------------|-------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------|\n",
    "| `sampleMetadata`    | Name of sheet in FAIRe template data Excel file with sample metadata.                                                    | \"sampleMetadata\"                                                                                  |\n",
    "| `experimentRunMetadata`         | Name of sheet in FAIRe template data Excel file with data about molecular preparation methods.                           | \"experimentRunMetadata\"                                                                                 |\n",
    "| `projectMetadata`        | Name of sheet in FAIRe template data Excel file with metadata about the study.                                           | \"projectMetadata\"                                                                                         |\n",
    "| `excel_file`        | Path of the FAIRe data Excel file.                                                                                  | \"../raw-v3/FAIRe_noaa-aoml-gomecc4.xlsx\"                                                  |\n",
    "| `FAIRe_NOAA_checklist`        | Path of the FAIRe NOAA Checklist, which contains information on mapping to DarwinCore and the expected files for OBIS submission.                                                                                  | \"../raw-v3/FAIRe_NOAA_checklist_v1.0.xlsx\"                                                  |\n",
    "| `datafiles`         | Python dictionary, where keys are the amplicon names and the values are the paths to the cooresponding ASV table. |   See example below to format raw data per analysis  |\n",
    "| `skip_sample_types` | Python list of sample_category values to skip from OBIS submission, such as controls or blanks.                       | [`negative control`, `positive control`] |\n",
    "| `skip_columns`      | Python list of columns to ignore when submitting to OBIS.                                                         | [`samp_collect_notes`]                                                                                   |\n",
    "| `taxonomic_api_source`      | Specify whether you want taxonomic assignment from either WoRMS or GBIF APIs.     |  `WoRMS` | \n",
    "<!-- | `analysisMetadata`     | Name of sheet in FAIRe template data Excel file with data about analysis methods.                                        | \"analysisMetadata_gomecc4_16s_p1\"                                                                                      | -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDIT THIS CELL\n",
    "# Here, you specify where all of your data is, set a few other parameters, and set parameters related to the taxonomic assignment.\n",
    "\n",
    "# STEP 1\n",
    "# Assign sheets from your FAIRe Excel metadata file\n",
    "# NOTE: Left side is what edna2obis calls that data. Right side is the actual sheetname from your FAIRe Excel metadata file:\n",
    "params = {}\n",
    "params['sampleMetadata'] = \"sampleMetadata\"\n",
    "params['experimentRunMetadata']= \"experimentRunMetadata\"\n",
    "params['projectMetadata'] = \"projectMetadata\"\n",
    "\n",
    "params['excel_file'] = \"../raw-v3/FAIRe_NOAA_noaa-aoml-gomecc4_SHARING.xlsx\"\n",
    "params['FAIRe_NOAA_checklist'] = \"../raw-v3/FAIRe_NOAA_checklist_v1.0.xlsx\"\n",
    "# Be sure to also include the paths for the FAIRe metadata Excel sheet, and the FAIRe NOAA Checklist\n",
    "\n",
    "\n",
    "# STEP 2\n",
    "# Assign pathnames for your raw data. Each analysis should have 2 raw data files associated with it.\n",
    "params['datafiles'] = {\n",
    "    'gomecc4_18s_p1-6_v2024.10_241122': {\n",
    "        'taxonomy_file': '../raw-v3/asvTaxaFeatures_gomecc4_18s_p1-6_v2024.10_241122.tsv',\n",
    "        'occurrence_file': '../raw-v3/table_gomecc4_18s_p1-6_v2024.10_241122.tsv'\n",
    "    }, \n",
    "    'gomecc4_16s_p3-6_v2024.10_241122': {\n",
    "        'taxonomy_file': '../raw-v3/asvTaxaFeatures_gomecc4_16s_p3-6_v2024.10_241122.tsv',\n",
    "        'occurrence_file': '../raw-v3/table_gomecc4_16s_p3-6_v2024.10_241122.tsv'\n",
    "    }, \n",
    "    'gomecc4_16s_p1-2_v2024.10_241122': {\n",
    "        'taxonomy_file': '../raw-v3/asvTaxaFeatures_gomecc4_16s_p1-2_v2024.10_241122.tsv',\n",
    "        'occurrence_file': '../raw-v3/table_gomecc4_16s_p1-2_v2024.10_241122.tsv'\n",
    "    },\n",
    "    # Add other analysis runs here, following the pattern:\n",
    "    # 'your_analysis_run_name': {\n",
    "    #     'taxonomy_file': 'path/to/your/asvTaxaFeatures_your_analysis_run_name.tsv',\n",
    "    #     'occurrence_file': 'path/to/your/table_your_analysis_run_name.tsv'\n",
    "    # },\n",
    "}\n",
    "\n",
    "\n",
    "# STEP 3:\n",
    "# However you denote control / blank samples, specify here\n",
    "# 'skip_sample_types' is the column name in your metadata, and negative + positive controls are values in that column\n",
    "params['skip_sample_types'] = ['negative control','positive control']\n",
    "params['skip_columns']= ['samp_collect_notes','date_modified','modified_by']\n",
    "\n",
    "\n",
    "# STEP 4:\n",
    "# Specify which API you would like to use to assign taxonomy. Options are either WoRMS or GBIF\n",
    "# Taxonomic Assignment Parameters:\n",
    "params['taxonomic_api_source'] = 'WoRMS'\n",
    "# params['taxonomic_api_source'] = 'GBIF'\n",
    "\n",
    "\n",
    "# STEP 5:\n",
    "# Define which assays should not consider 'species' rank in their taxonomic assignment.\n",
    "# This is because certain assays (for example, 16S) species' level assignments are not useful / correct.\n",
    "# \tAdditionally, for example, 18S species' level assignments are good, and we want them!\n",
    "# This should be the exact 'assay_name' value as found in your analysisMetadata sheets (cell D3)\n",
    "# and subsequently in the 'assay_name' column of the intermediate occurrence.csv\n",
    "params['user_defined_assays_to_skip_species'] = [\n",
    "    'ssu16sv4v5-emp',  # Example, replace with your actual 16S assay names\n",
    "    # 'another_16s_assay_name_if_any',\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load data\n",
    "\n",
    "Note that in a Jupyter Notebook, the current directory is always where the .ipynb file is being run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load project, sample, experimentRun, and analysis data from the FAIRe Excel file\n",
    "\n",
    "projectMetadata, sampleMetadata, and experimentRunMetadata can be loaded normally, but we dynamically load the analysisMetadata sheet(s). The user may have any number of analysisMetadata sheets in their submission, and the cell below will detect each one automatically and load their data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Excel sheet 'analysisMetadata_gomecc4_16s_p1': Found assay 'ssu16sv4v5-emp' with analysis run 'gomecc4_16s_p1-2_v2024.10_241122'\n",
      "Processing Excel sheet 'analysisMetadata_gomecc4_16s_p3': Found assay 'ssu16sv4v5-emp' with analysis run 'gomecc4_16s_p3-6_v2024.10_241122'\n",
      "Processing Excel sheet 'analysisMetadata_gomecc4_18s_p1': Found assay 'ssu18sv9-emp' with analysis run 'gomecc4_18s_p1-6_v2024.10_241122'\n",
      "\n",
      "Summary of analyses by assay (from analysisMetadata sheets):\n",
      "  - Assay 'ssu16sv4v5-emp': 2 analysis run(s)\n",
      "    - gomecc4_16s_p1-2_v2024.10_241122\n",
      "    - gomecc4_16s_p3-6_v2024.10_241122\n",
      "  - Assay 'ssu18sv9-emp': 1 analysis run(s)\n",
      "    - gomecc4_18s_p1-6_v2024.10_241122\n",
      "\n",
      "Summary of analyses and their corresponding raw data files:\n",
      "  Analysis Run Name: 'gomecc4_18s_p1-6_v2024.10_241122'\n",
      "    Taxonomy File: ../raw-v3/asvTaxaFeatures_gomecc4_18s_p1-6_v2024.10_241122.tsv\n",
      "    Occurrence File: ../raw-v3/table_gomecc4_18s_p1-6_v2024.10_241122.tsv\n",
      "  Analysis Run Name: 'gomecc4_16s_p3-6_v2024.10_241122'\n",
      "    Taxonomy File: ../raw-v3/asvTaxaFeatures_gomecc4_16s_p3-6_v2024.10_241122.tsv\n",
      "    Occurrence File: ../raw-v3/table_gomecc4_16s_p3-6_v2024.10_241122.tsv\n",
      "  Analysis Run Name: 'gomecc4_16s_p1-2_v2024.10_241122'\n",
      "    Taxonomy File: ../raw-v3/asvTaxaFeatures_gomecc4_16s_p1-2_v2024.10_241122.tsv\n",
      "    Occurrence File: ../raw-v3/table_gomecc4_16s_p1-2_v2024.10_241122.tsv\n"
     ]
    }
   ],
   "source": [
    "# Discover all sheets in the Excel file\n",
    "excel = pd.ExcelFile(params['excel_file'])\n",
    "all_sheets = excel.sheet_names\n",
    "\n",
    "# Find analysis metadata sheets\n",
    "analysis_sheets = [sheet for sheet in all_sheets if sheet.startswith('analysisMetadata')]\n",
    "\n",
    "# Load the main data sheets (projectMetadata, sampleMetadata, experimentRunMetadata)\n",
    "data = pd.read_excel(\n",
    "    params['excel_file'],\n",
    "    [params['projectMetadata'], params['sampleMetadata'], params['experimentRunMetadata']],\n",
    "    index_col=None, na_values=[\"\"], comment=\"#\"\n",
    ")\n",
    "\n",
    "# Load all analysis metadata sheets\n",
    "# This dictionary stores the actual DataFrames from each analysisMetadata sheet.\n",
    "analysis_data_by_assay = {}\n",
    "\n",
    "for sheet_name_iter in analysis_sheets:\n",
    "    # Load the sheet\n",
    "    analysis_df = pd.read_excel(params['excel_file'], sheet_name_iter)\n",
    "    \n",
    "    # Get assay_name and analysis_run_name from specific cells (D3 and D4 in FAIRe template)\n",
    "    # Ensure these are strings for reliable dictionary keys.\n",
    "    assay_name = str(analysis_df.iloc[1, 3])        # Corresponds to Excel cell D3\n",
    "    analysis_run_name = str(analysis_df.iloc[2, 3]) # Corresponds to Excel cell D4\n",
    "    \n",
    "    print(f\"Processing Excel sheet '{sheet_name_iter}': Found assay '{assay_name}' with analysis run '{analysis_run_name}'\")\n",
    "    \n",
    "    # Store the analysis DataFrame in analysis_data_by_assay, organized by assay_name then analysis_run_name\n",
    "    if assay_name not in analysis_data_by_assay:\n",
    "        analysis_data_by_assay[assay_name] = {}\n",
    "    analysis_data_by_assay[assay_name][analysis_run_name] = analysis_df\n",
    "\n",
    "# Add the structured analysis data to the main 'data' dictionary\n",
    "data['analysis_data_by_assay'] = analysis_data_by_assay\n",
    "\n",
    "# For backward compatibility or general reference, store the DataFrame of the first analysis sheet found\n",
    "if analysis_sheets:\n",
    "    data['analysisMetadata'] = pd.read_excel(params['excel_file'], analysis_sheets[0])\n",
    "else:\n",
    "    print(\"Warning: No analysis metadata sheets found! 'data['analysisMetadata']' will not be populated.\")\n",
    "\n",
    "# Print summary of analyses by assay from the loaded metadata sheets\n",
    "print(\"\\nSummary of analyses by assay (from analysisMetadata sheets):\")\n",
    "for assay, analyses_dict in analysis_data_by_assay.items():\n",
    "    print(f\"  - Assay '{assay}': {len(analyses_dict)} analysis run(s)\")\n",
    "    for run_name_key in analyses_dict.keys():\n",
    "        print(f\"    - {run_name_key}\")\n",
    "\n",
    "# Verify the user-defined params['analysis_files'] from Cell 8\n",
    "print(\"\\nSummary of analyses and their corresponding raw data files:\")\n",
    "if 'datafiles' in params and params['datafiles']:\n",
    "    for run_name, files_dict in params['datafiles'].items():\n",
    "        print(f\"  Analysis Run Name: '{run_name}'\")\n",
    "        print(f\"    Taxonomy File: {files_dict.get('taxonomy_file', 'Not specified')}\")\n",
    "        print(f\"    Occurrence File: {files_dict.get('occurrence_file', 'Not specified')}\")\n",
    "        # Check if this run_name from params matches one found in the Excel sheets\n",
    "        found_in_excel = any(run_name in an_dict for an_dict in analysis_data_by_assay.values())\n",
    "        if not found_in_excel:\n",
    "            print(f\"    Warning: Analysis run name '{run_name}' from params['datafiles'] was not found as an analysis_run_name in any analysisMetadata sheet.\")\n",
    "else:\n",
    "    print(\"  params['datafiles'] is empty or not defined in Cell 3.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename keys in data dictionary to a general term\n",
    "data['sampleMetadata'] = data.pop(params['sampleMetadata'])\n",
    "data['experimentRunMetadata'] = data.pop(params['experimentRunMetadata'])\n",
    "# The line below is already done in Cell 4. It is treated differently because the exact sheet name is not 'analysisMetadata'\n",
    "# data['analysisMetadata'] = data.pop(params['analysisMetadata'])\n",
    "data['projectMetadata'] = data.pop(params['projectMetadata'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sampleMetadata \n",
    "Contextual data about the samples collected, such as when it was collected, where it was collected from, what kind of sample it is, and what were the properties of the environment or experimental condition from which the sample was taken. Each row is a distinct sample, or Event. Most of this information is recorded during sample collection. This sheet contains terms from the FAIRe NOAA data template. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>samp_name</th>\n",
       "      <th>samp_category</th>\n",
       "      <th>neg_cont_type</th>\n",
       "      <th>pos_cont_type</th>\n",
       "      <th>materialSampleID</th>\n",
       "      <th>sample_derived_from</th>\n",
       "      <th>sample_composed_of</th>\n",
       "      <th>rel_cont_id</th>\n",
       "      <th>biological_rep_relation</th>\n",
       "      <th>decimalLongitude</th>\n",
       "      <th>decimalLatitude</th>\n",
       "      <th>verbatimLongitude</th>\n",
       "      <th>verbatimLatitude</th>\n",
       "      <th>verbatimCoordinateSystem</th>\n",
       "      <th>verbatimSRS</th>\n",
       "      <th>geo_loc_name</th>\n",
       "      <th>eventDate</th>\n",
       "      <th>eventDurationValue</th>\n",
       "      <th>verbatimEventDate</th>\n",
       "      <th>verbatimEventTime</th>\n",
       "      <th>env_broad_scale</th>\n",
       "      <th>env_local_scale</th>\n",
       "      <th>env_medium</th>\n",
       "      <th>habitat_natural_artificial_0_1</th>\n",
       "      <th>samp_collect_method</th>\n",
       "      <th>...</th>\n",
       "      <th>phosphate</th>\n",
       "      <th>phosphate_unit</th>\n",
       "      <th>pressure</th>\n",
       "      <th>pressure_unit</th>\n",
       "      <th>silicate</th>\n",
       "      <th>silicate_unit</th>\n",
       "      <th>tot_alkalinity</th>\n",
       "      <th>tot_alkalinity_unit</th>\n",
       "      <th>transmittance</th>\n",
       "      <th>transmittance_unit</th>\n",
       "      <th>serial_number</th>\n",
       "      <th>line_id</th>\n",
       "      <th>station_id</th>\n",
       "      <th>ctd_cast_number</th>\n",
       "      <th>ctd_bottle_number</th>\n",
       "      <th>replicate_number</th>\n",
       "      <th>extract_id</th>\n",
       "      <th>extract_plate</th>\n",
       "      <th>extract_well_number</th>\n",
       "      <th>extract_well_position</th>\n",
       "      <th>biosample_accession</th>\n",
       "      <th>organism</th>\n",
       "      <th>samp_collect_notes</th>\n",
       "      <th>dna_yield</th>\n",
       "      <th>dna_yield_unit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GOMECC4_27N_Sta1_Deep_A</td>\n",
       "      <td>sample</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GOMECC4_27N_Sta1_Deep</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-79.618</td>\n",
       "      <td>26.997</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WGS84</td>\n",
       "      <td>USA: Atlantic Ocean, east of Florida (27 N)</td>\n",
       "      <td>2021-09-14T11:00-04:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>marine biome [ENVO:00000447]</td>\n",
       "      <td>marine mesopelagic zone [ENVO:00000213]</td>\n",
       "      <td>sea water [ENVO:00002149]</td>\n",
       "      <td>0</td>\n",
       "      <td>https://zenodo.org/records/14224755 (v1.1.0) protocol_sampling_sterivex_dry.md</td>\n",
       "      <td>...</td>\n",
       "      <td>1.94489</td>\n",
       "      <td>µmol/kg</td>\n",
       "      <td>623</td>\n",
       "      <td>dbar</td>\n",
       "      <td>20.3569</td>\n",
       "      <td>µmol/kg</td>\n",
       "      <td>2318.9</td>\n",
       "      <td>µmol/kg</td>\n",
       "      <td>4.7221</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GOMECC4_001</td>\n",
       "      <td>27N</td>\n",
       "      <td>Sta1</td>\n",
       "      <td>not provided</td>\n",
       "      <td>3</td>\n",
       "      <td>A</td>\n",
       "      <td>Plate4_52</td>\n",
       "      <td>GOMECC2021_Plate4</td>\n",
       "      <td>52</td>\n",
       "      <td>D7</td>\n",
       "      <td>SAMN37516091</td>\n",
       "      <td>seawater metagenome</td>\n",
       "      <td>DCM = deep chlorophyl max.</td>\n",
       "      <td>12.057</td>\n",
       "      <td>ng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GOMECC4_27N_Sta1_Deep_B</td>\n",
       "      <td>sample</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GOMECC4_27N_Sta1_Deep</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-79.618</td>\n",
       "      <td>26.997</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WGS84</td>\n",
       "      <td>USA: Atlantic Ocean, east of Florida (27 N)</td>\n",
       "      <td>2021-09-14T11:00-04:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>marine biome [ENVO:00000447]</td>\n",
       "      <td>marine mesopelagic zone [ENVO:00000213]</td>\n",
       "      <td>sea water [ENVO:00002149]</td>\n",
       "      <td>0</td>\n",
       "      <td>https://zenodo.org/records/14224755 (v1.1.0) protocol_sampling_sterivex_dry.md</td>\n",
       "      <td>...</td>\n",
       "      <td>1.94489</td>\n",
       "      <td>µmol/kg</td>\n",
       "      <td>623</td>\n",
       "      <td>dbar</td>\n",
       "      <td>20.3569</td>\n",
       "      <td>µmol/kg</td>\n",
       "      <td>2318.9</td>\n",
       "      <td>µmol/kg</td>\n",
       "      <td>4.7221</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GOMECC4_002</td>\n",
       "      <td>27N</td>\n",
       "      <td>Sta1</td>\n",
       "      <td>not provided</td>\n",
       "      <td>3</td>\n",
       "      <td>B</td>\n",
       "      <td>Plate4_60</td>\n",
       "      <td>GOMECC2021_Plate4</td>\n",
       "      <td>60</td>\n",
       "      <td>D8</td>\n",
       "      <td>SAMN37516092</td>\n",
       "      <td>seawater metagenome</td>\n",
       "      <td>DCM was around 80 m and not well defined.</td>\n",
       "      <td>17.115</td>\n",
       "      <td>ng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GOMECC4_27N_Sta1_Deep_C</td>\n",
       "      <td>sample</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GOMECC4_27N_Sta1_Deep</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-79.618</td>\n",
       "      <td>26.997</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WGS84</td>\n",
       "      <td>USA: Atlantic Ocean, east of Florida (27 N)</td>\n",
       "      <td>2021-09-14T11:00-04:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>marine biome [ENVO:00000447]</td>\n",
       "      <td>marine mesopelagic zone [ENVO:00000213]</td>\n",
       "      <td>sea water [ENVO:00002149]</td>\n",
       "      <td>0</td>\n",
       "      <td>https://zenodo.org/records/14224755 (v1.1.0) protocol_sampling_sterivex_dry.md</td>\n",
       "      <td>...</td>\n",
       "      <td>1.94489</td>\n",
       "      <td>µmol/kg</td>\n",
       "      <td>623</td>\n",
       "      <td>dbar</td>\n",
       "      <td>20.3569</td>\n",
       "      <td>µmol/kg</td>\n",
       "      <td>2318.9</td>\n",
       "      <td>µmol/kg</td>\n",
       "      <td>4.7221</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GOMECC4_003</td>\n",
       "      <td>27N</td>\n",
       "      <td>Sta1</td>\n",
       "      <td>not provided</td>\n",
       "      <td>3</td>\n",
       "      <td>C</td>\n",
       "      <td>Plate4_62</td>\n",
       "      <td>GOMECC2021_Plate4</td>\n",
       "      <td>62</td>\n",
       "      <td>F8</td>\n",
       "      <td>SAMN37516093</td>\n",
       "      <td>seawater metagenome</td>\n",
       "      <td>Surface CTD bottles did not fire correctly; hand niskin bottle used for the surface cast. PM cast.</td>\n",
       "      <td>10.8345</td>\n",
       "      <td>ng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GOMECC4_27N_Sta1_DCM_A</td>\n",
       "      <td>sample</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GOMECC4_27N_Sta1_DCM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-79.618</td>\n",
       "      <td>26.997</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WGS84</td>\n",
       "      <td>USA: Atlantic Ocean, east of Florida (27 N)</td>\n",
       "      <td>2021-09-14T11:00-04:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>marine biome [ENVO:00000447]</td>\n",
       "      <td>marine photic zone [ENVO:00000209]</td>\n",
       "      <td>sea water [ENVO:00002149]</td>\n",
       "      <td>0</td>\n",
       "      <td>https://zenodo.org/records/14224755 (v1.1.0) protocol_sampling_sterivex_dry.md</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0517</td>\n",
       "      <td>µmol/kg</td>\n",
       "      <td>49</td>\n",
       "      <td>dbar</td>\n",
       "      <td>1.05635</td>\n",
       "      <td>µmol/kg</td>\n",
       "      <td>2371</td>\n",
       "      <td>µmol/kg</td>\n",
       "      <td>4.665</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GOMECC4_004</td>\n",
       "      <td>27N</td>\n",
       "      <td>Sta1</td>\n",
       "      <td>not provided</td>\n",
       "      <td>14</td>\n",
       "      <td>A</td>\n",
       "      <td>Plate4_53</td>\n",
       "      <td>GOMECC2021_Plate4</td>\n",
       "      <td>53</td>\n",
       "      <td>E7</td>\n",
       "      <td>SAMN37516094</td>\n",
       "      <td>seawater metagenome</td>\n",
       "      <td>Only enough water for 2 surface replicates.</td>\n",
       "      <td>223.5</td>\n",
       "      <td>ng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GOMECC4_27N_Sta1_DCM_B</td>\n",
       "      <td>sample</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GOMECC4_27N_Sta1_DCM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-79.618</td>\n",
       "      <td>26.997</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WGS84</td>\n",
       "      <td>USA: Atlantic Ocean, east of Florida (27 N)</td>\n",
       "      <td>2021-09-14T11:00-04:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>marine biome [ENVO:00000447]</td>\n",
       "      <td>marine photic zone [ENVO:00000209]</td>\n",
       "      <td>sea water [ENVO:00002149]</td>\n",
       "      <td>0</td>\n",
       "      <td>https://zenodo.org/records/14224755 (v1.1.0) protocol_sampling_sterivex_dry.md</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0517</td>\n",
       "      <td>µmol/kg</td>\n",
       "      <td>49</td>\n",
       "      <td>dbar</td>\n",
       "      <td>1.05635</td>\n",
       "      <td>µmol/kg</td>\n",
       "      <td>2371</td>\n",
       "      <td>µmol/kg</td>\n",
       "      <td>4.665</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GOMECC4_005</td>\n",
       "      <td>27N</td>\n",
       "      <td>Sta1</td>\n",
       "      <td>not provided</td>\n",
       "      <td>14</td>\n",
       "      <td>B</td>\n",
       "      <td>Plate4_46</td>\n",
       "      <td>GOMECC2021_Plate4</td>\n",
       "      <td>46</td>\n",
       "      <td>F6</td>\n",
       "      <td>SAMN37516095</td>\n",
       "      <td>seawater metagenome</td>\n",
       "      <td>NaN</td>\n",
       "      <td>103.26</td>\n",
       "      <td>ng</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 166 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 samp_name samp_category neg_cont_type pos_cont_type  \\\n",
       "0  GOMECC4_27N_Sta1_Deep_A        sample           NaN           NaN   \n",
       "1  GOMECC4_27N_Sta1_Deep_B        sample           NaN           NaN   \n",
       "2  GOMECC4_27N_Sta1_Deep_C        sample           NaN           NaN   \n",
       "3   GOMECC4_27N_Sta1_DCM_A        sample           NaN           NaN   \n",
       "4   GOMECC4_27N_Sta1_DCM_B        sample           NaN           NaN   \n",
       "\n",
       "        materialSampleID  sample_derived_from  sample_composed_of  \\\n",
       "0  GOMECC4_27N_Sta1_Deep                  NaN                 NaN   \n",
       "1  GOMECC4_27N_Sta1_Deep                  NaN                 NaN   \n",
       "2  GOMECC4_27N_Sta1_Deep                  NaN                 NaN   \n",
       "3   GOMECC4_27N_Sta1_DCM                  NaN                 NaN   \n",
       "4   GOMECC4_27N_Sta1_DCM                  NaN                 NaN   \n",
       "\n",
       "   rel_cont_id  biological_rep_relation decimalLongitude decimalLatitude  \\\n",
       "0          NaN                      NaN          -79.618          26.997   \n",
       "1          NaN                      NaN          -79.618          26.997   \n",
       "2          NaN                      NaN          -79.618          26.997   \n",
       "3          NaN                      NaN          -79.618          26.997   \n",
       "4          NaN                      NaN          -79.618          26.997   \n",
       "\n",
       "   verbatimLongitude  verbatimLatitude  verbatimCoordinateSystem verbatimSRS  \\\n",
       "0                NaN               NaN                       NaN       WGS84   \n",
       "1                NaN               NaN                       NaN       WGS84   \n",
       "2                NaN               NaN                       NaN       WGS84   \n",
       "3                NaN               NaN                       NaN       WGS84   \n",
       "4                NaN               NaN                       NaN       WGS84   \n",
       "\n",
       "                                  geo_loc_name               eventDate  \\\n",
       "0  USA: Atlantic Ocean, east of Florida (27 N)  2021-09-14T11:00-04:00   \n",
       "1  USA: Atlantic Ocean, east of Florida (27 N)  2021-09-14T11:00-04:00   \n",
       "2  USA: Atlantic Ocean, east of Florida (27 N)  2021-09-14T11:00-04:00   \n",
       "3  USA: Atlantic Ocean, east of Florida (27 N)  2021-09-14T11:00-04:00   \n",
       "4  USA: Atlantic Ocean, east of Florida (27 N)  2021-09-14T11:00-04:00   \n",
       "\n",
       "   eventDurationValue  verbatimEventDate  verbatimEventTime  \\\n",
       "0                 NaN                NaN                NaN   \n",
       "1                 NaN                NaN                NaN   \n",
       "2                 NaN                NaN                NaN   \n",
       "3                 NaN                NaN                NaN   \n",
       "4                 NaN                NaN                NaN   \n",
       "\n",
       "                env_broad_scale                          env_local_scale  \\\n",
       "0  marine biome [ENVO:00000447]  marine mesopelagic zone [ENVO:00000213]   \n",
       "1  marine biome [ENVO:00000447]  marine mesopelagic zone [ENVO:00000213]   \n",
       "2  marine biome [ENVO:00000447]  marine mesopelagic zone [ENVO:00000213]   \n",
       "3  marine biome [ENVO:00000447]       marine photic zone [ENVO:00000209]   \n",
       "4  marine biome [ENVO:00000447]       marine photic zone [ENVO:00000209]   \n",
       "\n",
       "                  env_medium  habitat_natural_artificial_0_1  \\\n",
       "0  sea water [ENVO:00002149]                               0   \n",
       "1  sea water [ENVO:00002149]                               0   \n",
       "2  sea water [ENVO:00002149]                               0   \n",
       "3  sea water [ENVO:00002149]                               0   \n",
       "4  sea water [ENVO:00002149]                               0   \n",
       "\n",
       "                                                              samp_collect_method  \\\n",
       "0  https://zenodo.org/records/14224755 (v1.1.0) protocol_sampling_sterivex_dry.md   \n",
       "1  https://zenodo.org/records/14224755 (v1.1.0) protocol_sampling_sterivex_dry.md   \n",
       "2  https://zenodo.org/records/14224755 (v1.1.0) protocol_sampling_sterivex_dry.md   \n",
       "3  https://zenodo.org/records/14224755 (v1.1.0) protocol_sampling_sterivex_dry.md   \n",
       "4  https://zenodo.org/records/14224755 (v1.1.0) protocol_sampling_sterivex_dry.md   \n",
       "\n",
       "   ... phosphate phosphate_unit pressure pressure_unit silicate silicate_unit  \\\n",
       "0  ...   1.94489        µmol/kg      623          dbar  20.3569       µmol/kg   \n",
       "1  ...   1.94489        µmol/kg      623          dbar  20.3569       µmol/kg   \n",
       "2  ...   1.94489        µmol/kg      623          dbar  20.3569       µmol/kg   \n",
       "3  ...    0.0517        µmol/kg       49          dbar  1.05635       µmol/kg   \n",
       "4  ...    0.0517        µmol/kg       49          dbar  1.05635       µmol/kg   \n",
       "\n",
       "  tot_alkalinity tot_alkalinity_unit  transmittance transmittance_unit  \\\n",
       "0         2318.9             µmol/kg         4.7221                NaN   \n",
       "1         2318.9             µmol/kg         4.7221                NaN   \n",
       "2         2318.9             µmol/kg         4.7221                NaN   \n",
       "3           2371             µmol/kg          4.665                NaN   \n",
       "4           2371             µmol/kg          4.665                NaN   \n",
       "\n",
       "  serial_number  line_id  station_id  ctd_cast_number  ctd_bottle_number  \\\n",
       "0   GOMECC4_001      27N        Sta1     not provided                  3   \n",
       "1   GOMECC4_002      27N        Sta1     not provided                  3   \n",
       "2   GOMECC4_003      27N        Sta1     not provided                  3   \n",
       "3   GOMECC4_004      27N        Sta1     not provided                 14   \n",
       "4   GOMECC4_005      27N        Sta1     not provided                 14   \n",
       "\n",
       "  replicate_number extract_id      extract_plate  extract_well_number  \\\n",
       "0                A  Plate4_52  GOMECC2021_Plate4                   52   \n",
       "1                B  Plate4_60  GOMECC2021_Plate4                   60   \n",
       "2                C  Plate4_62  GOMECC2021_Plate4                   62   \n",
       "3                A  Plate4_53  GOMECC2021_Plate4                   53   \n",
       "4                B  Plate4_46  GOMECC2021_Plate4                   46   \n",
       "\n",
       "  extract_well_position biosample_accession             organism  \\\n",
       "0                    D7        SAMN37516091  seawater metagenome   \n",
       "1                    D8        SAMN37516092  seawater metagenome   \n",
       "2                    F8        SAMN37516093  seawater metagenome   \n",
       "3                    E7        SAMN37516094  seawater metagenome   \n",
       "4                    F6        SAMN37516095  seawater metagenome   \n",
       "\n",
       "                                                                                   samp_collect_notes  \\\n",
       "0                                                                          DCM = deep chlorophyl max.   \n",
       "1                                                           DCM was around 80 m and not well defined.   \n",
       "2  Surface CTD bottles did not fire correctly; hand niskin bottle used for the surface cast. PM cast.   \n",
       "3                                                         Only enough water for 2 surface replicates.   \n",
       "4                                                                                                 NaN   \n",
       "\n",
       "  dna_yield dna_yield_unit  \n",
       "0    12.057             ng  \n",
       "1    17.115             ng  \n",
       "2   10.8345             ng  \n",
       "3     223.5             ng  \n",
       "4    103.26             ng  \n",
       "\n",
       "[5 rows x 166 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['sampleMetadata'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### experimentRunMetadata  \n",
    "Contextual data about how the samples were prepared for sequencing. Includes how they were extracted, what amplicon was targeted, how they were sequenced. Each row is a separate sequencing library preparation, distinguished by a unique lib_id. **TODO: MIGHT NEED HELP WITH THIS DESCRIPTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>samp_name</th>\n",
       "      <th>assay_name</th>\n",
       "      <th>pcr_plate_id</th>\n",
       "      <th>lib_id</th>\n",
       "      <th>seq_run_id</th>\n",
       "      <th>lib_conc</th>\n",
       "      <th>lib_conc_unit</th>\n",
       "      <th>lib_conc_meth</th>\n",
       "      <th>phix_perc</th>\n",
       "      <th>mid_forward</th>\n",
       "      <th>mid_reverse</th>\n",
       "      <th>filename</th>\n",
       "      <th>filename2</th>\n",
       "      <th>checksum_filename</th>\n",
       "      <th>checksum_filename2</th>\n",
       "      <th>associatedSequences</th>\n",
       "      <th>input_read_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GOMECC4_NegativeControl_1</td>\n",
       "      <td>ssu16sv4v5-emp</td>\n",
       "      <td>not applicable</td>\n",
       "      <td>GOMECC16S_Neg1</td>\n",
       "      <td>20220613_Amplicon_PE250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TAGCAGCT</td>\n",
       "      <td>CGTCGCTA</td>\n",
       "      <td>GOMECC16S_Neg1_S499_L001_R1_001.fastq.gz</td>\n",
       "      <td>GOMECC16S_Neg1_S499_L001_R2_001.fastq.gz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/sra/SRR26148505 | https://www.ncbi.nlm.nih.gov/biosample/SAMN37516589 | https://www.ncbi.nlm.nih.gov/bioproject/PRJNA...</td>\n",
       "      <td>29319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GOMECC4_NegativeControl_2</td>\n",
       "      <td>ssu16sv4v5-emp</td>\n",
       "      <td>not applicable</td>\n",
       "      <td>GOMECC16S_Neg2</td>\n",
       "      <td>20220613_Amplicon_PE250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TAGCAGCT</td>\n",
       "      <td>CTAGAGCT</td>\n",
       "      <td>GOMECC16S_Neg2_S500_L001_R1_001.fastq.gz</td>\n",
       "      <td>GOMECC16S_Neg2_S500_L001_R2_001.fastq.gz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/sra/SRR26148503 | https://www.ncbi.nlm.nih.gov/biosample/SAMN37516590 | https://www.ncbi.nlm.nih.gov/bioproject/PRJNA...</td>\n",
       "      <td>30829</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   samp_name      assay_name    pcr_plate_id          lib_id  \\\n",
       "0  GOMECC4_NegativeControl_1  ssu16sv4v5-emp  not applicable  GOMECC16S_Neg1   \n",
       "1  GOMECC4_NegativeControl_2  ssu16sv4v5-emp  not applicable  GOMECC16S_Neg2   \n",
       "\n",
       "                seq_run_id  lib_conc  lib_conc_unit  lib_conc_meth  phix_perc  \\\n",
       "0  20220613_Amplicon_PE250       NaN            NaN            NaN        NaN   \n",
       "1  20220613_Amplicon_PE250       NaN            NaN            NaN        NaN   \n",
       "\n",
       "  mid_forward mid_reverse                                  filename  \\\n",
       "0    TAGCAGCT    CGTCGCTA  GOMECC16S_Neg1_S499_L001_R1_001.fastq.gz   \n",
       "1    TAGCAGCT    CTAGAGCT  GOMECC16S_Neg2_S500_L001_R1_001.fastq.gz   \n",
       "\n",
       "                                  filename2  checksum_filename  \\\n",
       "0  GOMECC16S_Neg1_S499_L001_R2_001.fastq.gz                NaN   \n",
       "1  GOMECC16S_Neg2_S500_L001_R2_001.fastq.gz                NaN   \n",
       "\n",
       "   checksum_filename2  \\\n",
       "0                 NaN   \n",
       "1                 NaN   \n",
       "\n",
       "                                                                                                                                     associatedSequences  \\\n",
       "0  https://www.ncbi.nlm.nih.gov/sra/SRR26148505 | https://www.ncbi.nlm.nih.gov/biosample/SAMN37516589 | https://www.ncbi.nlm.nih.gov/bioproject/PRJNA...   \n",
       "1  https://www.ncbi.nlm.nih.gov/sra/SRR26148503 | https://www.ncbi.nlm.nih.gov/biosample/SAMN37516590 | https://www.ncbi.nlm.nih.gov/bioproject/PRJNA...   \n",
       "\n",
       "   input_read_count  \n",
       "0             29319  \n",
       "1             30829  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['experimentRunMetadata'].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load ASV data  \n",
    "There is one ASV file for each marker that was sequenced. The ASV data files have one row for each unique amplicon sequence variants (ASVs). They contain the ASV DNA sequence, a unique hash identifier the taxonomic assignment for each ASV, the confidence given that assignment by the naive-bayes classifier, and then the number of reads observed in each sample. \n",
    "\n",
    "This file is created automatically with [Tourmaline v.2023.5+](https://github.com/aomlomics/tourmaline), and is found in `01-taxonomy/asv_taxa_sample_table.tsv`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| column name    | definition                                                                                                                                                                                                                                                                                                                                                                                              |\n",
    "|----------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| featureid      | A hash of the ASV sequence, used as a unique identifier for the ASV.                                                                                                                                                                                                                                                                                                                                    |\n",
    "| taxonomy       | The full taxonomy assigned to an ASV sequence. This string could be formatted in very different ways depending on the reference database used during classification, however it should always be in reverse rank order separated by ;. We provide examples for how to process results from a Silva classifier and the PR2 18S classifier. For other taxonomy formats, the code will need to be adapted. |\n",
    "| Confidence     | This is the confidence score assigned the taxonomic classification with a naive-bayes classifier.                                                                                                                                                                                                                                                                                                       |\n",
    "| sample columns | The next columns each represent a sample (or eventID), and the number of reads for that ASV observed in the sample.                                                                                                                                                                                                                                                                                     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw data into dataframes:\n",
      "  Processing analysis run: gomecc4_18s_p1-6_v2024.10_241122\n",
      "    Successfully loaded taxonomy file: ../raw-v3/asvTaxaFeatures_gomecc4_18s_p1-6_v2024.10_241122.tsv\n",
      "    Successfully loaded occurrence file: ../raw-v3/table_gomecc4_18s_p1-6_v2024.10_241122.tsv\n",
      "  Processing analysis run: gomecc4_16s_p3-6_v2024.10_241122\n",
      "    Successfully loaded taxonomy file: ../raw-v3/asvTaxaFeatures_gomecc4_16s_p3-6_v2024.10_241122.tsv\n",
      "    Successfully loaded occurrence file: ../raw-v3/table_gomecc4_16s_p3-6_v2024.10_241122.tsv\n",
      "  Processing analysis run: gomecc4_16s_p1-2_v2024.10_241122\n",
      "    Successfully loaded taxonomy file: ../raw-v3/asvTaxaFeatures_gomecc4_16s_p1-2_v2024.10_241122.tsv\n",
      "    Successfully loaded occurrence file: ../raw-v3/table_gomecc4_16s_p1-2_v2024.10_241122.tsv\n"
     ]
    }
   ],
   "source": [
    "# Read in ASV tables for each analysis run.\n",
    "# We now have separate taxonomy and occurrence files per analysis_run_name.\n",
    "\n",
    "raw_data_tables = {} # This will store DataFrames for taxonomy and occurrences for each run\n",
    "\n",
    "print(\"Loading raw data into dataframes:\")\n",
    "if 'datafiles' in params and params['datafiles']:\n",
    "    for analysis_run_name, file_paths in params['datafiles'].items():\n",
    "        print(f\"  Processing analysis run: {analysis_run_name}\")\n",
    "        raw_data_tables[analysis_run_name] = {}\n",
    "        \n",
    "        # Load taxonomy file\n",
    "        if 'taxonomy_file' in file_paths:\n",
    "            tax_path = file_paths['taxonomy_file']\n",
    "            try:\n",
    "                raw_data_tables[analysis_run_name]['taxonomy'] = pd.read_table(tax_path, sep='\\t', low_memory=False)\n",
    "                print(f\"    Successfully loaded taxonomy file: {tax_path}\")\n",
    "                # Optional: print shape or head if useful for verification\n",
    "                # print(f\"      Shape: {raw_data_tables[analysis_run_name]['taxonomy'].shape}\")\n",
    "            except FileNotFoundError:\n",
    "                print(f\"    ERROR: Taxonomy file not found at {tax_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"    ERROR: Could not load taxonomy file {tax_path}. Error: {e}\")\n",
    "        else:\n",
    "            print(f\"    Warning: Taxonomy file path not specified for {analysis_run_name}\")\n",
    "            \n",
    "        # Load occurrence file\n",
    "        if 'occurrence_file' in file_paths:\n",
    "            occ_path = file_paths['occurrence_file']\n",
    "            try:\n",
    "                # Row 1 is a comment, Row 2 is header, Data starts Row 3.\n",
    "                # skiprows=1 means skip the first row (the comment).\n",
    "                # header=0 (after skipping 1 row) means use the NEW first row (original Row 2) as headers.\n",
    "                df_occ = pd.read_table(occ_path, \n",
    "                                       sep='\\t', \n",
    "                                       skiprows=1,  # Skip the comment line (original Row 1)\n",
    "                                       header=0,    # Use the next line (original Row 2) as column headers\n",
    "                                       low_memory=False)\n",
    "\n",
    "                # It's common for TSV tools to export the first column header with a '#' prefix, e.g., \"#OTU ID\".\n",
    "                # If so, remove the leading '#'.\n",
    "                if df_occ.columns[0].startswith('#'):\n",
    "                    df_occ.rename(columns={df_occ.columns[0]: df_occ.columns[0][1:]}, inplace=True)\n",
    "                \n",
    "                raw_data_tables[analysis_run_name]['occurrence'] = df_occ\n",
    "                print(f\"    Successfully loaded occurrence file: {occ_path}\")\n",
    "            except FileNotFoundError:\n",
    "                print(f\"    ERROR: Occurrence file not found at {occ_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"    ERROR: Could not load occurrence file {occ_path}. Error: {e}\")\n",
    "        else:\n",
    "            print(f\"    Warning: Occurrence file path not specified for {analysis_run_name}\")\n",
    "\n",
    "# EXAMPLE: to access the taxonomy DataFrame for 'gomecc4_18s_p1-6_v2024.10_241122':\n",
    "# raw_data_tables['gomecc4_18s_p1-6_v2024.10_241122']['taxonomy']\n",
    "# And its occurrence DataFrame:\n",
    "# raw_data_tables['gomecc4_18s_p1-6_v2024.10_241122']['occurrence']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OPTIONAL: Verify the shapes of the raw data before OBIS/GBIF conversion takes place. Does the number of rows and columns match your raw data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis runs for which data was loaded:\n",
      "  - gomecc4_18s_p1-6_v2024.10_241122\n",
      "    - Taxonomy table shape: (24473, 14)\n",
      "    - Occurrence table shape: (24473, 501)\n",
      "  - gomecc4_16s_p3-6_v2024.10_241122\n",
      "    - Taxonomy table shape: (49523, 12)\n",
      "    - Occurrence table shape: (49523, 312)\n",
      "  - gomecc4_16s_p1-2_v2024.10_241122\n",
      "    - Taxonomy table shape: (19540, 12)\n",
      "    - Occurrence table shape: (19540, 198)\n"
     ]
    }
   ],
   "source": [
    "# Inspect the keys of the newly loaded raw_data_tables\n",
    "# This will show the analysis_run_names for which data was loaded.\n",
    "if 'raw_data_tables' in locals() and raw_data_tables: # Check if it exists and is not empty\n",
    "    print(\"Analysis runs for which data was loaded:\")\n",
    "    for run_name in raw_data_tables.keys():\n",
    "        print(f\"  - {run_name}\")\n",
    "        if 'taxonomy' in raw_data_tables[run_name]:\n",
    "            print(f\"    - Taxonomy table shape: {raw_data_tables[run_name]['taxonomy'].shape}\")\n",
    "        else:\n",
    "            print(f\"    - Taxonomy table: Not loaded or error during load.\")\n",
    "        if 'occurrence' in raw_data_tables[run_name]:\n",
    "            print(f\"    - Occurrence table shape: {raw_data_tables[run_name]['occurrence'].shape}\")\n",
    "        else:\n",
    "            print(f\"    - Occurrence table: Not loaded or error during load.\")\n",
    "else:\n",
    "    print(\"raw_data_tables dictionary is not defined or is empty.\")\n",
    "\n",
    "# Example of how to access a specific table\\:\n",
    "# print(raw_data_tables['gomecc4_18s_p1-6_v2024.10_241122']['taxonomy'].head(2))\n",
    "# print(raw_data_tables['gomecc4_18s_p1-6_v2024.10_241122']['occurrence'].head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head of OCCURRENCE table for 'gomecc4_18s_p1-6_v2024.10_241122':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OTU ID</th>\n",
       "      <th>GOMECC4_27N_Sta1_DCM_A</th>\n",
       "      <th>GOMECC4_27N_Sta1_DCM_B</th>\n",
       "      <th>GOMECC4_27N_Sta1_DCM_C</th>\n",
       "      <th>GOMECC4_27N_Sta1_Deep_A</th>\n",
       "      <th>GOMECC4_27N_Sta1_Deep_B</th>\n",
       "      <th>GOMECC4_27N_Sta1_Deep_C</th>\n",
       "      <th>GOMECC4_27N_Sta1_Surface_A</th>\n",
       "      <th>GOMECC4_27N_Sta1_Surface_B</th>\n",
       "      <th>GOMECC4_27N_Sta4_DCM_A</th>\n",
       "      <th>GOMECC4_27N_Sta4_DCM_B</th>\n",
       "      <th>GOMECC4_27N_Sta4_DCM_C</th>\n",
       "      <th>GOMECC4_27N_Sta4_Deep_A</th>\n",
       "      <th>GOMECC4_27N_Sta4_Deep_B</th>\n",
       "      <th>GOMECC4_27N_Sta4_Deep_C</th>\n",
       "      <th>GOMECC4_27N_Sta4_Surface_A</th>\n",
       "      <th>GOMECC4_27N_Sta4_Surface_B</th>\n",
       "      <th>GOMECC4_27N_Sta4_Surface_C</th>\n",
       "      <th>GOMECC4_27N_Sta6_DCM_A</th>\n",
       "      <th>GOMECC4_27N_Sta6_DCM_B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36aa75f9b28f5f831c2d631ba65c2bcb</td>\n",
       "      <td>1518.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4268.0</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9532.0</td>\n",
       "      <td>1930.0</td>\n",
       "      <td>2037.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4e38e8ced9070952b314e1880bede1ca</td>\n",
       "      <td>963.0</td>\n",
       "      <td>316.0</td>\n",
       "      <td>543.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>613.0</td>\n",
       "      <td>561.0</td>\n",
       "      <td>434.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>395.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>915.0</td>\n",
       "      <td>1447.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5d4df37251121c08397c6fbc27b06175</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>864.0</td>\n",
       "      <td>409.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f863f671a575c6ab587e8de0190d3335</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>2672.0</td>\n",
       "      <td>2424.0</td>\n",
       "      <td>2605.0</td>\n",
       "      <td>1918.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2a31e5c01634165da99e7381279baa75</td>\n",
       "      <td>1165.0</td>\n",
       "      <td>2267.0</td>\n",
       "      <td>2206.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1103.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>941.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             OTU ID  GOMECC4_27N_Sta1_DCM_A  \\\n",
       "0  36aa75f9b28f5f831c2d631ba65c2bcb                  1518.0   \n",
       "1  4e38e8ced9070952b314e1880bede1ca                   963.0   \n",
       "2  5d4df37251121c08397c6fbc27b06175                     0.0   \n",
       "3  f863f671a575c6ab587e8de0190d3335                     0.0   \n",
       "4  2a31e5c01634165da99e7381279baa75                  1165.0   \n",
       "\n",
       "   GOMECC4_27N_Sta1_DCM_B  GOMECC4_27N_Sta1_DCM_C  GOMECC4_27N_Sta1_Deep_A  \\\n",
       "0                     0.0                     0.0                      6.0   \n",
       "1                   316.0                   543.0                     19.0   \n",
       "2                     4.0                     0.0                     12.0   \n",
       "3                     0.0                     0.0                      0.0   \n",
       "4                  2267.0                  2206.0                      2.0   \n",
       "\n",
       "   GOMECC4_27N_Sta1_Deep_B  GOMECC4_27N_Sta1_Deep_C  \\\n",
       "0                      0.0                      0.0   \n",
       "1                     10.0                      0.0   \n",
       "2                      5.0                      0.0   \n",
       "3                      0.0                      0.0   \n",
       "4                      0.0                      0.0   \n",
       "\n",
       "   GOMECC4_27N_Sta1_Surface_A  GOMECC4_27N_Sta1_Surface_B  \\\n",
       "0                         0.0                      4268.0   \n",
       "1                         0.0                         0.0   \n",
       "2                         0.0                         0.0   \n",
       "3                         0.0                         0.0   \n",
       "4                         0.0                         0.0   \n",
       "\n",
       "   GOMECC4_27N_Sta4_DCM_A  GOMECC4_27N_Sta4_DCM_B  GOMECC4_27N_Sta4_DCM_C  \\\n",
       "0                  2002.0                     0.0                    14.0   \n",
       "1                   613.0                   561.0                   434.0   \n",
       "2                     9.0                     0.0                     0.0   \n",
       "3                     0.0                     0.0                     5.0   \n",
       "4                     0.0                     0.0                     0.0   \n",
       "\n",
       "   GOMECC4_27N_Sta4_Deep_A  GOMECC4_27N_Sta4_Deep_B  GOMECC4_27N_Sta4_Deep_C  \\\n",
       "0                      0.0                      0.0                      0.0   \n",
       "1                      0.0                    395.0                    297.0   \n",
       "2                      0.0                      0.0                      0.0   \n",
       "3                      0.0                      0.0                      0.0   \n",
       "4                      0.0                      2.0                      0.0   \n",
       "\n",
       "   GOMECC4_27N_Sta4_Surface_A  GOMECC4_27N_Sta4_Surface_B  \\\n",
       "0                      9532.0                      1930.0   \n",
       "1                        76.0                       915.0   \n",
       "2                         0.0                       300.0   \n",
       "3                        92.0                      2672.0   \n",
       "4                      1103.0                       157.0   \n",
       "\n",
       "   GOMECC4_27N_Sta4_Surface_C  GOMECC4_27N_Sta6_DCM_A  GOMECC4_27N_Sta6_DCM_B  \n",
       "0                      2037.0                     0.0                     0.0  \n",
       "1                      1447.0                   140.0                     0.0  \n",
       "2                         0.0                   864.0                   409.0  \n",
       "3                      2424.0                  2605.0                  1918.0  \n",
       "4                         0.0                   104.0                   941.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Choose an analysis run name that you have defined in params['datafiles']\n",
    "# For example, the first one:\n",
    "analysis_to_inspect = list(params['datafiles'].keys())[0] if params['datafiles'] else None\n",
    "\n",
    "if analysis_to_inspect and analysis_to_inspect in raw_data_tables:\n",
    "    if 'occurrence' in raw_data_tables[analysis_to_inspect]:\n",
    "        print(f\"Head of OCCURRENCE table for '{analysis_to_inspect}':\")\n",
    "        display(raw_data_tables[analysis_to_inspect]['occurrence'].iloc[:, 0:20].head())\n",
    "    else:\n",
    "        print(f\"Occurrence table not found for '{analysis_to_inspect}'.\")\n",
    "else:\n",
    "    if not analysis_to_inspect:\n",
    "        print(\"params['datafiles'] is empty. No analysis run to inspect.\")\n",
    "    else:\n",
    "        print(f\"Analysis run '{analysis_to_inspect}' not found in loaded raw_data_tables.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Drop samples with unwanted sample types  \n",
    "\n",
    "Often with eDNA projects, we have control samples that are sequenced along with our survey samples. These can include filtering distilled water, using pure water instead of DNA in a PCR or DNA extraction protocol, or a mock community of known microbial taxa. Controls can help identify and mitigate contaminant DNA in our samples, but are not useful for biodiversity platforms like OBIS. You can select which sample_type values to drop with the `skip_sample_types` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should be your Cell 23 (or equivalent)\n",
    "samps_to_remove = data['sampleMetadata']['samp_category'].isin(params['skip_sample_types'])\n",
    "samples_to_drop = data['sampleMetadata']['samp_name'][samps_to_remove].astype(str).str.strip().tolist() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can view the list of samples to be dropped below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GOMECC4_Blank_DIW_20210915_A',\n",
       " 'GOMECC4_Blank_DIW_20210915_B',\n",
       " 'GOMECC4_Blank_DIW_20210915_C',\n",
       " 'GOMECC4_Blank_DIW_20210930_A',\n",
       " 'GOMECC4_Blank_DIW_20210930_B',\n",
       " 'GOMECC4_Blank_DIW_20210930_C',\n",
       " 'GOMECC4_Blank_DIW_20211011_A',\n",
       " 'GOMECC4_Blank_DIW_20211011_B',\n",
       " 'GOMECC4_Blank_DIW_20211011_C',\n",
       " 'GOMECC4_Blank_DIW_20211016_A',\n",
       " 'GOMECC4_Blank_DIW_20211016_B',\n",
       " 'GOMECC4_Blank_DIW_20211016_C',\n",
       " 'GOMECC4_ExtractionBlank_1',\n",
       " 'GOMECC4_ExtractionBlank_11',\n",
       " 'GOMECC4_ExtractionBlank_12',\n",
       " 'GOMECC4_ExtractionBlank_3',\n",
       " 'GOMECC4_ExtractionBlank_5',\n",
       " 'GOMECC4_ExtractionBlank_7',\n",
       " 'GOMECC4_ExtractionBlank_9',\n",
       " 'GOMECC4_MSUControl_1',\n",
       " 'GOMECC4_MSUControl_2',\n",
       " 'GOMECC4_MSUControl_3',\n",
       " 'GOMECC4_MSUControl_4',\n",
       " 'GOMECC4_MSUControl_5',\n",
       " 'GOMECC4_MSUControl_6',\n",
       " 'GOMECC4_MSUControl_7',\n",
       " 'GOMECC4_NegativeControl_1',\n",
       " 'GOMECC4_NegativeControl_2',\n",
       " 'GOMECC4_PositiveControl_1',\n",
       " 'GOMECC4_PositiveControl_2']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove samples from sampleMetadata sheet\n",
    "data['sampleMetadata'] = data['sampleMetadata'][~samps_to_remove]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['sample'], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the samp_category values left in your sampleMetadata. We only want 'sample' (indicating it is not a control or blank).\n",
    "data['sampleMetadata']['samp_category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove samples from experimentRunMetadata\n",
    "prep_samps_to_remove = data['experimentRunMetadata']['samp_name'].isin(samples_to_drop)\n",
    "data['experimentRunMetadata'] = data['experimentRunMetadata'][~prep_samps_to_remove]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Drop unwanted samples from ASV files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to remove blank/control samples from loaded occurrence tables...\n",
      "  For analysis run 'gomecc4_18s_p1-6_v2024.10_241122':\n",
      "    Original columns: 501, Columns after removal: 473\n",
      "    Removed (28 total for this run): ['GOMECC4_Blank_DIW_20210915_A', 'GOMECC4_Blank_DIW_20210915_B', 'GOMECC4_Blank_DIW_20210915_C', 'GOMECC4_Blank_DIW_20210930_A', 'GOMECC4_Blank_DIW_20210930_B', 'GOMECC4_Blank_DIW_20210930_C', 'GOMECC4_Blank_DIW_20211011_A', 'GOMECC4_Blank_DIW_20211011_B', 'GOMECC4_Blank_DIW_20211011_C', 'GOMECC4_Blank_DIW_20211016_A', 'GOMECC4_Blank_DIW_20211016_B', 'GOMECC4_Blank_DIW_20211016_C', 'GOMECC4_ExtractionBlank_1', 'GOMECC4_ExtractionBlank_11', 'GOMECC4_ExtractionBlank_12', 'GOMECC4_ExtractionBlank_3', 'GOMECC4_ExtractionBlank_5', 'GOMECC4_ExtractionBlank_7', 'GOMECC4_ExtractionBlank_9', 'GOMECC4_MSUControl_1', 'GOMECC4_MSUControl_2', 'GOMECC4_MSUControl_3', 'GOMECC4_MSUControl_4', 'GOMECC4_MSUControl_5', 'GOMECC4_MSUControl_6', 'GOMECC4_MSUControl_7', 'GOMECC4_NegativeControl_1', 'GOMECC4_NegativeControl_2']\n",
      "  For analysis run 'gomecc4_16s_p3-6_v2024.10_241122':\n",
      "    Original columns: 312, Columns after removal: 291\n",
      "    Removed (21 total for this run): ['GOMECC4_Blank_DIW_20210915_A', 'GOMECC4_Blank_DIW_20210915_B', 'GOMECC4_Blank_DIW_20210915_C', 'GOMECC4_Blank_DIW_20211016_A', 'GOMECC4_Blank_DIW_20211016_B', 'GOMECC4_Blank_DIW_20211016_C', 'GOMECC4_ExtractionBlank_11', 'GOMECC4_ExtractionBlank_12', 'GOMECC4_ExtractionBlank_5', 'GOMECC4_ExtractionBlank_7', 'GOMECC4_ExtractionBlank_9', 'GOMECC4_MSUControl_1', 'GOMECC4_MSUControl_2', 'GOMECC4_MSUControl_3', 'GOMECC4_MSUControl_4', 'GOMECC4_MSUControl_5', 'GOMECC4_MSUControl_6', 'GOMECC4_NegativeControl_1', 'GOMECC4_NegativeControl_2', 'GOMECC4_PositiveControl_1', 'GOMECC4_PositiveControl_2']\n",
      "  For analysis run 'gomecc4_16s_p1-2_v2024.10_241122':\n",
      "    Original columns: 198, Columns after removal: 183\n",
      "    Removed (15 total for this run): ['GOMECC4_Blank_DIW_20210930_A', 'GOMECC4_Blank_DIW_20210930_B', 'GOMECC4_Blank_DIW_20210930_C', 'GOMECC4_Blank_DIW_20211011_A', 'GOMECC4_Blank_DIW_20211011_B', 'GOMECC4_Blank_DIW_20211011_C', 'GOMECC4_ExtractionBlank_1', 'GOMECC4_ExtractionBlank_3', 'GOMECC4_MSUControl_1', 'GOMECC4_MSUControl_2', 'GOMECC4_MSUControl_3', 'GOMECC4_MSUControl_4', 'GOMECC4_MSUControl_5', 'GOMECC4_MSUControl_6', 'GOMECC4_PositiveControl_1']\n",
      "Finished blank/control sample removal process.\n"
     ]
    }
   ],
   "source": [
    "# This cell REMOVES blank/control samples from the ALREADY LOADED raw data / abundance tables\n",
    "\n",
    "print(\"Attempting to remove blank/control samples from loaded occurrence tables...\")\n",
    "if 'raw_data_tables' in locals() and raw_data_tables:\n",
    "    if 'samples_to_drop' in locals() and samples_to_drop:\n",
    "        for analysis_run_name, tables_dict in raw_data_tables.items():\n",
    "            if 'occurrence' in tables_dict and not tables_dict['occurrence'].empty:\n",
    "                occ_df = tables_dict['occurrence'] # Get a reference to the occurrence DataFrame\n",
    "                original_cols_count = len(occ_df.columns)\n",
    "                \n",
    "                # Identify which of the samples_to_drop are actual columns in THIS occurrence table\n",
    "                cols_to_remove_in_this_df = [col for col in samples_to_drop if col in occ_df.columns]\n",
    "                \n",
    "                if cols_to_remove_in_this_df:\n",
    "                    # Perform the drop operation. This MODIFIES the DataFrame in raw_data_tables.\n",
    "                    raw_data_tables[analysis_run_name]['occurrence'] = occ_df.drop(columns=cols_to_remove_in_this_df)\n",
    "                    \n",
    "                    print(f\"  For analysis run '{analysis_run_name}':\")\n",
    "                    print(f\"    Original columns: {original_cols_count}, Columns after removal: {len(raw_data_tables[analysis_run_name]['occurrence'].columns)}\")\n",
    "                    print(f\"    Removed ({len(cols_to_remove_in_this_df)} total for this run): {cols_to_remove_in_this_df}\") # Print all removed\n",
    "                else:\n",
    "                    print(f\"  For analysis run '{analysis_run_name}': No specified blank/control samples found to remove in this table's columns.\")\n",
    "            else:\n",
    "                print(f\"  Skipping analysis run '{analysis_run_name}': No 'occurrence' table found or it is empty.\")\n",
    "        print(\"Finished blank/control sample removal process.\")\n",
    "    else:\n",
    "        print(\"WARNING: 'samples_to_drop' list not found or empty. No columns removed from occurrence tables.\")\n",
    "else:\n",
    "    print(\"WARNING: 'raw_data_tables' not found or empty. Cannot remove columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Drop columns with all NAs  \n",
    "\n",
    "If your project data file has columns with only NAs, this code will check for those, provide their column headers for verification, then remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped = pd.DataFrame()\n",
    "\n",
    "for sheet_name in ['sampleMetadata', 'experimentRunMetadata']:\n",
    "    # Safety check: ensure the sheet exists in data and is not empty\n",
    "    if sheet_name in data and not data[sheet_name].empty:\n",
    "        all_na_cols = data[sheet_name].columns[data[sheet_name].isnull().all(axis=0)]\n",
    "        res = pd.Series(all_na_cols, name=sheet_name)\n",
    "        dropped = pd.concat([dropped, res], axis=1)\n",
    "    elif sheet_name not in data:\n",
    "        print(f\"FYI: Sheet '{sheet_name}' not found in 'data' dictionary. Cannot check for all-NA columns.\")\n",
    "    else: # In data but empty\n",
    "        print(f\"FYI: Sheet '{sheet_name}' is empty. Cannot check for all-NA columns.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which columns in each sheet have only NA values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sampleMetadata</th>\n",
       "      <th>experimentRunMetadata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neg_cont_type</td>\n",
       "      <td>lib_conc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pos_cont_type</td>\n",
       "      <td>lib_conc_unit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sample_derived_from</td>\n",
       "      <td>lib_conc_meth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sample_composed_of</td>\n",
       "      <td>phix_perc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rel_cont_id</td>\n",
       "      <td>checksum_filename</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>org_matter</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>org_matter_unit</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>org_nitro</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>org_nitro_unit</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>transmittance_unit</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sampleMetadata experimentRunMetadata\n",
       "0         neg_cont_type              lib_conc\n",
       "1         pos_cont_type         lib_conc_unit\n",
       "2   sample_derived_from         lib_conc_meth\n",
       "3    sample_composed_of             phix_perc\n",
       "4           rel_cont_id     checksum_filename\n",
       "..                  ...                   ...\n",
       "71           org_matter                   NaN\n",
       "72      org_matter_unit                   NaN\n",
       "73            org_nitro                   NaN\n",
       "74       org_nitro_unit                   NaN\n",
       "75   transmittance_unit                   NaN\n",
       "\n",
       "[76 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are fine with leaving these columns out, proceed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping from data['sampleMetadata']: ['neg_cont_type', 'pos_cont_type', 'sample_derived_from', 'sample_composed_of', 'rel_cont_id', 'biological_rep_relation', 'verbatimLongitude', 'verbatimLatitude', 'verbatimCoordinateSystem', 'eventDurationValue', 'verbatimEventDate', 'verbatimEventTime', 'samp_store_method_additional', 'stationed_sample_dur', 'pump_flow_rate', 'pump_flow_rate_unit', 'prefilter_material', 'filter_diameter', 'filter_surface_area', 'prepped_samp_store_temp', 'prepped_samp_store_sol', 'prepped_samp_store_dur', 'prep_method_additional', 'date_ext', 'nucl_acid_ext_modify', 'dna_cleanup_0_1', 'dna_cleanup_method', 'concentration_method', 'ratioOfAbsorbance260_280', 'pool_dna_num', 'nucl_acid_ext_method_additional', 'samp_weather', 'elev', 'light_intensity', 'suspend_part_matter', 'tidal_stage', 'turbidity', 'water_current', 'solar_irradiance', 'wind_direction', 'wind_speed', 'diss_inorg_nitro', 'diss_inorg_nitro_unit', 'diss_org_carb', 'diss_org_carb_unit', 'diss_org_nitro', 'diss_org_nitro_unit', 'tot_diss_nitro', 'tot_diss_nitro_unit', 'tot_inorg_nitro', 'tot_inorg_nitro_unit', 'tot_nitro', 'tot_nitro_unit', 'tot_part_carb', 'tot_part_carb_unit', 'tot_org_carb', 'tot_org_carb_unit', 'tot_org_c_meth', 'tot_nitro_content', 'tot_nitro_content_unit', 'tot_nitro_cont_meth', 'tot_carb', 'tot_carb_unit', 'part_org_carb', 'part_org_carb_unit', 'part_org_nitro', 'part_org_nitro_unit', 'nitro', 'nitro_unit', 'org_carb', 'org_carb_unit', 'org_matter', 'org_matter_unit', 'org_nitro', 'org_nitro_unit', 'transmittance_unit']\n",
      "Dropping from data['experimentRunMetadata']: ['lib_conc', 'lib_conc_unit', 'lib_conc_meth', 'phix_perc', 'checksum_filename', 'checksum_filename2']\n"
     ]
    }
   ],
   "source": [
    "# Drops all-NA columns from 'sampleMetadata' and 'experimentRunMetadata'\n",
    "# based on the 'dropped' DataFrame.\n",
    "\n",
    "sheets_to_clean = ['sampleMetadata', 'experimentRunMetadata']\n",
    "\n",
    "for sheet_name in sheets_to_clean:\n",
    "    # Check if 'dropped' has a column for this sheet AND if that column lists any actual columns to drop\n",
    "    if sheet_name in dropped.columns and not dropped[sheet_name].dropna().empty:\n",
    "        cols_to_drop = list(dropped[sheet_name].dropna())\n",
    "        \n",
    "        if sheet_name in data:\n",
    "            print(f\"Dropping from data['{sheet_name}']: {cols_to_drop}\")\n",
    "            data[sheet_name].drop(columns=cols_to_drop, inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now lets drop NA rows of each analysisMetadata sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identifying rows with empty 'values' column in analysisMetadata sheets...\n",
      "\n",
      "For Analysis (Assay: 'ssu16sv4v5-emp', Run: 'gomecc4_16s_p1-2_v2024.10_241122'), found 36 empty rows:\n",
      "  ['screen_contam_method', 'screen_geograph_method', 'screen_nontarget_method', 'screen_other', 'bioinfo_method_additional', 'output_read_count', 'output_otu_num', 'otu_num_tax_assigned', 'discard_untrimmed', 'qiime2_version', 'tourmaline_asv_method', 'dada2_trunc_len_f', 'dada2pe_trunc_len_r', 'dada2_trim_left_f', 'dada2pe_trim_left_r', 'dada2_max_ee_f', 'dada2pe_max_ee_r', 'dada2_trunc_q', 'dada2_pooling_method', 'dada2_chimera_method', 'dada2_min_fold_parent_over_abundance', 'dada2_n_reads_learn', 'deblur_trim_length', 'deblur_mean_error', 'deblur_indel_prob', 'deblur_indel_max', 'deblur_min_reads', 'deblur_min_size', 'repseqs_min_abundance', 'repseqs_min_length', 'repseqs_max_length', 'repseqs_min_prevalence', 'skl_confidence', 'min_consensus', 'tourmaline_classify_method', 'blca_confidence']\n",
      "\n",
      "For Analysis (Assay: 'ssu16sv4v5-emp', Run: 'gomecc4_16s_p3-6_v2024.10_241122'), found 36 empty rows:\n",
      "  ['screen_contam_method', 'screen_geograph_method', 'screen_nontarget_method', 'screen_other', 'bioinfo_method_additional', 'output_read_count', 'output_otu_num', 'otu_num_tax_assigned', 'discard_untrimmed', 'qiime2_version', 'tourmaline_asv_method', 'dada2_trunc_len_f', 'dada2pe_trunc_len_r', 'dada2_trim_left_f', 'dada2pe_trim_left_r', 'dada2_max_ee_f', 'dada2pe_max_ee_r', 'dada2_trunc_q', 'dada2_pooling_method', 'dada2_chimera_method', 'dada2_min_fold_parent_over_abundance', 'dada2_n_reads_learn', 'deblur_trim_length', 'deblur_mean_error', 'deblur_indel_prob', 'deblur_indel_max', 'deblur_min_reads', 'deblur_min_size', 'repseqs_min_abundance', 'repseqs_min_length', 'repseqs_max_length', 'repseqs_min_prevalence', 'skl_confidence', 'min_consensus', 'tourmaline_classify_method', 'blca_confidence']\n",
      "\n",
      "For Analysis (Assay: 'ssu18sv9-emp', Run: 'gomecc4_18s_p1-6_v2024.10_241122'), found 37 empty rows:\n",
      "  ['trim_param', 'screen_contam_method', 'screen_geograph_method', 'screen_nontarget_method', 'screen_other', 'bioinfo_method_additional', 'output_read_count', 'output_otu_num', 'otu_num_tax_assigned', 'discard_untrimmed', 'qiime2_version', 'tourmaline_asv_method', 'dada2_trunc_len_f', 'dada2pe_trunc_len_r', 'dada2_trim_left_f', 'dada2pe_trim_left_r', 'dada2_max_ee_f', 'dada2pe_max_ee_r', 'dada2_trunc_q', 'dada2_pooling_method', 'dada2_chimera_method', 'dada2_min_fold_parent_over_abundance', 'dada2_n_reads_learn', 'deblur_trim_length', 'deblur_mean_error', 'deblur_indel_prob', 'deblur_indel_max', 'deblur_min_reads', 'deblur_min_size', 'repseqs_min_abundance', 'repseqs_min_length', 'repseqs_max_length', 'repseqs_min_prevalence', 'skl_confidence', 'min_consensus', 'tourmaline_classify_method', 'blca_confidence']\n",
      "Summary: The terms listed above will be dropped from their respective sheets.\n",
      "Proceed to the next cell to finalize their removal.\n"
     ]
    }
   ],
   "source": [
    "# Cell to identify and report rows with empty 'values' in analysisMetadata sheets\n",
    "\n",
    "analysis_rows_to_drop_info = {} \n",
    "expected_value_col_name = 'values' # As per your Excel sheet (column D header)\n",
    "\n",
    "print(f\"Identifying rows with empty '{expected_value_col_name}' column in analysisMetadata sheets...\")\n",
    "\n",
    "if 'analysis_data_by_assay' in data and data['analysis_data_by_assay']:\n",
    "    for assay_name, analyses_dict in data['analysis_data_by_assay'].items():\n",
    "        if not isinstance(analyses_dict, dict): continue\n",
    "        for run_name, analysis_df in analyses_dict.items():\n",
    "            if not isinstance(analysis_df, pd.DataFrame) or analysis_df.empty: continue\n",
    "\n",
    "            # Ensure the 'values' column exists\n",
    "            if expected_value_col_name not in analysis_df.columns:\n",
    "                print(f\"  Warning: Column '{expected_value_col_name}' not found in Assay: '{assay_name}', Run: '{run_name}'. Skipping.\")\n",
    "                continue\n",
    "            \n",
    "            # Identify rows where the 'values' column is NA\n",
    "            empty_values_mask = analysis_df[expected_value_col_name].isnull()\n",
    "            rows_to_drop_indices = analysis_df[empty_values_mask].index.tolist()\n",
    "\n",
    "            if rows_to_drop_indices:\n",
    "                # Get the list of term names for the empty rows\n",
    "                terms_to_drop = analysis_df.loc[rows_to_drop_indices, 'term_name'].tolist() if 'term_name' in analysis_df.columns else []\n",
    "                \n",
    "                # This is the important part: the logic to save the indices for the next cell remains unchanged\n",
    "                analysis_rows_to_drop_info[(assay_name, run_name)] = rows_to_drop_indices\n",
    "\n",
    "                # New, cleaner printing logic\n",
    "                print(f\"\\nFor Analysis (Assay: '{assay_name}', Run: '{run_name}'), found {len(terms_to_drop)} empty rows:\")\n",
    "                print(f\"  {terms_to_drop}\")\n",
    "\n",
    "\n",
    "if not analysis_rows_to_drop_info:\n",
    "    print(f\"\\nNo rows with an empty '{expected_value_col_name}' column were found in any analysisMetadata sheet.\")\n",
    "else:\n",
    "    print(\"Summary: The terms listed above will be dropped from their respective sheets.\")\n",
    "    print(\"Proceed to the next cell to finalize their removal.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you are okay with those rows being deleted, continue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Removing identified rows with empty 'values' from analysisMetadata sheets...\n",
      "  For Assay: 'ssu16sv4v5-emp', Run: 'gomecc4_16s_p1-2_v2024.10_241122': Removed 36 row(s).\n",
      "  For Assay: 'ssu16sv4v5-emp', Run: 'gomecc4_16s_p3-6_v2024.10_241122': Removed 36 row(s).\n",
      "  For Assay: 'ssu18sv9-emp', Run: 'gomecc4_18s_p1-6_v2024.10_241122': Removed 37 row(s).\n",
      "\n",
      "Finished removing rows with empty 'values' from analysisMetadata sheets.\n"
     ]
    }
   ],
   "source": [
    "# Cell to perform the deletion of rows with empty 'values' from analysisMetadata sheets\n",
    "\n",
    "if 'analysis_rows_to_drop_info' in locals() and analysis_rows_to_drop_info:\n",
    "    print(f\"\\nRemoving identified rows with empty 'values' from analysisMetadata sheets...\")\n",
    "    for (assay_name, run_name), indices_to_drop in analysis_rows_to_drop_info.items():\n",
    "        if indices_to_drop: \n",
    "            try:\n",
    "                original_count = len(data['analysis_data_by_assay'][assay_name][run_name])\n",
    "                data['analysis_data_by_assay'][assay_name][run_name].drop(index=indices_to_drop, inplace=True)\n",
    "                new_count = len(data['analysis_data_by_assay'][assay_name][run_name])\n",
    "                print(f\"  For Assay: '{assay_name}', Run: '{run_name}': Removed {original_count - new_count} row(s).\")\n",
    "            except Exception as e:\n",
    "                print(f\"  An error occurred while dropping rows for Assay: '{assay_name}', Run: '{run_name}': {e}\")\n",
    "    print(\"\\nFinished removing rows with empty 'values' from analysisMetadata sheets.\")\n",
    "    # del analysis_rows_to_drop_info # Optional: clear the info variable\n",
    "else:\n",
    "    print(\"\\nNo rows with empty 'values' were previously identified for deletion from analysisMetadata sheets, or 'analysis_rows_to_drop_info' not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now lets check for columns or rows with SOME missing values. OBIS wants those deleted as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check which columns have missing values in some of the rows. These should be filled in on the Excel sheet with the appropriate term ('not applicable', 'missing', or 'not collected'). Alternatively, you can drop the column if it is not needed for submission to OBIS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with some NAs:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sampleMetadata</th>\n",
       "      <th>experimentRunMetadata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ph_meth</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>carbonate_unit</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pco2_unit</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>samp_collect_notes</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sampleMetadata experimentRunMetadata\n",
       "0             ph_meth                   NaN\n",
       "1      carbonate_unit                   NaN\n",
       "2           pco2_unit                   NaN\n",
       "3  samp_collect_notes                   NaN"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some = pd.DataFrame()\n",
    "\n",
    "# Sheets to check for columns with *some* NAs.\n",
    "sheets_to_examine_for_some_na = ['sampleMetadata', 'experimentRunMetadata']\n",
    "\n",
    "for sheet_name in sheets_to_examine_for_some_na:\n",
    "    if sheet_name in data and not data[sheet_name].empty:\n",
    "        cols_with_some_na = data[sheet_name].columns[data[sheet_name].isnull().any(axis=0)]\n",
    "        res = pd.Series(cols_with_some_na.tolist(), name=sheet_name) # .tolist() for cleaner Series\n",
    "        some = pd.concat([some, res], axis=1)\n",
    "\n",
    "print(\"Columns with some NAs:\")\n",
    "some"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I'm going to drop all the columns with some missing data, as I don't need them for submission to OBIS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From data['sampleMetadata']: Dropped columns: ['ph_meth', 'carbonate_unit', 'pco2_unit', 'samp_collect_notes']\n",
      "No columns dropped from data['experimentRunMetadata']\n"
     ]
    }
   ],
   "source": [
    "sheets_to_clean = ['sampleMetadata', 'experimentRunMetadata']\n",
    "\n",
    "for sheet_name in sheets_to_clean:\n",
    "    if sheet_name in data and not data[sheet_name].empty: # Check if DataFrame exists and is not empty\n",
    "        original_columns = data[sheet_name].columns.tolist() # Get column names before dropping\n",
    "        \n",
    "        data[sheet_name].dropna(axis=1, how='any', inplace=True)\n",
    "        \n",
    "        current_columns = data[sheet_name].columns.tolist() # Get column names after dropping\n",
    "        dropped_column_names = [col for col in original_columns if col not in current_columns]\n",
    "        \n",
    "        if dropped_column_names:\n",
    "            print(f\"From data['{sheet_name}']: Dropped columns: {dropped_column_names}\")\n",
    "        else: # Optional: if no columns were dropped\n",
    "            print(f\"No columns dropped from data['{sheet_name}']\")\n",
    "    elif sheet_name not in data:\n",
    "        print(f\"FYI: Sheet '{sheet_name}' not found in 'data'. No columns dropped.\")\n",
    "    else: # Sheet is in data but empty\n",
    "        print(f\"FYI: Sheet '{sheet_name}' is empty. No columns dropped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load data dictionary Excel file \n",
    "This FAIRe NOAA Checklist Excel file also contains columns for mapping FAIRe fields to the appropriate Darwin Core terms which OBIS is expecting. Currently, we are only preparing an Occurrence core file and a DNA-derived extension file, with Event information in the Occurrence file. Future versions of this workflow will prepare an extendedMeasurementOrFact file as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Darwin Core term mapping created: \n",
      "   Occurrence Core mappings: 26 \n",
      "   DNA Derived Extension mappings: 27\n"
     ]
    }
   ],
   "source": [
    "dwc_data = {}\n",
    "checklist_df = pd.DataFrame()\n",
    "\n",
    "try:\n",
    "    checklist_df = pd.read_excel(\n",
    "        params['FAIRe_NOAA_checklist'], # Ensure this param is set in your params cell\n",
    "        sheet_name='checklist',\n",
    "        na_values=[\"\"]\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Error loading 'checklist' sheet: {e}\")\n",
    "\n",
    "# Define relevant column names from your checklist\n",
    "col_faire_term = 'term_name'\n",
    "col_output_spec = 'edna2obis_output_file'\n",
    "col_dwc_mapping = 'dwc_term'\n",
    "\n",
    "occurrence_maps = []\n",
    "dna_derived_maps = []\n",
    "\n",
    "# Process the checklist if it loaded successfully and has the required columns\n",
    "if not checklist_df.empty and all(col in checklist_df.columns for col in [col_faire_term, col_output_spec, col_dwc_mapping]):\n",
    "    for _, row in checklist_df.iterrows():\n",
    "        faire_term = row[col_faire_term]\n",
    "        output_file = str(row[col_output_spec]).lower() # Convert to string and lowercase\n",
    "        dwc_term = row[col_dwc_mapping]\n",
    "\n",
    "        # Add to lists if terms are valid\n",
    "        if pd.notna(faire_term) and pd.notna(dwc_term) and str(faire_term).strip() and str(dwc_term).strip():\n",
    "            if 'occurrence' in output_file:\n",
    "                occurrence_maps.append({'DwC_term': dwc_term, 'FAIRe_term': faire_term})\n",
    "            if 'dnaderived' in output_file:\n",
    "                dna_derived_maps.append({'DwC_term': dwc_term, 'FAIRe_term': faire_term})\n",
    "    \n",
    "    # Create DataFrames, using DwC_term as index\n",
    "    dwc_data['occurrence'] = pd.DataFrame(occurrence_maps).drop_duplicates().set_index('DwC_term') if occurrence_maps else \\\n",
    "                             pd.DataFrame(columns=['FAIRe_term']).set_index(pd.Index([], name='DwC_term'))\n",
    "    \n",
    "    dwc_data['dnaDerived'] = pd.DataFrame(dna_derived_maps).drop_duplicates().set_index('DwC_term') if dna_derived_maps else \\\n",
    "                             pd.DataFrame(columns=['FAIRe_term']).set_index(pd.Index([], name='DwC_term'))\n",
    "else:\n",
    "    # If checklist is empty or missing columns, create empty structures for dwc_data\n",
    "    if checklist_df.empty and 'FAIRe_NOAA_checklist_excel' in params: # Avoid double error message if file load failed\n",
    "        print(f\"Checklist DataFrame is empty or required columns are missing. Creating empty DwC mappings.\")\n",
    "    dwc_data['occurrence'] = pd.DataFrame(columns=['FAIRe_term']).set_index(pd.Index([], name='DwC_term'))\n",
    "    dwc_data['dnaDerived'] = pd.DataFrame(columns=['FAIRe_term']).set_index(pd.Index([], name='DwC_term'))\n",
    "\n",
    "# Print summary\n",
    "print(f\"Darwin Core term mapping created: \\n   Occurrence Core mappings: {len(dwc_data['occurrence'])} \\n   DNA Derived Extension mappings: {len(dwc_data['dnaDerived'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Darwin Core Occurrence Mappings:\n",
      "==================================================\n",
      "              DwC_term             FAIRe_term\n",
      "            recordedBy             recordedBy\n",
      "             datasetID             project_id\n",
      "         parentEventID              samp_name\n",
      "      materialSampleID       materialSampleID\n",
      "      decimalLongitude       decimalLongitude\n",
      "       decimalLatitude        decimalLatitude\n",
      "         geodeticDatum            verbatimSRS\n",
      "              locality           geo_loc_name\n",
      "             eventDate              eventDate\n",
      "       sampleSizeValue              samp_size\n",
      "        sampleSizeUnit         samp_size_unit\n",
      "               eventID                 lib_id\n",
      "   associatedSequences    associatedSequences\n",
      "               kingdom                kingdom\n",
      "                phylum                 phylum\n",
      "                 class                  class\n",
      "                 order                  order\n",
      "                family                 family\n",
      "                 genus                  genus\n",
      "        scientificName         scientificName\n",
      "             taxonRank              taxonRank\n",
      "               taxonID                taxonID\n",
      "verbatimIdentification verbatimIdentification\n",
      " identificationRemarks  identificationRemarks\n",
      "  minimumDepthInMeters   minimumDepthInMeters\n",
      "  maximumDepthInMeters   maximumDepthInMeters\n"
     ]
    }
   ],
   "source": [
    "# Print the mapping for the Occurrence Core\n",
    "# Keep in mind, some terms are hard coded later in this workflow, or are derived by more than one FAIRe term\n",
    "# This means there may be less fields listed below than what the Occurrence Core will have upon completion of edna2obis\n",
    "print(\"Darwin Core Occurrence Mappings:\")\n",
    "print(\"=\" * 50)\n",
    "occurrence_display = dwc_data['occurrence'].reset_index()\n",
    "print(occurrence_display.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "DNA Derived Data Mappings:\n",
      "==================================================\n",
      "               DwC_term                   FAIRe_term\n",
      "        env_broad_scale              env_broad_scale\n",
      "        env_local_scale              env_local_scale\n",
      "             env_medium                   env_medium\n",
      "    samp_collect_device          samp_collect_device\n",
      "       samp_mat_process             samp_mat_process\n",
      "              size_frac                    size_frac\n",
      "    samp_vol_we_dna_ext          samp_vol_we_dna_ext\n",
      "          nucl_acid_ext                nucl_acid_ext\n",
      "          concentration                concentration\n",
      "      concentrationUnit           concentration_unit\n",
      "            target_gene                  target_gene\n",
      "     target_subfragment           target_subfragment\n",
      "           ampliconSize                 ampliconSize\n",
      "     pcr_primer_forward           pcr_primer_forward\n",
      "     pcr_primer_reverse           pcr_primer_reverse\n",
      "pcr_primer_name_forward      pcr_primer_name_forward\n",
      "pcr_primer_name_reverse      pcr_primer_name_reverse\n",
      "   pcr_primer_reference pcr_primer_reference_forward\n",
      "          nucl_acid_amp                nucl_acid_amp\n",
      "               pcr_cond                     pcr_cond\n",
      "               seq_meth                   instrument\n",
      "             lib_layout                   lib_layout\n",
      "                eventID                       lib_id\n",
      "           DNA_sequence                 dna_sequence\n",
      "         otu_class_appr               otu_clust_tool\n",
      "                 otu_db                       otu_db\n",
      "      otu_seq_comp_appr            otu_seq_comp_appr\n"
     ]
    }
   ],
   "source": [
    "# Print the mapping for the DNA Derived Extension\n",
    "# Keep in mind, some terms are hard coded later in this workflow, or are derived by more than one FAIRe term\n",
    "# This means there may be less fields listed below than what the DNA Derived Extension will have upon completion of edna2obis\n",
    "print(\"\\n\\nDNA Derived Data Mappings:\")\n",
    "print(\"=\" * 50)\n",
    "dna_display = dwc_data['dnaDerived'].reset_index()\n",
    "print(dna_display.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to Occurrence file\n",
    "In order to link the DNA-derived extension metadata to our OBIS occurrence records, we have to use the Occurrence core. For this data set, a `parentEvent` is a filtered water sample that was DNA extracted, a sequencing library from that DNA extraction is an `event`, and an `occurrence` is an ASV observed within a library. We will have an an occurence file and a DNA derived data file. Future versions will generate a measurements file.   \n",
    "**Define files**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a combined Occurrence Core for all Analyses\n",
    "This code creates an Occurrence Core for each analysis in the submission, then combines them into one Occurence Core. This operation is complex, includes merging dataframes together, adding the missing fields which OBIS and GBIF expect, and parsing taxonomy raw data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting data processing for 3 analysis run(s) to generate occurrence records.\n",
      "\n",
      "Processing Analysis Run: gomecc4_18s_p1-6_v2024.10_241122\n",
      "  Successfully processed gomecc4_18s_p1-6_v2024.10_241122: Generated 147083 records.\n",
      "\n",
      "Processing Analysis Run: gomecc4_16s_p3-6_v2024.10_241122\n",
      "  Successfully processed gomecc4_16s_p3-6_v2024.10_241122: Generated 118671 records.\n",
      "\n",
      "Processing Analysis Run: gomecc4_16s_p1-2_v2024.10_241122\n",
      "  Successfully processed gomecc4_16s_p1-2_v2024.10_241122: Generated 46460 records.\n",
      "\n",
      "🏁 LOOP COMPLETED: Successful runs: 3, Failed runs: 0, Total DataFrames to combine: 3\n",
      "🔄 No duplicate occurrenceID records found to drop.\n",
      "\n",
      "💾 Combined occurrence file 'occurrence.csv' saved to '../processed-v3/occurrence.csv' with 312214 records.\n",
      "\n",
      "👀 Preview of final combined occurrence data (first 5 rows, selected columns):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eventID</th>\n",
       "      <th>occurrenceID</th>\n",
       "      <th>assay_name</th>\n",
       "      <th>parentEventID</th>\n",
       "      <th>datasetID</th>\n",
       "      <th>recordedBy</th>\n",
       "      <th>locality</th>\n",
       "      <th>decimalLatitude</th>\n",
       "      <th>decimalLongitude</th>\n",
       "      <th>geodeticDatum</th>\n",
       "      <th>identificationRemarks</th>\n",
       "      <th>locationID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GOMECC18S_Plate4_53</td>\n",
       "      <td>GOMECC18S_Plate4_53_occ_36aa75f9b28f5f831c2d631ba65c2bcb</td>\n",
       "      <td>ssu18sv9-emp</td>\n",
       "      <td>GOMECC4_27N_Sta1_DCM_A</td>\n",
       "      <td>noaa-aoml-gomecc4</td>\n",
       "      <td>Luke Thompson</td>\n",
       "      <td>USA: Atlantic Ocean, east of Florida (27 N)</td>\n",
       "      <td>26.997</td>\n",
       "      <td>-79.618</td>\n",
       "      <td>WGS84</td>\n",
       "      <td>qiime2-2021.2; naive bayes classifier; scikit-learn 0.23.1, confidence: 0.99999999990944, against reference database: PR2 v5.0.1; V9 1391f-1510r r...</td>\n",
       "      <td>27N_Sta1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GOMECC18S_Plate4_53</td>\n",
       "      <td>GOMECC18S_Plate4_53_occ_4e38e8ced9070952b314e1880bede1ca</td>\n",
       "      <td>ssu18sv9-emp</td>\n",
       "      <td>GOMECC4_27N_Sta1_DCM_A</td>\n",
       "      <td>noaa-aoml-gomecc4</td>\n",
       "      <td>Luke Thompson</td>\n",
       "      <td>USA: Atlantic Ocean, east of Florida (27 N)</td>\n",
       "      <td>26.997</td>\n",
       "      <td>-79.618</td>\n",
       "      <td>WGS84</td>\n",
       "      <td>qiime2-2021.2; naive bayes classifier; scikit-learn 0.23.1, confidence: 0.999067062720315, against reference database: PR2 v5.0.1; V9 1391f-1510r ...</td>\n",
       "      <td>27N_Sta1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GOMECC18S_Plate4_53</td>\n",
       "      <td>GOMECC18S_Plate4_53_occ_2a31e5c01634165da99e7381279baa75</td>\n",
       "      <td>ssu18sv9-emp</td>\n",
       "      <td>GOMECC4_27N_Sta1_DCM_A</td>\n",
       "      <td>noaa-aoml-gomecc4</td>\n",
       "      <td>Luke Thompson</td>\n",
       "      <td>USA: Atlantic Ocean, east of Florida (27 N)</td>\n",
       "      <td>26.997</td>\n",
       "      <td>-79.618</td>\n",
       "      <td>WGS84</td>\n",
       "      <td>qiime2-2021.2; naive bayes classifier; scikit-learn 0.23.1, confidence: 0.8911679667827849, against reference database: PR2 v5.0.1; V9 1391f-1510r...</td>\n",
       "      <td>27N_Sta1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GOMECC18S_Plate4_53</td>\n",
       "      <td>GOMECC18S_Plate4_53_occ_ecee60339b2fb88ea6d1c8d18054bed4</td>\n",
       "      <td>ssu18sv9-emp</td>\n",
       "      <td>GOMECC4_27N_Sta1_DCM_A</td>\n",
       "      <td>noaa-aoml-gomecc4</td>\n",
       "      <td>Luke Thompson</td>\n",
       "      <td>USA: Atlantic Ocean, east of Florida (27 N)</td>\n",
       "      <td>26.997</td>\n",
       "      <td>-79.618</td>\n",
       "      <td>WGS84</td>\n",
       "      <td>qiime2-2021.2; naive bayes classifier; scikit-learn 0.23.1, confidence: 0.9996383713806553, against reference database: PR2 v5.0.1; V9 1391f-1510r...</td>\n",
       "      <td>27N_Sta1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GOMECC18S_Plate4_53</td>\n",
       "      <td>GOMECC18S_Plate4_53_occ_fa1f1a97dd4ae7c826009186bad26384</td>\n",
       "      <td>ssu18sv9-emp</td>\n",
       "      <td>GOMECC4_27N_Sta1_DCM_A</td>\n",
       "      <td>noaa-aoml-gomecc4</td>\n",
       "      <td>Luke Thompson</td>\n",
       "      <td>USA: Atlantic Ocean, east of Florida (27 N)</td>\n",
       "      <td>26.997</td>\n",
       "      <td>-79.618</td>\n",
       "      <td>WGS84</td>\n",
       "      <td>qiime2-2021.2; naive bayes classifier; scikit-learn 0.23.1, confidence: 0.97066987594629, against reference database: PR2 v5.0.1; V9 1391f-1510r r...</td>\n",
       "      <td>27N_Sta1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               eventID  \\\n",
       "0  GOMECC18S_Plate4_53   \n",
       "1  GOMECC18S_Plate4_53   \n",
       "2  GOMECC18S_Plate4_53   \n",
       "3  GOMECC18S_Plate4_53   \n",
       "4  GOMECC18S_Plate4_53   \n",
       "\n",
       "                                               occurrenceID    assay_name  \\\n",
       "0  GOMECC18S_Plate4_53_occ_36aa75f9b28f5f831c2d631ba65c2bcb  ssu18sv9-emp   \n",
       "1  GOMECC18S_Plate4_53_occ_4e38e8ced9070952b314e1880bede1ca  ssu18sv9-emp   \n",
       "2  GOMECC18S_Plate4_53_occ_2a31e5c01634165da99e7381279baa75  ssu18sv9-emp   \n",
       "3  GOMECC18S_Plate4_53_occ_ecee60339b2fb88ea6d1c8d18054bed4  ssu18sv9-emp   \n",
       "4  GOMECC18S_Plate4_53_occ_fa1f1a97dd4ae7c826009186bad26384  ssu18sv9-emp   \n",
       "\n",
       "            parentEventID          datasetID     recordedBy  \\\n",
       "0  GOMECC4_27N_Sta1_DCM_A  noaa-aoml-gomecc4  Luke Thompson   \n",
       "1  GOMECC4_27N_Sta1_DCM_A  noaa-aoml-gomecc4  Luke Thompson   \n",
       "2  GOMECC4_27N_Sta1_DCM_A  noaa-aoml-gomecc4  Luke Thompson   \n",
       "3  GOMECC4_27N_Sta1_DCM_A  noaa-aoml-gomecc4  Luke Thompson   \n",
       "4  GOMECC4_27N_Sta1_DCM_A  noaa-aoml-gomecc4  Luke Thompson   \n",
       "\n",
       "                                      locality decimalLatitude  \\\n",
       "0  USA: Atlantic Ocean, east of Florida (27 N)          26.997   \n",
       "1  USA: Atlantic Ocean, east of Florida (27 N)          26.997   \n",
       "2  USA: Atlantic Ocean, east of Florida (27 N)          26.997   \n",
       "3  USA: Atlantic Ocean, east of Florida (27 N)          26.997   \n",
       "4  USA: Atlantic Ocean, east of Florida (27 N)          26.997   \n",
       "\n",
       "  decimalLongitude geodeticDatum  \\\n",
       "0          -79.618         WGS84   \n",
       "1          -79.618         WGS84   \n",
       "2          -79.618         WGS84   \n",
       "3          -79.618         WGS84   \n",
       "4          -79.618         WGS84   \n",
       "\n",
       "                                                                                                                                   identificationRemarks  \\\n",
       "0  qiime2-2021.2; naive bayes classifier; scikit-learn 0.23.1, confidence: 0.99999999990944, against reference database: PR2 v5.0.1; V9 1391f-1510r r...   \n",
       "1  qiime2-2021.2; naive bayes classifier; scikit-learn 0.23.1, confidence: 0.999067062720315, against reference database: PR2 v5.0.1; V9 1391f-1510r ...   \n",
       "2  qiime2-2021.2; naive bayes classifier; scikit-learn 0.23.1, confidence: 0.8911679667827849, against reference database: PR2 v5.0.1; V9 1391f-1510r...   \n",
       "3  qiime2-2021.2; naive bayes classifier; scikit-learn 0.23.1, confidence: 0.9996383713806553, against reference database: PR2 v5.0.1; V9 1391f-1510r...   \n",
       "4  qiime2-2021.2; naive bayes classifier; scikit-learn 0.23.1, confidence: 0.97066987594629, against reference database: PR2 v5.0.1; V9 1391f-1510r r...   \n",
       "\n",
       "  locationID  \n",
       "0   27N_Sta1  \n",
       "1   27N_Sta1  \n",
       "2   27N_Sta1  \n",
       "3   27N_Sta1  \n",
       "4   27N_Sta1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- MAIN DATA PROCESSING LOOP --- Loop through each analysis run defined in params['datafiles']\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "print(f\"🚀 Starting data processing for {len(params['datafiles'])} analysis run(s) to generate occurrence records.\")\n",
    "\n",
    "# Define the desired final columns for occurrence.csv IN THE SPECIFIC ORDER REQUIRED\n",
    "DESIRED_OCCURRENCE_COLUMNS_IN_ORDER = [\n",
    "    'eventID', 'organismQuantity', 'assay_name', 'occurrenceID', 'verbatimIdentification',\n",
    "    'kingdom', 'phylum', 'class', 'order', 'family', 'genus', 'species',  \n",
    "    'scientificName', 'scientificNameID', \n",
    "    'taxonRank', 'identificationRemarks',\n",
    "    'taxonID', 'basisOfRecord', 'nameAccordingTo', 'organismQuantityType',\n",
    "    'recordedBy', 'materialSampleID', 'sampleSizeValue', 'sampleSizeUnit',\n",
    "    'associatedSequences', 'locationID', 'eventDate', 'minimumDepthInMeters', 'maximumDepthInMeters',\n",
    "    'locality', 'decimalLatitude', 'decimalLongitude',\n",
    "    'geodeticDatum', 'parentEventID', 'datasetID', 'occurrenceStatus'\n",
    "]\n",
    "\n",
    "output_dir = \"../processed-v3/\" \n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_filename = \"occurrence.csv\"\n",
    "output_path = os.path.join(output_dir, output_filename)\n",
    "\n",
    "all_processed_occurrence_dfs = []\n",
    "successful_runs = 0\n",
    "failed_runs = 0\n",
    "\n",
    "project_recorded_by = \"recordedBy_NotProvided\"\n",
    "project_dataset_id = \"DatasetID_NotProvided\"\n",
    "\n",
    "def get_project_meta_value(project_meta_df, term_to_find, default_val=pd.NA):\n",
    "    if not all(col in project_meta_df.columns for col in ['term_name', 'project_level']):\n",
    "        return default_val\n",
    "    term_to_find_stripped = str(term_to_find).strip()\n",
    "    match = project_meta_df[project_meta_df['term_name'].astype(str).str.strip().str.lower() == term_to_find_stripped.lower()]\n",
    "    if not match.empty:\n",
    "        value = match['project_level'].iloc[0]\n",
    "        if pd.notna(value):\n",
    "            return str(value).strip()\n",
    "    return default_val\n",
    "\n",
    "if 'projectMetadata' in data and not data['projectMetadata'].empty:\n",
    "    project_meta_df = data['projectMetadata']\n",
    "    project_recorded_by = get_project_meta_value(project_meta_df, 'recordedBy', project_recorded_by)\n",
    "    project_dataset_id = get_project_meta_value(project_meta_df, 'project_id', project_dataset_id)\n",
    "else:\n",
    "    print(\"  ⚠️ Warning: projectMetadata is empty or not found. 'recordedBy' and 'datasetID' will use default placeholder values.\")\n",
    "\n",
    "# --- MAIN DATA PROCESSING LOOP ---\n",
    "for analysis_run_name, file_paths_dict in params['datafiles'].items():\n",
    "    print(f\"\\nProcessing Analysis Run: {analysis_run_name}\")\n",
    "    try:\n",
    "        # --- STEP 1: Load and Prepare Raw Taxonomy and Abundance Data ---\n",
    "        if not (analysis_run_name in raw_data_tables and\n",
    "                'taxonomy' in raw_data_tables[analysis_run_name] and not raw_data_tables[analysis_run_name]['taxonomy'].empty and\n",
    "                'occurrence' in raw_data_tables[analysis_run_name] and not raw_data_tables[analysis_run_name]['occurrence'].empty):\n",
    "            print(f\"  Skipping {analysis_run_name}: Raw taxonomy or occurrence data is missing or empty.\")\n",
    "            failed_runs += 1\n",
    "            continue\n",
    "\n",
    "        current_tax_df_raw = raw_data_tables[analysis_run_name]['taxonomy'].copy()\n",
    "        current_abundance_df_raw = raw_data_tables[analysis_run_name]['occurrence'].copy()\n",
    "\n",
    "        featureid_col_tax = current_tax_df_raw.columns[0]\n",
    "        current_tax_df_raw.rename(columns={featureid_col_tax: 'featureid'}, inplace=True)\n",
    "        featureid_col_abun = current_abundance_df_raw.columns[0] \n",
    "        current_abundance_df_raw.rename(columns={featureid_col_abun: 'featureid'}, inplace=True)\n",
    "\n",
    "        sequence_col_dwc = 'DNA_sequence'\n",
    "        sequence_col_input = 'sequence' \n",
    "        confidence_col_original_case = 'Confidence' \n",
    "        \n",
    "        if 'dna_sequence' in current_tax_df_raw.columns and sequence_col_input not in current_tax_df_raw.columns:\n",
    "             current_tax_df_raw.rename(columns={'dna_sequence': sequence_col_input}, inplace=True)\n",
    "        elif sequence_col_input not in current_tax_df_raw.columns:\n",
    "            current_tax_df_raw[sequence_col_input] = pd.NA\n",
    "        \n",
    "        # This is the name of the FAIRe column in current_tax_df_raw that holds the string to be used.\n",
    "        # It will be renamed to 'verbatimIdentification' in current_tax_df_processed.\n",
    "        source_column_for_verbatim_id = 'taxonomy' \n",
    "\n",
    "        if source_column_for_verbatim_id in current_tax_df_raw.columns:\n",
    "            verbatim_id_source_col = source_column_for_verbatim_id \n",
    "            # print(f\"  Using column '{verbatim_id_source_col}' from input as source for DwC 'verbatimIdentification'.\") # Use for Debug if needed\n",
    "        else:\n",
    "            # If the 'taxonomy' column is MISSING from the input file.\n",
    "            print(f\"  CRITICAL WARNING: Column '{source_column_for_verbatim_id}' not found in input taxonomy table for '{analysis_run_name}'. DwC 'verbatimIdentification' will use a placeholder.\")\n",
    "            current_tax_df_raw['verbatimIdentification_placeholder'] = f\"Data from '{source_column_for_verbatim_id}' column not available in source\"\n",
    "            verbatim_id_source_col = 'verbatimIdentification_placeholder'\n",
    "            # This placeholder column will be picked up by tax_cols_to_keep and then renamed.                               \n",
    "        \n",
    "        tax_cols_to_keep = ['featureid', sequence_col_input, verbatim_id_source_col]\n",
    "        if confidence_col_original_case in current_tax_df_raw.columns:\n",
    "            tax_cols_to_keep.append(confidence_col_original_case)\n",
    "        else:\n",
    "            current_tax_df_raw[confidence_col_original_case] = pd.NA \n",
    "            tax_cols_to_keep.append(confidence_col_original_case)\n",
    "\n",
    "        current_tax_df_processed = current_tax_df_raw[[col for col in tax_cols_to_keep if col in current_tax_df_raw.columns]].copy()\n",
    "        current_tax_df_processed.rename(columns={verbatim_id_source_col: 'verbatimIdentification', \n",
    "                                                 sequence_col_input: sequence_col_dwc}, inplace=True)\n",
    "\n",
    "        # --- STEP 2: Melt Abundance and Merge with Taxonomy ---\n",
    "        current_assay_occ_melted = pd.melt(\n",
    "            current_abundance_df_raw, id_vars=['featureid'],\n",
    "            var_name='samp_name', value_name='organismQuantity'\n",
    "        )\n",
    "        current_assay_occ_melted = current_assay_occ_melted[current_assay_occ_melted['organismQuantity'] > 0.0]\n",
    "        \n",
    "        current_assay_occurrence_intermediate_df = pd.merge(\n",
    "            current_assay_occ_melted, current_tax_df_processed,\n",
    "            on='featureid', how='left'\n",
    "        )\n",
    "\n",
    "        # --- STEP 3: Initialize ALL DwC Fields from DESIRED_OCCURRENCE_COLUMNS_IN_ORDER ---\n",
    "        for col in DESIRED_OCCURRENCE_COLUMNS_IN_ORDER:\n",
    "            if col not in current_assay_occurrence_intermediate_df.columns:\n",
    "                 current_assay_occurrence_intermediate_df[col] = pd.NA\n",
    "        \n",
    "        # Set values for fields that are constructed or have fixed values for this process\n",
    "        current_assay_occurrence_intermediate_df['taxonID'] = 'ASV:' + current_assay_occurrence_intermediate_df['featureid'].astype(str)\n",
    "        current_assay_occurrence_intermediate_df['organismQuantityType'] = 'DNA sequence reads'\n",
    "        current_assay_occurrence_intermediate_df['occurrenceStatus'] = 'present'\n",
    "        current_assay_occurrence_intermediate_df['basisOfRecord'] = 'MaterialSample'\n",
    "        current_assay_occurrence_intermediate_df['nameAccordingTo'] = 'Original Classification; WoRMS/GBIF (pending further matching)'\n",
    "        \n",
    "        # --- STEP 4: Assign Project-Level Metadata ---\n",
    "        current_assay_occurrence_intermediate_df['recordedBy'] = project_recorded_by\n",
    "        current_assay_occurrence_intermediate_df['datasetID'] = project_dataset_id\n",
    "\n",
    "        # --- STEP 5: Merge `sampleMetadata` and map FAIRe terms to DwC terms ---\n",
    "        if 'sampleMetadata' in data and not data['sampleMetadata'].empty:\n",
    "            sm_df_to_merge = data['sampleMetadata'].copy()\n",
    "            sm_df_to_merge['samp_name'] = sm_df_to_merge['samp_name'].astype(str).str.strip()\n",
    "            current_assay_occurrence_intermediate_df['samp_name'] = current_assay_occurrence_intermediate_df['samp_name'].astype(str).str.strip()\n",
    "\n",
    "            current_assay_occurrence_intermediate_df = pd.merge(\n",
    "                current_assay_occurrence_intermediate_df, sm_df_to_merge,\n",
    "                on='samp_name', how='left', suffixes=('', '_sm') \n",
    "            )\n",
    "            \n",
    "            # These columns have specific assignment logic/calculation later or are core IDs\n",
    "            # They are filled by mapping only if currently NA. They all have different, specific logic to construct.\n",
    "            cols_with_specific_logic_or_origin = [\n",
    "                'datasetID', 'recordedBy', 'eventID', 'occurrenceID', 'taxonID', \n",
    "                'organismQuantityType', 'occurrenceStatus', 'basisOfRecord', 'nameAccordingTo',\n",
    "                'parentEventID', 'associatedSequences', \n",
    "                'sampleSizeValue', 'sampleSizeUnit', 'identificationRemarks'\n",
    "            ]\n",
    "\n",
    "            # Populate DwC columns using the dwc_data['occurrence'] mapping\n",
    "            for dwc_col_target, faire_row in dwc_data['occurrence'].iterrows():\n",
    "                if dwc_col_target not in DESIRED_OCCURRENCE_COLUMNS_IN_ORDER: # Ensure we only care about desired output columns\n",
    "                    continue \n",
    "\n",
    "                faire_col_source_original = str(faire_row['FAIRe_term']).strip()\n",
    "                source_col_in_df = None\n",
    "                \n",
    "                # Check for the column with _sm suffix first (if a clash occurred during merge with sampleMetadata)\n",
    "                if faire_col_source_original + '_sm' in current_assay_occurrence_intermediate_df.columns:\n",
    "                    source_col_in_df = faire_col_source_original + '_sm'\n",
    "                # Else, check for the original FAIRe term name (if no clash)\n",
    "                elif faire_col_source_original in current_assay_occurrence_intermediate_df.columns:\n",
    "                    source_col_in_df = faire_col_source_original\n",
    "                \n",
    "                if source_col_in_df:\n",
    "                    # If the DwC target column has specific logic for its creation or is a core ID,\n",
    "                    # only fill it from sampleMetadata if it's currently NA.\n",
    "                    if dwc_col_target in cols_with_specific_logic_or_origin:\n",
    "                        current_assay_occurrence_intermediate_df[dwc_col_target] = current_assay_occurrence_intermediate_df[dwc_col_target].fillna(current_assay_occurrence_intermediate_df[source_col_in_df])\n",
    "                    else: \n",
    "                        # For other \"standard\" DwC columns (like locality, lat, lon, geodeticDatum, etc.),\n",
    "                        # directly assign from the source FAIRe column.\n",
    "                        current_assay_occurrence_intermediate_df[dwc_col_target] = current_assay_occurrence_intermediate_df[source_col_in_df]\n",
    "                elif dwc_col_target in ['locality', 'decimalLatitude', 'decimalLongitude', 'geodeticDatum', 'eventDate']: # Only print diagnostic for key terms if mapping is missing in checklist\n",
    "                     print(f\"  DIAGNOSTIC: For DwC term '{dwc_col_target}', its mapped FAIRe term '{faire_col_source_original}' (from checklist) was NOT found as a column in the merged sample data (checked as '{faire_col_source_original}' and '{faire_col_source_original}_sm'). The DwC column '{dwc_col_target}' will likely be empty if not populated by other means.\")\n",
    "        else:\n",
    "            print(f\"  Warning: 'sampleMetadata' is empty or not found. Cannot merge for DwC term population for run {analysis_run_name}.\")\n",
    "\n",
    "        # Construct 'locationID' \n",
    "        line_id_col_sm = 'line_id_sm' if 'line_id_sm' in current_assay_occurrence_intermediate_df.columns else 'line_id'\n",
    "        station_id_col_sm = 'station_id_sm' if 'station_id_sm' in current_assay_occurrence_intermediate_df.columns else 'station_id'\n",
    "\n",
    "        if line_id_col_sm in current_assay_occurrence_intermediate_df.columns and \\\n",
    "           station_id_col_sm in current_assay_occurrence_intermediate_df.columns:\n",
    "            line_ids = current_assay_occurrence_intermediate_df[line_id_col_sm].astype(str).fillna('NoLineID')\n",
    "            station_ids = current_assay_occurrence_intermediate_df[station_id_col_sm].astype(str).fillna('NoStationID')\n",
    "            current_assay_occurrence_intermediate_df['locationID'] = line_ids + \"_\" + station_ids\n",
    "            # print(f\"    Constructed 'locationID'.\") # Reduced verbosity\n",
    "        else:\n",
    "            print(f\"    Warning: Could not construct 'locationID' using '{line_id_col_sm}' or '{station_id_col_sm}'.\")\n",
    "            if 'locationID' not in current_assay_occurrence_intermediate_df.columns or current_assay_occurrence_intermediate_df['locationID'].isna().all():\n",
    "                 current_assay_occurrence_intermediate_df['locationID'] = \"LocationID_NotAvailable\"\n",
    "\n",
    "\n",
    "        # --- STEP 6: Merge `experimentRunMetadata` & Define `eventID`, `associatedSequences` ---\n",
    "        assay_name_for_current_run = next((an_key for an_key, runs_dict_val in data.get('analysis_data_by_assay', {}).items() if isinstance(runs_dict_val, dict) and analysis_run_name in runs_dict_val), None)\n",
    "        \n",
    "        current_assay_occurrence_intermediate_df['assay_name'] = assay_name_for_current_run # <-- ADD THIS LINE\n",
    "\n",
    "        if not assay_name_for_current_run:\n",
    "            print(f\"    ERROR: Could not determine assay_name for '{analysis_run_name}'.\")\n",
    "            current_assay_occurrence_intermediate_df['eventID'] = current_assay_occurrence_intermediate_df['eventID'].fillna(f\"ERROR_eventID_for_{analysis_run_name}\")\n",
    "            # Also ensure a placeholder for assay_name if it couldn't be found, though it should be an error condition\n",
    "            current_assay_occurrence_intermediate_df['assay_name'] = current_assay_occurrence_intermediate_df['assay_name'].fillna(f\"UNKNOWN_ASSAY_FOR_{analysis_run_name}\")\n",
    "        elif 'experimentRunMetadata' in data and not data['experimentRunMetadata'].empty:\n",
    "            erm_df = data['experimentRunMetadata'].copy()\n",
    "            erm_df['samp_name'] = erm_df['samp_name'].astype(str).str.strip()\n",
    "            erm_df_assay_specific = erm_df[erm_df['assay_name'].astype(str).str.strip() == str(assay_name_for_current_run).strip()]\n",
    "\n",
    "            if not erm_df_assay_specific.empty:\n",
    "                faire_lib_id_col = str(dwc_data['occurrence'].loc['eventID', 'FAIRe_term']).strip() if 'eventID' in dwc_data['occurrence'].index else 'lib_id'\n",
    "                faire_assoc_seq_col = str(dwc_data['occurrence'].loc['associatedSequences', 'FAIRe_term']).strip() if 'associatedSequences' in dwc_data['occurrence'].index else 'associatedSequences'\n",
    "\n",
    "                cols_to_select_from_erm = {'samp_name'}\n",
    "                if faire_lib_id_col in erm_df_assay_specific.columns: cols_to_select_from_erm.add(faire_lib_id_col)\n",
    "                if faire_assoc_seq_col in erm_df_assay_specific.columns: cols_to_select_from_erm.add(faire_assoc_seq_col)\n",
    "                \n",
    "                erm_to_merge = erm_df_assay_specific[list(cols_to_select_from_erm)].drop_duplicates(subset=['samp_name']).copy()\n",
    "                \n",
    "                current_assay_occurrence_intermediate_df = pd.merge(\n",
    "                    current_assay_occurrence_intermediate_df, erm_to_merge,\n",
    "                    on='samp_name', how='left', suffixes=('', '_erm')\n",
    "                )\n",
    "                \n",
    "                source_lib_id_col_actual = faire_lib_id_col + '_erm' if faire_lib_id_col + '_erm' in current_assay_occurrence_intermediate_df.columns else faire_lib_id_col\n",
    "                if source_lib_id_col_actual in current_assay_occurrence_intermediate_df.columns:\n",
    "                    current_assay_occurrence_intermediate_df['eventID'] = current_assay_occurrence_intermediate_df['eventID'].fillna(current_assay_occurrence_intermediate_df[source_lib_id_col_actual])\n",
    "                \n",
    "                source_assoc_seq_col_actual = faire_assoc_seq_col + '_erm' if faire_assoc_seq_col + '_erm' in current_assay_occurrence_intermediate_df.columns else faire_assoc_seq_col\n",
    "                if source_assoc_seq_col_actual in current_assay_occurrence_intermediate_df.columns:\n",
    "                     current_assay_occurrence_intermediate_df['associatedSequences'] = current_assay_occurrence_intermediate_df['associatedSequences'].fillna(current_assay_occurrence_intermediate_df[source_assoc_seq_col_actual])\n",
    "            else:\n",
    "                current_assay_occurrence_intermediate_df['eventID'] = current_assay_occurrence_intermediate_df['eventID'].fillna(f\"NoExpMeta_eventID_for_{analysis_run_name}\")\n",
    "        \n",
    "        current_assay_occurrence_intermediate_df['eventID'] = current_assay_occurrence_intermediate_df['eventID'].astype(str)\n",
    "        \n",
    "        # --- STEP 7: Construct `occurrenceID` ---\n",
    "        current_assay_occurrence_intermediate_df['occurrenceID'] = current_assay_occurrence_intermediate_df['eventID'] + '_occ_' + current_assay_occurrence_intermediate_df['featureid'].astype(str)\n",
    "\n",
    "        # --- STEP 8: `identificationRemarks`, `sampleSizeValue`/`Unit`, `parentEventID` ---\n",
    "        otu_seq_comp_appr_str = \"Unknown sequence comparison approach\"\n",
    "        taxa_class_method_str = \"\"\n",
    "        taxa_ref_db_str = \"Unknown reference DB\"\n",
    "\n",
    "        if assay_name_for_current_run and analysis_run_name in data.get('analysis_data_by_assay', {}).get(assay_name_for_current_run, {}):\n",
    "            analysis_meta_df_for_run = data['analysis_data_by_assay'][assay_name_for_current_run][analysis_run_name]\n",
    "            if 'term_name' in analysis_meta_df_for_run.columns and 'values' in analysis_meta_df_for_run.columns:\n",
    "                def get_analysis_meta(term, df, default):\n",
    "                    val_series = df[df['term_name'].astype(str).str.strip() == term]['values']\n",
    "                    return str(val_series.iloc[0]).strip() if not val_series.empty and pd.notna(val_series.iloc[0]) else default\n",
    "                otu_seq_comp_appr_str = get_analysis_meta('otu_seq_comp_appr', analysis_meta_df_for_run, otu_seq_comp_appr_str)\n",
    "                taxa_class_method_str = get_analysis_meta('taxa_class_method', analysis_meta_df_for_run, taxa_class_method_str)\n",
    "                taxa_ref_db_str = get_analysis_meta('otu_db', analysis_meta_df_for_run, taxa_ref_db_str)\n",
    "        \n",
    "        confidence_value_series = pd.Series([\"unknown confidence\"] * len(current_assay_occurrence_intermediate_df), index=current_assay_occurrence_intermediate_df.index, dtype=object)\n",
    "        if confidence_col_original_case in current_assay_occurrence_intermediate_df.columns:\n",
    "            confidence_value_series = current_assay_occurrence_intermediate_df[confidence_col_original_case].astype(str).fillna(\"unknown confidence\")\n",
    "        \n",
    "        current_assay_occurrence_intermediate_df['identificationRemarks'] = f\"{otu_seq_comp_appr_str}, confidence: \" + confidence_value_series + f\", against reference database: {taxa_ref_db_str}\"\n",
    "        \n",
    "        if 'eventID' in current_assay_occurrence_intermediate_df.columns and not current_assay_occurrence_intermediate_df['eventID'].isna().all():\n",
    "            sample_size_map = current_assay_occurrence_intermediate_df.groupby('eventID')['organismQuantity'].sum().to_dict()\n",
    "            current_assay_occurrence_intermediate_df['sampleSizeValue'] = current_assay_occurrence_intermediate_df['eventID'].map(sample_size_map)\n",
    "        current_assay_occurrence_intermediate_df['sampleSizeUnit'] = 'DNA sequence reads'\n",
    "        \n",
    "        if 'parentEventID' in dwc_data['occurrence'].index:\n",
    "            faire_parent_event_id_col_name = str(dwc_data['occurrence'].loc['parentEventID','FAIRe_term']).strip() \n",
    "            actual_parent_event_col_name = faire_parent_event_id_col_name + \"_sm\" if faire_parent_event_id_col_name + \"_sm\" in current_assay_occurrence_intermediate_df.columns else faire_parent_event_id_col_name\n",
    "            if actual_parent_event_col_name in current_assay_occurrence_intermediate_df.columns :\n",
    "                 current_assay_occurrence_intermediate_df['parentEventID'] = current_assay_occurrence_intermediate_df['parentEventID'].fillna(current_assay_occurrence_intermediate_df[actual_parent_event_col_name])\n",
    "\n",
    "        # --- STEP 9: Final Column Selection and Order for this assay's DataFrame ---\n",
    "        for col_final_desired in DESIRED_OCCURRENCE_COLUMNS_IN_ORDER:\n",
    "            if col_final_desired not in current_assay_occurrence_intermediate_df.columns:\n",
    "                current_assay_occurrence_intermediate_df[col_final_desired] = pd.NA\n",
    "        \n",
    "        current_assay_occurrence_final_df = current_assay_occurrence_intermediate_df[DESIRED_OCCURRENCE_COLUMNS_IN_ORDER].copy()\n",
    "        \n",
    "        all_processed_occurrence_dfs.append(current_assay_occurrence_final_df)\n",
    "        successful_runs += 1\n",
    "        print(f\"  Successfully processed {analysis_run_name}: Generated {len(current_assay_occurrence_final_df)} records.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"  ❌ Error processing {analysis_run_name}: {str(e)}\")\n",
    "        print(f\"  Traceback for {analysis_run_name}: {traceback.format_exc()}\")\n",
    "        failed_runs += 1\n",
    "\n",
    "# --- POST-LOOP CONCATENATION & FINALIZATION ---\n",
    "print(f\"\\n🏁 LOOP COMPLETED: Successful runs: {successful_runs}, Failed runs: {failed_runs}, Total DataFrames to combine: {len(all_processed_occurrence_dfs)}\")\n",
    "\n",
    "if all_processed_occurrence_dfs:\n",
    "    occ_all_final_combined = pd.concat(all_processed_occurrence_dfs, ignore_index=True, sort=False)\n",
    "    \n",
    "    for col_final_desired in DESIRED_OCCURRENCE_COLUMNS_IN_ORDER:\n",
    "        if col_final_desired not in occ_all_final_combined.columns:\n",
    "            occ_all_final_combined[col_final_desired] = pd.NA\n",
    "    \n",
    "    occ_all_final_output = occ_all_final_combined.reindex(columns=DESIRED_OCCURRENCE_COLUMNS_IN_ORDER)\n",
    "    \n",
    "    original_rows_before_dedup = len(occ_all_final_output)\n",
    "    if 'occurrenceID' in occ_all_final_output.columns and not occ_all_final_output['occurrenceID'].isna().all():\n",
    "        num_duplicates = occ_all_final_output.duplicated(subset=['occurrenceID']).sum()\n",
    "        if num_duplicates > 0:\n",
    "            occ_all_final_output.drop_duplicates(subset=['occurrenceID'], keep='first', inplace=True)\n",
    "            print(f\"🔄 Dropped {num_duplicates} duplicate occurrenceID records. Final rows: {len(occ_all_final_output)}.\")\n",
    "        else:\n",
    "            print(\"🔄 No duplicate occurrenceID records found to drop.\")\n",
    "    else:\n",
    "        print(\"  ⚠️ WARNING: 'occurrenceID' column not found or is all NA. Cannot effectively drop duplicates based on it.\")\n",
    "\n",
    "    try:\n",
    "        occ_all_final_output.to_csv(output_path, index=False, na_rep='') \n",
    "        print(f\"\\n💾 Combined occurrence file '{output_filename}' saved to '{output_path}' with {len(occ_all_final_output)} records.\")\n",
    "        print(f\"\\n👀 Preview of final combined occurrence data (first 5 rows, selected columns):\")\n",
    "        preview_cols_subset = ['eventID', 'occurrenceID', 'assay_name', 'parentEventID', 'datasetID', 'recordedBy', \n",
    "                               'locality', 'decimalLatitude', 'decimalLongitude', 'geodeticDatum', \n",
    "                               'identificationRemarks', 'locationID']\n",
    "        preview_cols_to_show = [col for col in preview_cols_subset if col in occ_all_final_output.columns]\n",
    "        display(occ_all_final_output[preview_cols_to_show].head())\n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ Error saving combined occurrence file: {str(e)}\")\n",
    "else:\n",
    "    print(f\"❌ No data to combine - all analysis runs may have failed or yielded no occurrence records.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE:\n",
    "The Occurrence Core at this step (before taxonomic assignment through WoRMS or GBIF), contains an assay_name column. This will be removed from the final Occurrence Core (after taxonomic assignment) but it is used by the taxonomic assignment code to know which assay's data you want to remove the 'species' rank from consideration. This is because some assays, like 16S for example, return non-usable assignments at species level, while, for example, 18S species assignments ARE useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEBUGGING ONLY: Create Smaller Subset for Faster Taxonomic Matching\n",
    "\n",
    "**Run this cell ONLY if you want to test taxonomic matching on a small subset.**\n",
    "This cell will take the `occ_df_for_processing_step` (which currently holds the full dataset from Cell 33), create a small subset from it, and then **overwrite** `occ_df_for_processing_step` with this smaller subset.\n",
    "If you want to run taxonomic matching on the **full dataset, simply DO NOT RUN THIS CELL.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load full occurrence data from: ../processed-v3/occurrence.csv\n",
      "Successfully loaded ../processed-v3/occurrence.csv with 312214 records.\n",
      "  Available assays in occurrence.csv: ['ssu18sv9-emp' 'ssu16sv4v5-emp']\n",
      "  Selected 10 rows for assay 'ssu18sv9-emp' in the subset.\n",
      "  Selected 10 rows for assay 'ssu16sv4v5-emp' in the subset.\n",
      "Successfully created subset with 20 total rows for taxonomic matching.\n",
      "  Unique assay_names in subset: ['ssu18sv9-emp' 'ssu16sv4v5-emp']\n",
      "\n",
      "Proceeding with the SUBSET (20 rows) for taxonomic matching.\n"
     ]
    }
   ],
   "source": [
    "# Create a small subset of occurrence.csv for faster testing of taxonomic matching\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Determine the correct output directory path\n",
    "current_output_dir = None\n",
    "if 'output_dir' in globals() and isinstance(output_dir, str):\n",
    "    current_output_dir = output_dir # From a previous run of cell 28\n",
    "elif 'params' in globals() and 'output_dir' in params and isinstance(params['output_dir'], str):\n",
    "    current_output_dir = params['output_dir'] # From cell 8\n",
    "else:\n",
    "    current_output_dir = \"../processed-v3/\" # Fallback\n",
    "    print(f\"Warning: 'output_dir' not found in globals or params, using default: {current_output_dir}\")\n",
    "\n",
    "full_occurrence_csv_path = os.path.join(current_output_dir, \"occurrence.csv\")\n",
    "occ_df_subset_for_matching = pd.DataFrame() # Initialize\n",
    "full_occ_df = pd.DataFrame() # Initialize\n",
    "\n",
    "N_ROWS_PER_ASSAY_SUBSET = 10 # Number of rows to take per assay for the subset\n",
    "print(f\"Attempting to load full occurrence data from: {full_occurrence_csv_path}\")\n",
    "if os.path.exists(full_occurrence_csv_path):\n",
    "    try:\n",
    "        full_occ_df = pd.read_csv(full_occurrence_csv_path, low_memory=False, dtype=str)\n",
    "        if 'organismQuantity' in full_occ_df.columns:\n",
    "            full_occ_df['organismQuantity'] = pd.to_numeric(full_occ_df['organismQuantity'], errors='coerce').fillna(0).astype(float)\n",
    "        print(f\"Successfully loaded {full_occurrence_csv_path} with {len(full_occ_df)} records.\")\n",
    "\n",
    "        if not full_occ_df.empty and 'assay_name' in full_occ_df.columns and 'verbatimIdentification' in full_occ_df.columns:\n",
    "            available_assays = full_occ_df['assay_name'].dropna().unique()\n",
    "            print(f\"  Available assays in occurrence.csv: {available_assays}\")\n",
    "            \n",
    "            subset_dfs = []\n",
    "            for assay in available_assays:\n",
    "                assay_subset = full_occ_df[full_occ_df['assay_name'] == assay].head(N_ROWS_PER_ASSAY_SUBSET)\n",
    "                if not assay_subset.empty:\n",
    "                    subset_dfs.append(assay_subset)\n",
    "                    print(f\"  Selected {len(assay_subset)} rows for assay '{assay}' in the subset.\")\n",
    "                else:\n",
    "                    print(f\"  No rows found for assay '{assay}' to include in subset.\")\n",
    "\n",
    "            if subset_dfs:\n",
    "                occ_df_subset_for_matching = pd.concat(subset_dfs, ignore_index=True)\n",
    "                print(f\"Successfully created subset with {len(occ_df_subset_for_matching)} total rows for taxonomic matching.\")\n",
    "                print(f\"  Unique assay_names in subset: {occ_df_subset_for_matching['assay_name'].unique()}\")\n",
    "                # For inspection:\n",
    "                # display(occ_df_subset_for_matching[['eventID', 'occurrenceID', 'verbatimIdentification', 'assay_name']].head())\n",
    "            else:\n",
    "                print(\"Could not create a valid subset. No assay data found or all assays resulted in empty subsets.\")\n",
    "        else:\n",
    "            print(\"Full occurrence DataFrame is empty or missing 'assay_name'/'verbatimIdentification' columns. Cannot create subset.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading or processing {full_occurrence_csv_path} for subsetting: {e}\")\n",
    "        import traceback\n",
    "        print(traceback.format_exc())\n",
    "else:\n",
    "    print(f\"ERROR: Full occurrence file not found at {full_occurrence_csv_path}. Cannot create subset. Ensure Cell 28 (occurrence.csv generation) ran successfully.\")\n",
    "\n",
    "# Determine which DataFrame to use for the actual matching process\n",
    "current_run_type = \"NO DATA\"\n",
    "if not occ_df_subset_for_matching.empty:\n",
    "    occ_df_for_actual_matching = occ_df_subset_for_matching\n",
    "    current_run_type = \"SUBSET\"\n",
    "    print(f\"\\nProceeding with the {current_run_type} ({len(occ_df_for_actual_matching)} rows) for taxonomic matching.\")\n",
    "elif not full_occ_df.empty: # Fallback to full if subset is empty but full loaded\n",
    "    print(\"\\nWARNING: Subset creation failed or resulted in an empty DataFrame. Proceeding with FULL dataset.\")\n",
    "    occ_df_for_actual_matching = full_occ_df\n",
    "    current_run_type = \"FULL DATASET\"\n",
    "else:\n",
    "    print(\"\\nERROR: Neither subset nor full occurrence data is available for matching. Please check previous cells.\")\n",
    "    occ_df_for_actual_matching = pd.DataFrame() # Ensure it's an empty DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taxonomic Assignment\n",
    "\n",
    "The following cells will perform taxonomic matching using the API specified in the notebook parameters (`params['taxonomic_api_source']`). This process will:\n",
    "1. Dynamically import the appropriate matching script (e.g., `WoRMS_v3_matching.py` or a future `GBIF_v3_matching.py`).\n",
    "2. Read the `occurrence.csv` file generated previously (which includes raw `verbatimIdentification` strings and `assay_name`).\n",
    "3. Use the imported script to query the selected API.\n",
    "4. Implement caching to avoid redundant API calls.\n",
    "5. Handle assay-specific rules (e.g., skipping species-level matching for specified assays).\n",
    "6. Update the occurrence data with API-derived `scientificName`, `scientificNameID`, `taxonRank`, and hierarchical rank columns.\n",
    "7. Set `nameAccordingTo` to reflect the API source.\n",
    "8. Handle specific post-processing rules, such as for \"Eukaryota\"-only verbatim strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported and reloaded WoRMS_v3_matching.py\n",
      "\n",
      "Taxonomic Matching Setup:\n",
      "  API Source: WoRMS\n",
      "  Assays configured by user to skip species-level matching: ['ssu16sv4v5-emp']\n",
      "  Number of processes for WoRMS: All available CPUs\n",
      "  Output directory for reading/writing files: ../processed-v3/\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import pandas as pd\n",
    "import os\n",
    "import WoRMS_v3_matching # Import the new script\n",
    "\n",
    "# Reload the module to pick up any changes if you edit the .py file\n",
    "importlib.reload(WoRMS_v3_matching)\n",
    "\n",
    "print(\"Successfully imported and reloaded WoRMS_v3_matching.py\")\n",
    "\n",
    "# --- Define Parameters for Taxonomic Matching ---\n",
    "\n",
    "# This should be defined in your main parameters cell (e.g., Cell 8 of your notebook)\n",
    "# Ensure 'params' dictionary exists from that cell.\n",
    "if 'params' not in globals():\n",
    "    print(\"CRITICAL ERROR: 'params' dictionary not found. It should be defined in an early cell (e.g., Cell 8).\")\n",
    "    params = {} # Initialize to prevent immediate crash, but this is not ideal\n",
    "\n",
    "# 1. API Source (already in your params from Cell 8)\n",
    "# params['taxonomic_api_source'] = 'WoRMS' # or 'GBIF' when ready\n",
    "\n",
    "# 2. Assays to skip SPECIES-level matching for (already in your params from Cell 8)\n",
    "# params['user_defined_assays_to_skip_species'] = ['ssu16sv4v5-emp'] # Example\n",
    "\n",
    "# 3. Number of processes for matching (0 means use all available CPUs)\n",
    "# This can also be in your main params cell or set here.\n",
    "params['worms_n_proc'] = params.get('worms_n_proc', 0) \n",
    "params['gbif_n_proc'] = params.get('gbif_n_proc', 0) # For future GBIF use\n",
    "\n",
    "# 4. Output directory (should be consistent with where occurrence.csv was saved)\n",
    "# This should also ideally come from your main params cell (Cell 8) or the occurrence.csv generation cell (Cell 28)\n",
    "if 'output_dir' not in params:\n",
    "    if 'output_dir' in globals() and isinstance(output_dir, str): # If set by cell 28\n",
    "         params['output_dir'] = output_dir\n",
    "    else:\n",
    "        params['output_dir'] = \"../processed-v3/\" # Fallback\n",
    "        print(f\"Warning: 'output_dir' not found in params, using fallback: {params['output_dir']}\")\n",
    "else: # If it is in params, ensure it's a string\n",
    "    if not isinstance(params['output_dir'], str):\n",
    "        params['output_dir'] = \"../processed-v3/\"\n",
    "        print(f\"Warning: params['output_dir'] was not a string, reset to default: {params['output_dir']}\")\n",
    "\n",
    "\n",
    "# --- Consolidate parameters for the matching function ---\n",
    "# The WoRMS_OBIS_matcher.py script will primarily use:\n",
    "# - params_dict['taxonomic_api_source']\n",
    "# - params_dict['assays_to_skip_species_match'] (derived from user_defined_assays_to_skip_species)\n",
    "# - params_dict['worms_n_proc'] (or 'gbif_n_proc' for GBIF)\n",
    "\n",
    "current_api_source = params.get('taxonomic_api_source', 'WoRMS') # Default to WoRMS if not set\n",
    "print(f\"\\nTaxonomic Matching Setup:\")\n",
    "print(f\"  API Source: {current_api_source}\")\n",
    "print(f\"  Assays configured by user to skip species-level matching: {params.get('user_defined_assays_to_skip_species', 'NONE DEFINED')}\")\n",
    "if current_api_source == 'WoRMS':\n",
    "    print(f\"  Number of processes for WoRMS: {params['worms_n_proc'] if params['worms_n_proc'] > 0 else 'All available CPUs'}\")\n",
    "elif current_api_source == 'GBIF':\n",
    "    print(f\"  Number of processes for GBIF: {params['gbif_n_proc'] if params['gbif_n_proc'] > 0 else 'All available CPUs'} (GBIF matching not yet fully implemented)\")\n",
    "print(f\"  Output directory for reading/writing files: {params['output_dir']}\")\n",
    "\n",
    "# Standard DwC ranks that the matching script will try to populate\n",
    "DWC_RANKS_STD = ['kingdom', 'phylum', 'class', 'order', 'family', 'genus', 'species']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPTIONAL: Use local reference database for 18S taxonomies\n",
    "This step is entirely optional. You can either continue and run this cell to SIGNIFICANTLY speed up taxonomic assignment for 18S taxonomies, or skip it, and edna2obis will still function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PR2 database from: ../raw-v3/pr2_version_5.0.0_taxonomy.xlsx\n",
      "Successfully created PR2-WoRMS AphiaID dictionary with 13553 entries.\n"
     ]
    }
   ],
   "source": [
    "# OPTIONAL: Pre-computation using PR2 database for 18S AphiaID mapping\n",
    "#\n",
    "# This cell loads the PR2 reference database, which contains pre-mapped WoRMS AphiaIDs for many 18S taxa.\n",
    "# By running this, you create a dictionary that the main matching function can use to bypass the slow,\n",
    "# name-based API search for these specific taxa, significantly speeding up the process for 18S data.\n",
    "#\n",
    "# If you do not have the PR2 file or wish to skip this step, simply DO NOT run this cell.\n",
    "# The main script will proceed without this optimization.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# --- Parameters for this step ---\n",
    "# Define the path to your local copy of the PR2 database Excel file.\n",
    "# You may need to download it first and place it in a relevant directory.\n",
    "pr2_database_path = \"../raw-v3/pr2_version_5.0.0_taxonomy.xlsx\" # <-- USER: UPDATE THIS PATH IF NEEDED\n",
    "\n",
    "# --- Logic to create the AphiaID dictionary ---\n",
    "pr2_worms_dict = {} # Initialize an empty dictionary\n",
    "\n",
    "if os.path.exists(pr2_database_path):\n",
    "    print(f\"Loading PR2 database from: {pr2_database_path}\")\n",
    "    try:\n",
    "        pr2_df = pd.read_excel(pr2_database_path, index_col=None, na_values=[\"\"])\n",
    "        \n",
    "        # Filter for rows that have a WoRMS ID and a species name\n",
    "        pr2_df.dropna(subset=['worms_id', 'species'], inplace=True)\n",
    "        \n",
    "        # Clean up the species names to match the format in our main data\n",
    "        # (e.g., replacing underscores, removing 'sp.', etc.)\n",
    "        species_cleaned = pr2_df['species'].str.replace('_', ' ', regex=False)\n",
    "        species_cleaned = species_cleaned.str.replace(' sp.', '', regex=False).str.strip()\n",
    "        \n",
    "        # Create the dictionary: {species_name: aphia_id}\n",
    "        pr2_worms_dict = dict(zip(species_cleaned, pr2_df['worms_id'].astype(int)))\n",
    "        \n",
    "        print(f\"Successfully created PR2-WoRMS AphiaID dictionary with {len(pr2_worms_dict)} entries.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"---\")\n",
    "        print(f\"WARNING: Could not load or process the PR2 database file at the specified path.\")\n",
    "        print(f\"Error: {e}\")\n",
    "        print(\"The taxonomic matching will proceed without AphiaID pre-matching.\")\n",
    "        print(f\"---\")\n",
    "        pr2_worms_dict = {} \n",
    "else:\n",
    "    print(f\"---\")\n",
    "    print(f\"WARNING: PR2 database file not found at: {pr2_database_path}\")\n",
    "    print(\"The taxonomic matching will proceed without AphiaID pre-matching.\")\n",
    "    print(f\"---\")\n",
    "\n",
    "# Make the dictionary available to the matching script by adding it to the main params\n",
    "if 'params' in locals():\n",
    "    params['pr2_worms_dict'] = pr2_worms_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will determine which ranks each assay uses in this submission, based on the raw taxonomy data files. This is essential, it enables you to specify which assays you would like to NOT have species be considered during taxonomic assignment. (Sometimes, certain assays 'species' ranks are not useful, and can make results inaccurate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Determining maximum taxonomic ranks for each assay...\n",
      "  - Assay 'ssu18sv9-emp': Found 9 rank columns.\n",
      "  - Assay 'ssu16sv4v5-emp': Found 7 rank columns.\n",
      "  - Assay 'ssu16sv4v5-emp': Found 7 rank columns.\n",
      "✅ Rank information stored successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- STEP A: Determine Maximum Ranks per Assay (User's Corrected Logic) ---\n",
    "# This cell implements the user's correct logic to determine the number of taxonomic ranks\n",
    "# for each assay by inspecting the structure of the asvTaxaFeatures files.\n",
    "\n",
    "print(\"📝 Determining maximum taxonomic ranks for each assay...\")\n",
    "assay_rank_info = {}\n",
    "for analysis_run_name, tables in raw_data_tables.items():\n",
    "    tax_df = tables['taxonomy']\n",
    "    assay_name = next((assay for assay, runs in data['analysis_data_by_assay'].items() if analysis_run_name in runs), None)\n",
    "    if assay_name:\n",
    "        try:\n",
    "            # Find columns between the two guaranteed markers\n",
    "            start_idx = tax_df.columns.get_loc('verbatimIdentification') + 1\n",
    "            end_idx = tax_df.columns.get_loc('Confidence')\n",
    "            rank_cols = tax_df.columns[start_idx:end_idx].tolist()\n",
    "            \n",
    "            assay_rank_info[assay_name] = {'max_depth': len(rank_cols)}\n",
    "            print(f\"  - Assay '{assay_name}': Found {len(rank_cols)} rank columns.\")\n",
    "        except KeyError:\n",
    "            print(f\"  ⚠️ WARNING: Could not find required columns for assay '{assay_name}'. Using a default of 7 ranks.\")\n",
    "            assay_rank_info[assay_name] = {'max_depth': 7}\n",
    "\n",
    "# Pass this information to the main params dictionary for the next step\n",
    "params['assay_rank_info'] = assay_rank_info\n",
    "print(\"✅ Rank information stored successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Taxonomic Matching\n",
    "\n",
    "This cell loads the intermediate `occurrence.csv` file (which should include the `assay_name` column) and applies the taxonomic matching using the selected API and its parameters. DEBUG statements will show progress.\n",
    "\n",
    "NOTE: This operation may take a few minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 16:46:41,161 - INFO - Found 3908 unique combinations to process.\n",
      "2025-06-11 16:46:41,162 - INFO - Starting Stage 1: AphiaID pre-matching.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Proceeding with the full dataset (312214 rows) for taxonomic matching.\n",
      "\n",
      "Starting taxonomic matching using 'WoRMS' API on FULL DATASET data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 16:50:49,588 - INFO - Finished Stage 1. Matched 339 taxa via AphiaID. Remaining: 3569.\n",
      "2025-06-11 16:50:49,604 - INFO - Starting Stage 2: Batch name matching for 3768 unique terms.\n",
      "2025-06-11 17:00:52,562 - INFO - Finished Stage 2. Remaining unmatched: 42.\n",
      "2025-06-11 17:00:52,564 - INFO - Starting Stage 3: Assigning 'incertae sedis' to 42 remaining taxa.\n",
      "2025-06-11 17:00:52,564 - INFO - Applying all results to DataFrame.\n",
      "2025-06-11 17:00:54,495 - INFO - \n",
      "--- WoRMS Matching Complete ---\n",
      "2025-06-11 17:00:54,496 - INFO - Total processing time: 853.50 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Taxonomic matching process via 'WoRMS' finished.\n",
      "\n",
      "Preview of DataFrame after matching (selected columns):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eventID</th>\n",
       "      <th>occurrenceID</th>\n",
       "      <th>verbatimIdentification</th>\n",
       "      <th>assay_name</th>\n",
       "      <th>scientificName</th>\n",
       "      <th>scientificNameID</th>\n",
       "      <th>taxonRank</th>\n",
       "      <th>nameAccordingTo</th>\n",
       "      <th>match_type_debug</th>\n",
       "      <th>kingdom</th>\n",
       "      <th>phylum</th>\n",
       "      <th>class</th>\n",
       "      <th>order</th>\n",
       "      <th>family</th>\n",
       "      <th>genus</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GOMECC18S_Plate4_53</td>\n",
       "      <td>GOMECC18S_Plate4_53_occ_36aa75f9b28f5f831c2d631ba65c2bcb</td>\n",
       "      <td>Eukaryota;Obazoa;Opisthokonta;Metazoa;Arthropoda;Crustacea;Maxillopoda</td>\n",
       "      <td>ssu18sv9-emp</td>\n",
       "      <td>Crustacea</td>\n",
       "      <td>urn:lsid:marinespecies.org:taxname:1066</td>\n",
       "      <td>Subphylum</td>\n",
       "      <td>WoRMS</td>\n",
       "      <td>Success_Batch_Crustacea</td>\n",
       "      <td>Animalia</td>\n",
       "      <td>Arthropoda</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GOMECC18S_Plate4_53</td>\n",
       "      <td>GOMECC18S_Plate4_53_occ_4e38e8ced9070952b314e1880bede1ca</td>\n",
       "      <td>Eukaryota;Obazoa;Opisthokonta;Metazoa;Arthropoda;Crustacea;Maxillopoda;Clausocalanus;Clausocalanus_furcatus;</td>\n",
       "      <td>ssu18sv9-emp</td>\n",
       "      <td>Clausocalanus furcatus</td>\n",
       "      <td>urn:lsid:marinespecies.org:taxname:104503</td>\n",
       "      <td>Species</td>\n",
       "      <td>WoRMS</td>\n",
       "      <td>Success_AphiaID_104503</td>\n",
       "      <td>Animalia</td>\n",
       "      <td>Arthropoda</td>\n",
       "      <td>Copepoda</td>\n",
       "      <td>Calanoida</td>\n",
       "      <td>Clausocalanidae</td>\n",
       "      <td>Clausocalanus</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GOMECC18S_Plate4_53</td>\n",
       "      <td>GOMECC18S_Plate4_53_occ_2a31e5c01634165da99e7381279baa75</td>\n",
       "      <td>Eukaryota;Obazoa;Opisthokonta;Metazoa;Arthropoda;Crustacea;Maxillopoda;Oithona;Oithona_sp.;</td>\n",
       "      <td>ssu18sv9-emp</td>\n",
       "      <td>Oithona</td>\n",
       "      <td>urn:lsid:marinespecies.org:taxname:106485</td>\n",
       "      <td>Genus</td>\n",
       "      <td>WoRMS</td>\n",
       "      <td>Success_Batch_Oithona sp.</td>\n",
       "      <td>Animalia</td>\n",
       "      <td>Arthropoda</td>\n",
       "      <td>Copepoda</td>\n",
       "      <td>Cyclopoida</td>\n",
       "      <td>Oithonidae</td>\n",
       "      <td>Oithona</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GOMECC18S_Plate4_53</td>\n",
       "      <td>GOMECC18S_Plate4_53_occ_ecee60339b2fb88ea6d1c8d18054bed4</td>\n",
       "      <td>Eukaryota;TSAR;Alveolata;Dinoflagellata;Dinophyceae</td>\n",
       "      <td>ssu18sv9-emp</td>\n",
       "      <td>Dinophyceae</td>\n",
       "      <td>urn:lsid:marinespecies.org:taxname:19542</td>\n",
       "      <td>Class</td>\n",
       "      <td>WoRMS</td>\n",
       "      <td>Success_Batch_Dinophyceae</td>\n",
       "      <td>Chromista</td>\n",
       "      <td>Myzozoa</td>\n",
       "      <td>Dinophyceae</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GOMECC18S_Plate4_53</td>\n",
       "      <td>GOMECC18S_Plate4_53_occ_fa1f1a97dd4ae7c826009186bad26384</td>\n",
       "      <td>Eukaryota;TSAR;Alveolata;Dinoflagellata;Dinophyceae;Gymnodiniales;Gymnodiniaceae</td>\n",
       "      <td>ssu18sv9-emp</td>\n",
       "      <td>Gymnodiniaceae</td>\n",
       "      <td>urn:lsid:marinespecies.org:taxname:109410</td>\n",
       "      <td>Family</td>\n",
       "      <td>WoRMS</td>\n",
       "      <td>Success_Batch_Gymnodiniaceae</td>\n",
       "      <td>Chromista</td>\n",
       "      <td>Myzozoa</td>\n",
       "      <td>Dinophyceae</td>\n",
       "      <td>Gymnodiniales</td>\n",
       "      <td>Gymnodiniaceae</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               eventID  \\\n",
       "0  GOMECC18S_Plate4_53   \n",
       "1  GOMECC18S_Plate4_53   \n",
       "2  GOMECC18S_Plate4_53   \n",
       "3  GOMECC18S_Plate4_53   \n",
       "4  GOMECC18S_Plate4_53   \n",
       "\n",
       "                                               occurrenceID  \\\n",
       "0  GOMECC18S_Plate4_53_occ_36aa75f9b28f5f831c2d631ba65c2bcb   \n",
       "1  GOMECC18S_Plate4_53_occ_4e38e8ced9070952b314e1880bede1ca   \n",
       "2  GOMECC18S_Plate4_53_occ_2a31e5c01634165da99e7381279baa75   \n",
       "3  GOMECC18S_Plate4_53_occ_ecee60339b2fb88ea6d1c8d18054bed4   \n",
       "4  GOMECC18S_Plate4_53_occ_fa1f1a97dd4ae7c826009186bad26384   \n",
       "\n",
       "                                                                                         verbatimIdentification  \\\n",
       "0                                        Eukaryota;Obazoa;Opisthokonta;Metazoa;Arthropoda;Crustacea;Maxillopoda   \n",
       "1  Eukaryota;Obazoa;Opisthokonta;Metazoa;Arthropoda;Crustacea;Maxillopoda;Clausocalanus;Clausocalanus_furcatus;   \n",
       "2                   Eukaryota;Obazoa;Opisthokonta;Metazoa;Arthropoda;Crustacea;Maxillopoda;Oithona;Oithona_sp.;   \n",
       "3                                                           Eukaryota;TSAR;Alveolata;Dinoflagellata;Dinophyceae   \n",
       "4                              Eukaryota;TSAR;Alveolata;Dinoflagellata;Dinophyceae;Gymnodiniales;Gymnodiniaceae   \n",
       "\n",
       "     assay_name          scientificName  \\\n",
       "0  ssu18sv9-emp               Crustacea   \n",
       "1  ssu18sv9-emp  Clausocalanus furcatus   \n",
       "2  ssu18sv9-emp                 Oithona   \n",
       "3  ssu18sv9-emp             Dinophyceae   \n",
       "4  ssu18sv9-emp          Gymnodiniaceae   \n",
       "\n",
       "                            scientificNameID  taxonRank nameAccordingTo  \\\n",
       "0    urn:lsid:marinespecies.org:taxname:1066  Subphylum           WoRMS   \n",
       "1  urn:lsid:marinespecies.org:taxname:104503    Species           WoRMS   \n",
       "2  urn:lsid:marinespecies.org:taxname:106485      Genus           WoRMS   \n",
       "3   urn:lsid:marinespecies.org:taxname:19542      Class           WoRMS   \n",
       "4  urn:lsid:marinespecies.org:taxname:109410     Family           WoRMS   \n",
       "\n",
       "               match_type_debug    kingdom      phylum        class  \\\n",
       "0       Success_Batch_Crustacea   Animalia  Arthropoda         <NA>   \n",
       "1        Success_AphiaID_104503   Animalia  Arthropoda     Copepoda   \n",
       "2     Success_Batch_Oithona sp.   Animalia  Arthropoda     Copepoda   \n",
       "3     Success_Batch_Dinophyceae  Chromista     Myzozoa  Dinophyceae   \n",
       "4  Success_Batch_Gymnodiniaceae  Chromista     Myzozoa  Dinophyceae   \n",
       "\n",
       "           order           family          genus species  \n",
       "0           <NA>             <NA>           <NA>    <NA>  \n",
       "1      Calanoida  Clausocalanidae  Clausocalanus    <NA>  \n",
       "2     Cyclopoida       Oithonidae        Oithona    <NA>  \n",
       "3           <NA>             <NA>           <NA>    <NA>  \n",
       "4  Gymnodiniales   Gymnodiniaceae           <NA>    <NA>  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Counts of 'WoRMS' match_type_debug:\n",
      "match_type_debug\n",
      "Success_Batch_Alphaproteobacteria              38874\n",
      "Success_Batch_Syndiniales                      34619\n",
      "Failed_All_Stages_IncertaeSedis                22762\n",
      "Success_Batch_Gammaproteobacteria              14519\n",
      "Success_Batch_Marinimicrobia (SAR406 clade)    13342\n",
      "                                               ...  \n",
      "Success_Batch_Enterovibrio                         1\n",
      "Success_AphiaID_134541                             1\n",
      "Success_Batch_Muricauda                            1\n",
      "Success_AphiaID_106306                             1\n",
      "Success_Batch_Haloplasma                           1\n",
      "Name: count, Length: 1337, dtype: int64\n",
      "\n",
      "Counts of resulting 'WoRMS' scientificName (top 10 unique non-NA values):\n",
      "scientificName\n",
      "Alphaproteobacteria    38874\n",
      "Syndiniales            34998\n",
      "incertae sedis         22762\n",
      "Gammaproteobacteria    14519\n",
      "Marinimicrobium        13343\n",
      "Bacteria               13165\n",
      "Dinophyceae             9867\n",
      "Rhizaria                7121\n",
      "Cyanobacterium          7092\n",
      "Myrine                  6623\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# --- Perform Taxonomic Matching ---\n",
    "# This cell calls the matching script which handles the API queries and \n",
    "# returns the full, taxonomically-enriched DataFrame, following the proven-working pattern.\n",
    "\n",
    "# This variable comes from the successful run of the cell that creates the combined occurrence core\n",
    "df_to_match = occ_all_final_output.copy() \n",
    "print(f\"INFO: Proceeding with the full dataset ({len(df_to_match)} rows) for taxonomic matching.\")\n",
    "current_run_type = \"FULL DATASET\" # Set for consistent output messaging\n",
    "\n",
    "# Proceed with matching if the DataFrame is not empty\n",
    "if df_to_match.empty:\n",
    "    print(\"Cannot proceed with matching as the dataframe is empty.\")\n",
    "    occ_df_matched_with_api = pd.DataFrame()\n",
    "elif 'params' not in globals():\n",
    "    print(\"ERROR: 'params' dictionary not found. Cannot proceed with taxonomic matching.\")\n",
    "    occ_df_matched_with_api = pd.DataFrame()\n",
    "else:\n",
    "    api_to_use = params.get('taxonomic_api_source')\n",
    "    print(f\"\\nStarting taxonomic matching using '{api_to_use}' API on {current_run_type} data...\")\n",
    "\n",
    "    occ_df_matched_with_api = pd.DataFrame()\n",
    "    if api_to_use == 'WoRMS':\n",
    "        # Consolidate all parameters needed by the matching script.\n",
    "        # This combines the structure from the user's working example with the new\n",
    "        # parameters needed for PR2/assay-specific logic.\n",
    "        matching_params = {\n",
    "            'taxonomic_api_source': 'WoRMS',\n",
    "            'assays_to_skip_species_match': params.get('user_defined_assays_to_skip_species', []),\n",
    "            'pr2_worms_dict': params.get('pr2_worms_dict', {}),\n",
    "            'assay_rank_info': params.get('assay_rank_info', {})\n",
    "        }\n",
    "        \n",
    "        # Call the main function from the helper script.\n",
    "        # This function is expected to return the *entire* dataframe with taxonomic columns already added/updated.\n",
    "        # This avoids the large memory merge that caused the error previously.\n",
    "        occ_df_matched_with_api = WoRMS_v3_matching.get_worms_match_for_dataframe(\n",
    "            occurrence_df=df_to_match,\n",
    "            params_dict=matching_params,\n",
    "            n_proc=params.get('worms_n_proc', 0)\n",
    "        )\n",
    "    else:\n",
    "        print(f\"API '{api_to_use}' is not yet implemented.\")\n",
    "        occ_df_matched_with_api = df_to_match.copy()\n",
    "\n",
    "    print(f\"\\nTaxonomic matching process via '{api_to_use}' finished.\")\n",
    "\n",
    "    # --- Display Preview ---\n",
    "    if not occ_df_matched_with_api.empty:\n",
    "        print(\"\\nPreview of DataFrame after matching (selected columns):\")\n",
    "        preview_cols = ['eventID', 'occurrenceID', 'verbatimIdentification', 'assay_name', \n",
    "                        'scientificName', 'scientificNameID', 'taxonRank', 'nameAccordingTo', 'match_type_debug'] + DWC_RANKS_STD\n",
    "        display_cols = [col for col in preview_cols if col in occ_df_matched_with_api.columns]\n",
    "        display(occ_df_matched_with_api[display_cols].head())\n",
    "        \n",
    "        if 'match_type_debug' in occ_df_matched_with_api.columns:\n",
    "            print(f\"\\nCounts of '{api_to_use}' match_type_debug:\")\n",
    "            print(occ_df_matched_with_api['match_type_debug'].value_counts(dropna=False))\n",
    "        if 'scientificName' in occ_df_matched_with_api.columns:\n",
    "            print(f\"\\nCounts of resulting '{api_to_use}' scientificName (top 10 unique non-NA values):\")\n",
    "            print(occ_df_matched_with_api['scientificName'].dropna().value_counts().head(10))\n",
    "    else:\n",
    "        print(f\"DataFrame 'occ_df_matched_with_api' is empty after '{api_to_use}' matching attempt.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-Matching Processing and Final Save\n",
    "\n",
    "This cell handles API-specific post-processing. For example, if WoRMS was used, it checks for verbatim strings that were effectively \"Eukaryota\"-only and ensures they are set to \"incertae sedis\" if not already resolved to a lower rank. The temporary `assay_name` and `match_type_debug` columns are then removed, and the file is saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting post-matching processing.\n",
      "  Post-processing (WoRMS): Found 10864 records with 'Eukaryota'-only verbatim strings that matched to high-level Eukaryota or were already 'incertae sedis'. Standardizing to 'incertae sedis'.\n",
      "  Removed temporary columns: ['assay_name', 'match_type_debug']\n",
      "\n",
      "💾 Taxonomically updated FULL DATASET occurrence file 'occurrence_worms_matched.csv' saved to '../processed-v3/occurrence_worms_matched.csv' with 312214 records.\n",
      "  Final columns written (35 total): ['eventID', 'organismQuantity', 'occurrenceID', 'verbatimIdentification', 'kingdom', 'phylum', 'class', 'order', 'family', 'genus', 'species', 'scientificName', 'scientificNameID', 'taxonRank', 'identificationRemarks', 'taxonID', 'basisOfRecord', 'nameAccordingTo', 'organismQuantityType', 'recordedBy', 'materialSampleID', 'sampleSizeValue', 'sampleSizeUnit', 'associatedSequences', 'locationID', 'eventDate', 'minimumDepthInMeters', 'maximumDepthInMeters', 'locality', 'decimalLatitude', 'decimalLongitude', 'geodeticDatum', 'parentEventID', 'datasetID', 'occurrenceStatus']\n",
      "\n",
      "👀 Preview of final taxonomically updated FULL DATASET occurrence data (first 5 rows, selected columns):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eventID</th>\n",
       "      <th>occurrenceID</th>\n",
       "      <th>verbatimIdentification</th>\n",
       "      <th>scientificName</th>\n",
       "      <th>scientificNameID</th>\n",
       "      <th>taxonRank</th>\n",
       "      <th>nameAccordingTo</th>\n",
       "      <th>kingdom</th>\n",
       "      <th>phylum</th>\n",
       "      <th>class</th>\n",
       "      <th>order</th>\n",
       "      <th>family</th>\n",
       "      <th>genus</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GOMECC18S_Plate4_53</td>\n",
       "      <td>GOMECC18S_Plate4_53_occ_36aa75f9b28f5f831c2d631ba65c2bcb</td>\n",
       "      <td>Eukaryota;Obazoa;Opisthokonta;Metazoa;Arthropoda;Crustacea;Maxillopoda</td>\n",
       "      <td>Crustacea</td>\n",
       "      <td>urn:lsid:marinespecies.org:taxname:1066</td>\n",
       "      <td>Subphylum</td>\n",
       "      <td>WoRMS</td>\n",
       "      <td>Animalia</td>\n",
       "      <td>Arthropoda</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GOMECC18S_Plate4_53</td>\n",
       "      <td>GOMECC18S_Plate4_53_occ_4e38e8ced9070952b314e1880bede1ca</td>\n",
       "      <td>Eukaryota;Obazoa;Opisthokonta;Metazoa;Arthropoda;Crustacea;Maxillopoda;Clausocalanus;Clausocalanus_furcatus;</td>\n",
       "      <td>Clausocalanus furcatus</td>\n",
       "      <td>urn:lsid:marinespecies.org:taxname:104503</td>\n",
       "      <td>Species</td>\n",
       "      <td>WoRMS</td>\n",
       "      <td>Animalia</td>\n",
       "      <td>Arthropoda</td>\n",
       "      <td>Copepoda</td>\n",
       "      <td>Calanoida</td>\n",
       "      <td>Clausocalanidae</td>\n",
       "      <td>Clausocalanus</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GOMECC18S_Plate4_53</td>\n",
       "      <td>GOMECC18S_Plate4_53_occ_2a31e5c01634165da99e7381279baa75</td>\n",
       "      <td>Eukaryota;Obazoa;Opisthokonta;Metazoa;Arthropoda;Crustacea;Maxillopoda;Oithona;Oithona_sp.;</td>\n",
       "      <td>Oithona</td>\n",
       "      <td>urn:lsid:marinespecies.org:taxname:106485</td>\n",
       "      <td>Genus</td>\n",
       "      <td>WoRMS</td>\n",
       "      <td>Animalia</td>\n",
       "      <td>Arthropoda</td>\n",
       "      <td>Copepoda</td>\n",
       "      <td>Cyclopoida</td>\n",
       "      <td>Oithonidae</td>\n",
       "      <td>Oithona</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GOMECC18S_Plate4_53</td>\n",
       "      <td>GOMECC18S_Plate4_53_occ_ecee60339b2fb88ea6d1c8d18054bed4</td>\n",
       "      <td>Eukaryota;TSAR;Alveolata;Dinoflagellata;Dinophyceae</td>\n",
       "      <td>Dinophyceae</td>\n",
       "      <td>urn:lsid:marinespecies.org:taxname:19542</td>\n",
       "      <td>Class</td>\n",
       "      <td>WoRMS</td>\n",
       "      <td>Chromista</td>\n",
       "      <td>Myzozoa</td>\n",
       "      <td>Dinophyceae</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GOMECC18S_Plate4_53</td>\n",
       "      <td>GOMECC18S_Plate4_53_occ_fa1f1a97dd4ae7c826009186bad26384</td>\n",
       "      <td>Eukaryota;TSAR;Alveolata;Dinoflagellata;Dinophyceae;Gymnodiniales;Gymnodiniaceae</td>\n",
       "      <td>Gymnodiniaceae</td>\n",
       "      <td>urn:lsid:marinespecies.org:taxname:109410</td>\n",
       "      <td>Family</td>\n",
       "      <td>WoRMS</td>\n",
       "      <td>Chromista</td>\n",
       "      <td>Myzozoa</td>\n",
       "      <td>Dinophyceae</td>\n",
       "      <td>Gymnodiniales</td>\n",
       "      <td>Gymnodiniaceae</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               eventID  \\\n",
       "0  GOMECC18S_Plate4_53   \n",
       "1  GOMECC18S_Plate4_53   \n",
       "2  GOMECC18S_Plate4_53   \n",
       "3  GOMECC18S_Plate4_53   \n",
       "4  GOMECC18S_Plate4_53   \n",
       "\n",
       "                                               occurrenceID  \\\n",
       "0  GOMECC18S_Plate4_53_occ_36aa75f9b28f5f831c2d631ba65c2bcb   \n",
       "1  GOMECC18S_Plate4_53_occ_4e38e8ced9070952b314e1880bede1ca   \n",
       "2  GOMECC18S_Plate4_53_occ_2a31e5c01634165da99e7381279baa75   \n",
       "3  GOMECC18S_Plate4_53_occ_ecee60339b2fb88ea6d1c8d18054bed4   \n",
       "4  GOMECC18S_Plate4_53_occ_fa1f1a97dd4ae7c826009186bad26384   \n",
       "\n",
       "                                                                                         verbatimIdentification  \\\n",
       "0                                        Eukaryota;Obazoa;Opisthokonta;Metazoa;Arthropoda;Crustacea;Maxillopoda   \n",
       "1  Eukaryota;Obazoa;Opisthokonta;Metazoa;Arthropoda;Crustacea;Maxillopoda;Clausocalanus;Clausocalanus_furcatus;   \n",
       "2                   Eukaryota;Obazoa;Opisthokonta;Metazoa;Arthropoda;Crustacea;Maxillopoda;Oithona;Oithona_sp.;   \n",
       "3                                                           Eukaryota;TSAR;Alveolata;Dinoflagellata;Dinophyceae   \n",
       "4                              Eukaryota;TSAR;Alveolata;Dinoflagellata;Dinophyceae;Gymnodiniales;Gymnodiniaceae   \n",
       "\n",
       "           scientificName                           scientificNameID  \\\n",
       "0               Crustacea    urn:lsid:marinespecies.org:taxname:1066   \n",
       "1  Clausocalanus furcatus  urn:lsid:marinespecies.org:taxname:104503   \n",
       "2                 Oithona  urn:lsid:marinespecies.org:taxname:106485   \n",
       "3             Dinophyceae   urn:lsid:marinespecies.org:taxname:19542   \n",
       "4          Gymnodiniaceae  urn:lsid:marinespecies.org:taxname:109410   \n",
       "\n",
       "   taxonRank nameAccordingTo    kingdom      phylum        class  \\\n",
       "0  Subphylum           WoRMS   Animalia  Arthropoda         <NA>   \n",
       "1    Species           WoRMS   Animalia  Arthropoda     Copepoda   \n",
       "2      Genus           WoRMS   Animalia  Arthropoda     Copepoda   \n",
       "3      Class           WoRMS  Chromista     Myzozoa  Dinophyceae   \n",
       "4     Family           WoRMS  Chromista     Myzozoa  Dinophyceae   \n",
       "\n",
       "           order           family          genus species  \n",
       "0           <NA>             <NA>           <NA>    <NA>  \n",
       "1      Calanoida  Clausocalanidae  Clausocalanus    <NA>  \n",
       "2     Cyclopoida       Oithonidae        Oithona    <NA>  \n",
       "3           <NA>             <NA>           <NA>    <NA>  \n",
       "4  Gymnodiniales   Gymnodiniaceae           <NA>    <NA>  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Post-Matching Processing and Final Save\n",
    "\n",
    "if 'occ_df_matched_with_api' not in globals() or occ_df_matched_with_api.empty:\n",
    "    print(\"Skipping post-matching and save: 'occ_df_matched_with_api' is not defined or is empty.\")\n",
    "else:\n",
    "    print(\"Starting post-matching processing.\")\n",
    "    api_source_used = params.get('taxonomic_api_source', 'UnknownAPI') # Get from params\n",
    "    \n",
    "    # --- Handle \"Eukaryota\"-only verbatim strings specifically after WoRMS matching ---\n",
    "    if api_source_used == 'WoRMS':\n",
    "        eukaryota_verbatim_mask = pd.Series([False] * len(occ_df_matched_with_api), index=occ_df_matched_with_api.index)\n",
    "        if 'verbatimIdentification' in occ_df_matched_with_api.columns and \\\n",
    "           'scientificName' in occ_df_matched_with_api.columns and \\\n",
    "           'taxonRank' in occ_df_matched_with_api.columns:\n",
    "\n",
    "            for index, row in occ_df_matched_with_api.iterrows():\n",
    "                vi = str(row.get('verbatimIdentification', '')).strip().lower()\n",
    "                parsed_vi = [term.strip() for term in vi.split(';') if term.strip()]\n",
    "                \n",
    "                is_effectively_eukaryota_only_verbatim = False\n",
    "                if len(parsed_vi) == 1 and parsed_vi[0] == 'eukaryota':\n",
    "                    is_effectively_eukaryota_only_verbatim = True\n",
    "                elif len(parsed_vi) > 0 and parsed_vi[-1] == 'eukaryota': \n",
    "                    preceding_meaningful = [term for term in parsed_vi[:-1] if term and term != 'unassigned']\n",
    "                    if not preceding_meaningful: # If only \"eukaryota\" is meaningful at the end\n",
    "                        is_effectively_eukaryota_only_verbatim = True\n",
    "                \n",
    "                if is_effectively_eukaryota_only_verbatim:\n",
    "                    current_sn = str(row.get('scientificName', '')).lower()\n",
    "                    current_tr = str(row.get('taxonRank', '')).lower()\n",
    "                    high_ranks = ['kingdom', 'superkingdom', 'domain', 'subkingdom', 'no rank', 'unranked', ''] # Include empty/None as high rank\n",
    "                    \n",
    "                    if current_sn == 'eukaryota' and current_tr in high_ranks:\n",
    "                        eukaryota_verbatim_mask[index] = True\n",
    "                    elif current_sn == 'incertae sedis' and is_effectively_eukaryota_only_verbatim:\n",
    "                         eukaryota_verbatim_mask[index] = True\n",
    "\n",
    "\n",
    "            num_eukaryota_verbatim_to_convert = eukaryota_verbatim_mask.sum()\n",
    "            if num_eukaryota_verbatim_to_convert > 0:\n",
    "                print(f\"  Post-processing ({api_source_used}): Found {num_eukaryota_verbatim_to_convert} records with 'Eukaryota'-only verbatim strings that matched to high-level Eukaryota or were already 'incertae sedis'. Standardizing to 'incertae sedis'.\")\n",
    "                occ_df_matched_with_api.loc[eukaryota_verbatim_mask, 'scientificName'] = 'incertae sedis'\n",
    "                occ_df_matched_with_api.loc[eukaryota_verbatim_mask, 'scientificNameID'] = 'urn:lsid:marinespecies.org:taxname:12' # AphiaID for Incertae Sedis in WoRMS\n",
    "                occ_df_matched_with_api.loc[eukaryota_verbatim_mask, 'taxonRank'] = 'no rank'\n",
    "                for rank_col in DWC_RANKS_STD: # Clear out other ranks\n",
    "                    if rank_col in occ_df_matched_with_api.columns:\n",
    "                        occ_df_matched_with_api.loc[eukaryota_verbatim_mask, rank_col] = None \n",
    "                if 'match_type_debug' in occ_df_matched_with_api.columns:\n",
    "                     occ_df_matched_with_api.loc[eukaryota_verbatim_mask, 'match_type_debug'] = 'Standardized_IncertaeSedis_From_EukaryotaOnlyVerbatim'\n",
    "                if 'nameAccordingTo' in occ_df_matched_with_api.columns:\n",
    "                    occ_df_matched_with_api.loc[eukaryota_verbatim_mask, 'nameAccordingTo'] = api_source_used + \"; Local rule for Eukaryota-only verbatim\"\n",
    "\n",
    "\n",
    "    # --- Remove temporary/intermediate columns ---\n",
    "    columns_to_drop_final = []\n",
    "    if 'assay_name' in occ_df_matched_with_api.columns: columns_to_drop_final.append('assay_name')\n",
    "    if 'match_type_debug' in occ_df_matched_with_api.columns: columns_to_drop_final.append('match_type_debug')\n",
    "    \n",
    "    if columns_to_drop_final:\n",
    "        occ_df_matched_with_api.drop(columns=columns_to_drop_final, inplace=True, errors='ignore')\n",
    "        print(f\"  Removed temporary columns: {columns_to_drop_final}\")\n",
    "\n",
    "    # --- Define final column order ---\n",
    "    if 'DESIRED_OCCURRENCE_COLUMNS_IN_ORDER' in globals():\n",
    "        final_dwc_columns_ordered_post_match = [col for col in DESIRED_OCCURRENCE_COLUMNS_IN_ORDER if col != 'assay_name']\n",
    "    else: \n",
    "        print(\"Warning: DESIRED_OCCURRENCE_COLUMNS_IN_ORDER not found. Using columns present in DataFrame.\")\n",
    "        final_dwc_columns_ordered_post_match = occ_df_matched_with_api.columns.tolist()\n",
    "\n",
    "    # Ensure all desired columns exist, add as NA if missing\n",
    "    for col in final_dwc_columns_ordered_post_match:\n",
    "        if col not in occ_df_matched_with_api.columns:\n",
    "            occ_df_matched_with_api[col] = pd.NA \n",
    "            print(f\"  Warning: Final column '{col}' was missing from matched data and added as NA.\")\n",
    "\n",
    "    occ_df_final_output = occ_df_matched_with_api.reindex(columns=final_dwc_columns_ordered_post_match)\n",
    "    \n",
    "    # --- Save the final, taxonomically enriched occurrence file ---\n",
    "    final_occurrence_filename = f\"occurrence_{api_source_used.lower()}_matched.csv\" \n",
    "    \n",
    "    output_directory_for_save = params.get('output_dir', \"../processed-v3/\")\n",
    "    if not isinstance(output_directory_for_save, str): # Safeguard\n",
    "        output_directory_for_save = \"../processed-v3/\"\n",
    "    os.makedirs(output_directory_for_save, exist_ok=True) # Ensure directory exists\n",
    "    \n",
    "    final_output_path = os.path.join(output_directory_for_save, final_occurrence_filename)\n",
    "\n",
    "    try:\n",
    "        occ_df_final_output.to_csv(final_output_path, index=False, na_rep='') \n",
    "        # The `current_run_type` variable is defined in the previous cell and is used here.\n",
    "        print(f\"\\n💾 Taxonomically updated {current_run_type} occurrence file '{final_occurrence_filename}' saved to '{final_output_path}' with {len(occ_df_final_output)} records.\")\n",
    "        print(f\"  Final columns written ({len(occ_df_final_output.columns)} total): {occ_df_final_output.columns.tolist()}\")\n",
    "        \n",
    "        print(f\"\\n👀 Preview of final taxonomically updated {current_run_type} occurrence data (first 5 rows, selected columns):\")\n",
    "        preview_cols_final = ['eventID', 'occurrenceID', 'verbatimIdentification', \n",
    "                              'scientificName', 'scientificNameID', 'taxonRank', 'nameAccordingTo'] + DWC_RANKS_STD\n",
    "        display_cols_final = [col for col in preview_cols_final if col in occ_df_final_output.columns]\n",
    "        display(occ_df_final_output[display_cols_final].head())\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ ERROR saving final taxonomically updated {current_run_type} occurrence file: {str(e)}\")\n",
    "        import traceback\n",
    "        print(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DNA Derived Extension\n",
    "\n",
    "First we clearly define the desired columns for the DNA Derived Extension output file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNA derived extension will have 29 columns\n"
     ]
    }
   ],
   "source": [
    "# Define desired columns for DNA derived extension in output order\n",
    "DESIRED_DNA_DERIVED_COLUMNS = [\n",
    "    'eventID', 'source_mat_id', 'env_broad_scale', 'env_local_scale', 'env_medium', \n",
    "    'samp_vol_we_dna_ext', 'samp_collect_device', 'samp_mat_process', 'size_frac', \n",
    "    'concentration', 'lib_layout', 'seq_meth', 'nucl_acid_ext', 'target_gene', \n",
    "    'target_subfragment', 'pcr_primer_forward', 'pcr_primer_reverse', \n",
    "    'pcr_primer_name_forward', 'pcr_primer_name_reverse', 'pcr_primer_reference', \n",
    "    'pcr_cond', 'nucl_acid_amp', 'ampliconSize', 'otu_seq_comp_appr', 'otu_db', \n",
    "    'occurrenceID', 'DNA_sequence', 'concentrationUnit', 'otu_class_appr'\n",
    "]\n",
    "\n",
    "print(f\"DNA derived extension will have {len(DESIRED_DNA_DERIVED_COLUMNS)} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create foundation from occurrence core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with the Occurrence Core's shape: (312214, 36)\n",
      "DNA derived base shape: (312214, 7)\n",
      "Columns in base: ['eventID', 'parentEventID', 'occurrenceID', 'source_mat_id', 'taxonID', 'assay_name', 'featureID']\n"
     ]
    }
   ],
   "source": [
    "# Start with the occurrence core data (occ_all_final_output) \n",
    "# Get the essential columns for DNA derived extension, and KEEP the correct assay_name column\n",
    "print(f\"Starting with the Occurrence Core's shape: {occ_all_final_output.shape}\")\n",
    "\n",
    "# Create foundation with the columns we need, including the correct assay_name\n",
    "dna_derived_base = occ_all_final_output[['eventID', 'parentEventID', 'occurrenceID', 'materialSampleID', 'taxonID', 'assay_name']].copy()\n",
    "\n",
    "# Rename materialSampleID to source_mat_id for final output\n",
    "dna_derived_base = dna_derived_base.rename(columns={'materialSampleID': 'source_mat_id'})\n",
    "\n",
    "# Extract featureID from taxonID (format: 'ASV:<featureid>')\n",
    "dna_derived_base['featureID'] = dna_derived_base['taxonID'].str.replace('ASV:', '')\n",
    "\n",
    "print(f\"DNA derived base shape: {dna_derived_base.shape}\")\n",
    "print(f\"Columns in base: {list(dna_derived_base.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge with sampleMetadata to get sample collection information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before merge - dna_derived_base shape: (312214, 7)\n",
      "data['sampleMetadata'] shape: (472, 86)\n",
      "\n",
      "After merge with sample metadata - shape: (312214, 92)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eventID</th>\n",
       "      <th>parentEventID</th>\n",
       "      <th>occurrenceID</th>\n",
       "      <th>source_mat_id</th>\n",
       "      <th>taxonID</th>\n",
       "      <th>assay_name</th>\n",
       "      <th>featureID</th>\n",
       "      <th>samp_name</th>\n",
       "      <th>samp_category</th>\n",
       "      <th>materialSampleID</th>\n",
       "      <th>decimalLongitude</th>\n",
       "      <th>decimalLatitude</th>\n",
       "      <th>verbatimSRS</th>\n",
       "      <th>geo_loc_name</th>\n",
       "      <th>eventDate</th>\n",
       "      <th>env_broad_scale</th>\n",
       "      <th>env_local_scale</th>\n",
       "      <th>env_medium</th>\n",
       "      <th>habitat_natural_artificial_0_1</th>\n",
       "      <th>samp_collect_method</th>\n",
       "      <th>samp_collect_device</th>\n",
       "      <th>samp_size</th>\n",
       "      <th>samp_size_unit</th>\n",
       "      <th>samp_store_temp</th>\n",
       "      <th>samp_store_sol</th>\n",
       "      <th>...</th>\n",
       "      <th>omega_arag</th>\n",
       "      <th>pco2</th>\n",
       "      <th>phosphate</th>\n",
       "      <th>phosphate_unit</th>\n",
       "      <th>pressure</th>\n",
       "      <th>pressure_unit</th>\n",
       "      <th>silicate</th>\n",
       "      <th>silicate_unit</th>\n",
       "      <th>tot_alkalinity</th>\n",
       "      <th>tot_alkalinity_unit</th>\n",
       "      <th>transmittance</th>\n",
       "      <th>serial_number</th>\n",
       "      <th>line_id</th>\n",
       "      <th>station_id</th>\n",
       "      <th>ctd_cast_number</th>\n",
       "      <th>ctd_bottle_number</th>\n",
       "      <th>replicate_number</th>\n",
       "      <th>extract_id</th>\n",
       "      <th>extract_plate</th>\n",
       "      <th>extract_well_number</th>\n",
       "      <th>extract_well_position</th>\n",
       "      <th>biosample_accession</th>\n",
       "      <th>organism</th>\n",
       "      <th>dna_yield</th>\n",
       "      <th>dna_yield_unit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GOMECC18S_Plate4_53</td>\n",
       "      <td>GOMECC4_27N_Sta1_DCM_A</td>\n",
       "      <td>GOMECC18S_Plate4_53_occ_36aa75f9b28f5f831c2d631ba65c2bcb</td>\n",
       "      <td>GOMECC4_27N_Sta1_DCM</td>\n",
       "      <td>ASV:36aa75f9b28f5f831c2d631ba65c2bcb</td>\n",
       "      <td>ssu18sv9-emp</td>\n",
       "      <td>36aa75f9b28f5f831c2d631ba65c2bcb</td>\n",
       "      <td>GOMECC4_27N_Sta1_DCM_A</td>\n",
       "      <td>sample</td>\n",
       "      <td>GOMECC4_27N_Sta1_DCM</td>\n",
       "      <td>-79.618</td>\n",
       "      <td>26.997</td>\n",
       "      <td>WGS84</td>\n",
       "      <td>USA: Atlantic Ocean, east of Florida (27 N)</td>\n",
       "      <td>2021-09-14T11:00-04:00</td>\n",
       "      <td>marine biome [ENVO:00000447]</td>\n",
       "      <td>marine photic zone [ENVO:00000209]</td>\n",
       "      <td>sea water [ENVO:00002149]</td>\n",
       "      <td>0</td>\n",
       "      <td>https://zenodo.org/records/14224755 (v1.1.0) protocol_sampling_sterivex_dry.md</td>\n",
       "      <td>Niskin bottle on CTD rosette</td>\n",
       "      <td>1540</td>\n",
       "      <td>mL</td>\n",
       "      <td>-80</td>\n",
       "      <td>none</td>\n",
       "      <td>...</td>\n",
       "      <td>3.805</td>\n",
       "      <td>423</td>\n",
       "      <td>0.0517</td>\n",
       "      <td>µmol/kg</td>\n",
       "      <td>49</td>\n",
       "      <td>dbar</td>\n",
       "      <td>1.05635</td>\n",
       "      <td>µmol/kg</td>\n",
       "      <td>2371</td>\n",
       "      <td>µmol/kg</td>\n",
       "      <td>4.665</td>\n",
       "      <td>GOMECC4_004</td>\n",
       "      <td>27N</td>\n",
       "      <td>Sta1</td>\n",
       "      <td>not provided</td>\n",
       "      <td>14</td>\n",
       "      <td>A</td>\n",
       "      <td>Plate4_53</td>\n",
       "      <td>GOMECC2021_Plate4</td>\n",
       "      <td>53</td>\n",
       "      <td>E7</td>\n",
       "      <td>SAMN37516094</td>\n",
       "      <td>seawater metagenome</td>\n",
       "      <td>223.5</td>\n",
       "      <td>ng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GOMECC18S_Plate4_53</td>\n",
       "      <td>GOMECC4_27N_Sta1_DCM_A</td>\n",
       "      <td>GOMECC18S_Plate4_53_occ_4e38e8ced9070952b314e1880bede1ca</td>\n",
       "      <td>GOMECC4_27N_Sta1_DCM</td>\n",
       "      <td>ASV:4e38e8ced9070952b314e1880bede1ca</td>\n",
       "      <td>ssu18sv9-emp</td>\n",
       "      <td>4e38e8ced9070952b314e1880bede1ca</td>\n",
       "      <td>GOMECC4_27N_Sta1_DCM_A</td>\n",
       "      <td>sample</td>\n",
       "      <td>GOMECC4_27N_Sta1_DCM</td>\n",
       "      <td>-79.618</td>\n",
       "      <td>26.997</td>\n",
       "      <td>WGS84</td>\n",
       "      <td>USA: Atlantic Ocean, east of Florida (27 N)</td>\n",
       "      <td>2021-09-14T11:00-04:00</td>\n",
       "      <td>marine biome [ENVO:00000447]</td>\n",
       "      <td>marine photic zone [ENVO:00000209]</td>\n",
       "      <td>sea water [ENVO:00002149]</td>\n",
       "      <td>0</td>\n",
       "      <td>https://zenodo.org/records/14224755 (v1.1.0) protocol_sampling_sterivex_dry.md</td>\n",
       "      <td>Niskin bottle on CTD rosette</td>\n",
       "      <td>1540</td>\n",
       "      <td>mL</td>\n",
       "      <td>-80</td>\n",
       "      <td>none</td>\n",
       "      <td>...</td>\n",
       "      <td>3.805</td>\n",
       "      <td>423</td>\n",
       "      <td>0.0517</td>\n",
       "      <td>µmol/kg</td>\n",
       "      <td>49</td>\n",
       "      <td>dbar</td>\n",
       "      <td>1.05635</td>\n",
       "      <td>µmol/kg</td>\n",
       "      <td>2371</td>\n",
       "      <td>µmol/kg</td>\n",
       "      <td>4.665</td>\n",
       "      <td>GOMECC4_004</td>\n",
       "      <td>27N</td>\n",
       "      <td>Sta1</td>\n",
       "      <td>not provided</td>\n",
       "      <td>14</td>\n",
       "      <td>A</td>\n",
       "      <td>Plate4_53</td>\n",
       "      <td>GOMECC2021_Plate4</td>\n",
       "      <td>53</td>\n",
       "      <td>E7</td>\n",
       "      <td>SAMN37516094</td>\n",
       "      <td>seawater metagenome</td>\n",
       "      <td>223.5</td>\n",
       "      <td>ng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GOMECC18S_Plate4_53</td>\n",
       "      <td>GOMECC4_27N_Sta1_DCM_A</td>\n",
       "      <td>GOMECC18S_Plate4_53_occ_2a31e5c01634165da99e7381279baa75</td>\n",
       "      <td>GOMECC4_27N_Sta1_DCM</td>\n",
       "      <td>ASV:2a31e5c01634165da99e7381279baa75</td>\n",
       "      <td>ssu18sv9-emp</td>\n",
       "      <td>2a31e5c01634165da99e7381279baa75</td>\n",
       "      <td>GOMECC4_27N_Sta1_DCM_A</td>\n",
       "      <td>sample</td>\n",
       "      <td>GOMECC4_27N_Sta1_DCM</td>\n",
       "      <td>-79.618</td>\n",
       "      <td>26.997</td>\n",
       "      <td>WGS84</td>\n",
       "      <td>USA: Atlantic Ocean, east of Florida (27 N)</td>\n",
       "      <td>2021-09-14T11:00-04:00</td>\n",
       "      <td>marine biome [ENVO:00000447]</td>\n",
       "      <td>marine photic zone [ENVO:00000209]</td>\n",
       "      <td>sea water [ENVO:00002149]</td>\n",
       "      <td>0</td>\n",
       "      <td>https://zenodo.org/records/14224755 (v1.1.0) protocol_sampling_sterivex_dry.md</td>\n",
       "      <td>Niskin bottle on CTD rosette</td>\n",
       "      <td>1540</td>\n",
       "      <td>mL</td>\n",
       "      <td>-80</td>\n",
       "      <td>none</td>\n",
       "      <td>...</td>\n",
       "      <td>3.805</td>\n",
       "      <td>423</td>\n",
       "      <td>0.0517</td>\n",
       "      <td>µmol/kg</td>\n",
       "      <td>49</td>\n",
       "      <td>dbar</td>\n",
       "      <td>1.05635</td>\n",
       "      <td>µmol/kg</td>\n",
       "      <td>2371</td>\n",
       "      <td>µmol/kg</td>\n",
       "      <td>4.665</td>\n",
       "      <td>GOMECC4_004</td>\n",
       "      <td>27N</td>\n",
       "      <td>Sta1</td>\n",
       "      <td>not provided</td>\n",
       "      <td>14</td>\n",
       "      <td>A</td>\n",
       "      <td>Plate4_53</td>\n",
       "      <td>GOMECC2021_Plate4</td>\n",
       "      <td>53</td>\n",
       "      <td>E7</td>\n",
       "      <td>SAMN37516094</td>\n",
       "      <td>seawater metagenome</td>\n",
       "      <td>223.5</td>\n",
       "      <td>ng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GOMECC18S_Plate4_53</td>\n",
       "      <td>GOMECC4_27N_Sta1_DCM_A</td>\n",
       "      <td>GOMECC18S_Plate4_53_occ_ecee60339b2fb88ea6d1c8d18054bed4</td>\n",
       "      <td>GOMECC4_27N_Sta1_DCM</td>\n",
       "      <td>ASV:ecee60339b2fb88ea6d1c8d18054bed4</td>\n",
       "      <td>ssu18sv9-emp</td>\n",
       "      <td>ecee60339b2fb88ea6d1c8d18054bed4</td>\n",
       "      <td>GOMECC4_27N_Sta1_DCM_A</td>\n",
       "      <td>sample</td>\n",
       "      <td>GOMECC4_27N_Sta1_DCM</td>\n",
       "      <td>-79.618</td>\n",
       "      <td>26.997</td>\n",
       "      <td>WGS84</td>\n",
       "      <td>USA: Atlantic Ocean, east of Florida (27 N)</td>\n",
       "      <td>2021-09-14T11:00-04:00</td>\n",
       "      <td>marine biome [ENVO:00000447]</td>\n",
       "      <td>marine photic zone [ENVO:00000209]</td>\n",
       "      <td>sea water [ENVO:00002149]</td>\n",
       "      <td>0</td>\n",
       "      <td>https://zenodo.org/records/14224755 (v1.1.0) protocol_sampling_sterivex_dry.md</td>\n",
       "      <td>Niskin bottle on CTD rosette</td>\n",
       "      <td>1540</td>\n",
       "      <td>mL</td>\n",
       "      <td>-80</td>\n",
       "      <td>none</td>\n",
       "      <td>...</td>\n",
       "      <td>3.805</td>\n",
       "      <td>423</td>\n",
       "      <td>0.0517</td>\n",
       "      <td>µmol/kg</td>\n",
       "      <td>49</td>\n",
       "      <td>dbar</td>\n",
       "      <td>1.05635</td>\n",
       "      <td>µmol/kg</td>\n",
       "      <td>2371</td>\n",
       "      <td>µmol/kg</td>\n",
       "      <td>4.665</td>\n",
       "      <td>GOMECC4_004</td>\n",
       "      <td>27N</td>\n",
       "      <td>Sta1</td>\n",
       "      <td>not provided</td>\n",
       "      <td>14</td>\n",
       "      <td>A</td>\n",
       "      <td>Plate4_53</td>\n",
       "      <td>GOMECC2021_Plate4</td>\n",
       "      <td>53</td>\n",
       "      <td>E7</td>\n",
       "      <td>SAMN37516094</td>\n",
       "      <td>seawater metagenome</td>\n",
       "      <td>223.5</td>\n",
       "      <td>ng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GOMECC18S_Plate4_53</td>\n",
       "      <td>GOMECC4_27N_Sta1_DCM_A</td>\n",
       "      <td>GOMECC18S_Plate4_53_occ_fa1f1a97dd4ae7c826009186bad26384</td>\n",
       "      <td>GOMECC4_27N_Sta1_DCM</td>\n",
       "      <td>ASV:fa1f1a97dd4ae7c826009186bad26384</td>\n",
       "      <td>ssu18sv9-emp</td>\n",
       "      <td>fa1f1a97dd4ae7c826009186bad26384</td>\n",
       "      <td>GOMECC4_27N_Sta1_DCM_A</td>\n",
       "      <td>sample</td>\n",
       "      <td>GOMECC4_27N_Sta1_DCM</td>\n",
       "      <td>-79.618</td>\n",
       "      <td>26.997</td>\n",
       "      <td>WGS84</td>\n",
       "      <td>USA: Atlantic Ocean, east of Florida (27 N)</td>\n",
       "      <td>2021-09-14T11:00-04:00</td>\n",
       "      <td>marine biome [ENVO:00000447]</td>\n",
       "      <td>marine photic zone [ENVO:00000209]</td>\n",
       "      <td>sea water [ENVO:00002149]</td>\n",
       "      <td>0</td>\n",
       "      <td>https://zenodo.org/records/14224755 (v1.1.0) protocol_sampling_sterivex_dry.md</td>\n",
       "      <td>Niskin bottle on CTD rosette</td>\n",
       "      <td>1540</td>\n",
       "      <td>mL</td>\n",
       "      <td>-80</td>\n",
       "      <td>none</td>\n",
       "      <td>...</td>\n",
       "      <td>3.805</td>\n",
       "      <td>423</td>\n",
       "      <td>0.0517</td>\n",
       "      <td>µmol/kg</td>\n",
       "      <td>49</td>\n",
       "      <td>dbar</td>\n",
       "      <td>1.05635</td>\n",
       "      <td>µmol/kg</td>\n",
       "      <td>2371</td>\n",
       "      <td>µmol/kg</td>\n",
       "      <td>4.665</td>\n",
       "      <td>GOMECC4_004</td>\n",
       "      <td>27N</td>\n",
       "      <td>Sta1</td>\n",
       "      <td>not provided</td>\n",
       "      <td>14</td>\n",
       "      <td>A</td>\n",
       "      <td>Plate4_53</td>\n",
       "      <td>GOMECC2021_Plate4</td>\n",
       "      <td>53</td>\n",
       "      <td>E7</td>\n",
       "      <td>SAMN37516094</td>\n",
       "      <td>seawater metagenome</td>\n",
       "      <td>223.5</td>\n",
       "      <td>ng</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 92 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               eventID           parentEventID  \\\n",
       "0  GOMECC18S_Plate4_53  GOMECC4_27N_Sta1_DCM_A   \n",
       "1  GOMECC18S_Plate4_53  GOMECC4_27N_Sta1_DCM_A   \n",
       "2  GOMECC18S_Plate4_53  GOMECC4_27N_Sta1_DCM_A   \n",
       "3  GOMECC18S_Plate4_53  GOMECC4_27N_Sta1_DCM_A   \n",
       "4  GOMECC18S_Plate4_53  GOMECC4_27N_Sta1_DCM_A   \n",
       "\n",
       "                                               occurrenceID  \\\n",
       "0  GOMECC18S_Plate4_53_occ_36aa75f9b28f5f831c2d631ba65c2bcb   \n",
       "1  GOMECC18S_Plate4_53_occ_4e38e8ced9070952b314e1880bede1ca   \n",
       "2  GOMECC18S_Plate4_53_occ_2a31e5c01634165da99e7381279baa75   \n",
       "3  GOMECC18S_Plate4_53_occ_ecee60339b2fb88ea6d1c8d18054bed4   \n",
       "4  GOMECC18S_Plate4_53_occ_fa1f1a97dd4ae7c826009186bad26384   \n",
       "\n",
       "          source_mat_id                               taxonID    assay_name  \\\n",
       "0  GOMECC4_27N_Sta1_DCM  ASV:36aa75f9b28f5f831c2d631ba65c2bcb  ssu18sv9-emp   \n",
       "1  GOMECC4_27N_Sta1_DCM  ASV:4e38e8ced9070952b314e1880bede1ca  ssu18sv9-emp   \n",
       "2  GOMECC4_27N_Sta1_DCM  ASV:2a31e5c01634165da99e7381279baa75  ssu18sv9-emp   \n",
       "3  GOMECC4_27N_Sta1_DCM  ASV:ecee60339b2fb88ea6d1c8d18054bed4  ssu18sv9-emp   \n",
       "4  GOMECC4_27N_Sta1_DCM  ASV:fa1f1a97dd4ae7c826009186bad26384  ssu18sv9-emp   \n",
       "\n",
       "                          featureID               samp_name samp_category  \\\n",
       "0  36aa75f9b28f5f831c2d631ba65c2bcb  GOMECC4_27N_Sta1_DCM_A        sample   \n",
       "1  4e38e8ced9070952b314e1880bede1ca  GOMECC4_27N_Sta1_DCM_A        sample   \n",
       "2  2a31e5c01634165da99e7381279baa75  GOMECC4_27N_Sta1_DCM_A        sample   \n",
       "3  ecee60339b2fb88ea6d1c8d18054bed4  GOMECC4_27N_Sta1_DCM_A        sample   \n",
       "4  fa1f1a97dd4ae7c826009186bad26384  GOMECC4_27N_Sta1_DCM_A        sample   \n",
       "\n",
       "       materialSampleID decimalLongitude decimalLatitude verbatimSRS  \\\n",
       "0  GOMECC4_27N_Sta1_DCM          -79.618          26.997       WGS84   \n",
       "1  GOMECC4_27N_Sta1_DCM          -79.618          26.997       WGS84   \n",
       "2  GOMECC4_27N_Sta1_DCM          -79.618          26.997       WGS84   \n",
       "3  GOMECC4_27N_Sta1_DCM          -79.618          26.997       WGS84   \n",
       "4  GOMECC4_27N_Sta1_DCM          -79.618          26.997       WGS84   \n",
       "\n",
       "                                  geo_loc_name               eventDate  \\\n",
       "0  USA: Atlantic Ocean, east of Florida (27 N)  2021-09-14T11:00-04:00   \n",
       "1  USA: Atlantic Ocean, east of Florida (27 N)  2021-09-14T11:00-04:00   \n",
       "2  USA: Atlantic Ocean, east of Florida (27 N)  2021-09-14T11:00-04:00   \n",
       "3  USA: Atlantic Ocean, east of Florida (27 N)  2021-09-14T11:00-04:00   \n",
       "4  USA: Atlantic Ocean, east of Florida (27 N)  2021-09-14T11:00-04:00   \n",
       "\n",
       "                env_broad_scale                     env_local_scale  \\\n",
       "0  marine biome [ENVO:00000447]  marine photic zone [ENVO:00000209]   \n",
       "1  marine biome [ENVO:00000447]  marine photic zone [ENVO:00000209]   \n",
       "2  marine biome [ENVO:00000447]  marine photic zone [ENVO:00000209]   \n",
       "3  marine biome [ENVO:00000447]  marine photic zone [ENVO:00000209]   \n",
       "4  marine biome [ENVO:00000447]  marine photic zone [ENVO:00000209]   \n",
       "\n",
       "                  env_medium  habitat_natural_artificial_0_1  \\\n",
       "0  sea water [ENVO:00002149]                               0   \n",
       "1  sea water [ENVO:00002149]                               0   \n",
       "2  sea water [ENVO:00002149]                               0   \n",
       "3  sea water [ENVO:00002149]                               0   \n",
       "4  sea water [ENVO:00002149]                               0   \n",
       "\n",
       "                                                              samp_collect_method  \\\n",
       "0  https://zenodo.org/records/14224755 (v1.1.0) protocol_sampling_sterivex_dry.md   \n",
       "1  https://zenodo.org/records/14224755 (v1.1.0) protocol_sampling_sterivex_dry.md   \n",
       "2  https://zenodo.org/records/14224755 (v1.1.0) protocol_sampling_sterivex_dry.md   \n",
       "3  https://zenodo.org/records/14224755 (v1.1.0) protocol_sampling_sterivex_dry.md   \n",
       "4  https://zenodo.org/records/14224755 (v1.1.0) protocol_sampling_sterivex_dry.md   \n",
       "\n",
       "            samp_collect_device samp_size samp_size_unit samp_store_temp  \\\n",
       "0  Niskin bottle on CTD rosette      1540             mL             -80   \n",
       "1  Niskin bottle on CTD rosette      1540             mL             -80   \n",
       "2  Niskin bottle on CTD rosette      1540             mL             -80   \n",
       "3  Niskin bottle on CTD rosette      1540             mL             -80   \n",
       "4  Niskin bottle on CTD rosette      1540             mL             -80   \n",
       "\n",
       "  samp_store_sol  ... omega_arag pco2 phosphate phosphate_unit pressure  \\\n",
       "0           none  ...      3.805  423    0.0517        µmol/kg       49   \n",
       "1           none  ...      3.805  423    0.0517        µmol/kg       49   \n",
       "2           none  ...      3.805  423    0.0517        µmol/kg       49   \n",
       "3           none  ...      3.805  423    0.0517        µmol/kg       49   \n",
       "4           none  ...      3.805  423    0.0517        µmol/kg       49   \n",
       "\n",
       "  pressure_unit silicate silicate_unit tot_alkalinity tot_alkalinity_unit  \\\n",
       "0          dbar  1.05635       µmol/kg           2371             µmol/kg   \n",
       "1          dbar  1.05635       µmol/kg           2371             µmol/kg   \n",
       "2          dbar  1.05635       µmol/kg           2371             µmol/kg   \n",
       "3          dbar  1.05635       µmol/kg           2371             µmol/kg   \n",
       "4          dbar  1.05635       µmol/kg           2371             µmol/kg   \n",
       "\n",
       "  transmittance serial_number line_id station_id ctd_cast_number  \\\n",
       "0         4.665   GOMECC4_004     27N       Sta1    not provided   \n",
       "1         4.665   GOMECC4_004     27N       Sta1    not provided   \n",
       "2         4.665   GOMECC4_004     27N       Sta1    not provided   \n",
       "3         4.665   GOMECC4_004     27N       Sta1    not provided   \n",
       "4         4.665   GOMECC4_004     27N       Sta1    not provided   \n",
       "\n",
       "  ctd_bottle_number replicate_number extract_id      extract_plate  \\\n",
       "0                14                A  Plate4_53  GOMECC2021_Plate4   \n",
       "1                14                A  Plate4_53  GOMECC2021_Plate4   \n",
       "2                14                A  Plate4_53  GOMECC2021_Plate4   \n",
       "3                14                A  Plate4_53  GOMECC2021_Plate4   \n",
       "4                14                A  Plate4_53  GOMECC2021_Plate4   \n",
       "\n",
       "  extract_well_number extract_well_position biosample_accession  \\\n",
       "0                  53                    E7        SAMN37516094   \n",
       "1                  53                    E7        SAMN37516094   \n",
       "2                  53                    E7        SAMN37516094   \n",
       "3                  53                    E7        SAMN37516094   \n",
       "4                  53                    E7        SAMN37516094   \n",
       "\n",
       "              organism dna_yield dna_yield_unit  \n",
       "0  seawater metagenome     223.5             ng  \n",
       "1  seawater metagenome     223.5             ng  \n",
       "2  seawater metagenome     223.5             ng  \n",
       "3  seawater metagenome     223.5             ng  \n",
       "4  seawater metagenome     223.5             ng  \n",
       "\n",
       "[5 rows x 92 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge with sample metadata to get environmental and sample processing data\n",
    "# Merge on: parentEventID (from dna_derived_base) = samp_name (from data['sampleMetadata'])\n",
    "\n",
    "print(f\"Before merge - dna_derived_base shape: {dna_derived_base.shape}\")\n",
    "print(f\"data['sampleMetadata'] shape: {data['sampleMetadata'].shape}\")\n",
    "\n",
    "# Perform the merge. Pandas will add suffixes _x and _y for conflicting columns like 'assay_name'.\n",
    "dna_derived_with_sample = dna_derived_base.merge(\n",
    "    data['sampleMetadata'], \n",
    "    left_on='parentEventID', \n",
    "    right_on='samp_name', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# FIX: The correct assay_name (from dna_derived_base) is now 'assay_name_x'.\n",
    "# We rename it back to 'assay_name' and drop the useless one from the merge ('assay_name_y').\n",
    "if 'assay_name_x' in dna_derived_with_sample.columns:\n",
    "    dna_derived_with_sample.rename(columns={'assay_name_x': 'assay_name'}, inplace=True)\n",
    "    if 'assay_name_y' in dna_derived_with_sample.columns:\n",
    "        dna_derived_with_sample.drop(columns=['assay_name_y'], inplace=True)\n",
    "\n",
    "print(f\"\\nAfter merge with sample metadata - shape: {dna_derived_with_sample.shape}\")\n",
    "dna_derived_with_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add projectMetadata fields with proper project_level logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current shape: (312214, 92)\n",
      "All assays in this submission: ['ssu18sv9-emp' 'ssu16sv4v5-emp']\n",
      "\n",
      "\n",
      "Processing projectMetadata fields:\n",
      "lib_layout\n",
      "  -> Assigned project_level value 'paired end' to all rows.\n",
      "instrument\n",
      "  -> Using assay-specific values:\n",
      "  -> Assigned values: ['Illumina MiSeq [OBI_0002003]']\n",
      "target_gene\n",
      "  -> Using assay-specific values:\n",
      "  -> Assigned values: ['18S rRNA (SSU eukaryote)', '16S rRNA (SSU prokaryote)']\n",
      "target_subfragment\n",
      "  -> Using assay-specific values:\n",
      "  -> Assigned values: ['V9', 'V4-V5']\n",
      "pcr_primer_forward\n",
      "  -> Using assay-specific values:\n",
      "  -> Assigned values: ['GTACACACCGCCCGTC', 'GTGYCAGCMGCCGCGGTAA']\n",
      "pcr_primer_reverse\n",
      "  -> Using assay-specific values:\n",
      "  -> Assigned values: ['TGATCCTTCTGCAGGTTCACCTAC', 'CCGYCAATTYMTTTRAGTTT']\n",
      "pcr_primer_name_forward\n",
      "  -> Using assay-specific values:\n",
      "  -> Assigned values: ['1391f', '515F-Y']\n",
      "pcr_primer_name_reverse\n",
      "  -> Using assay-specific values:\n",
      "  -> Assigned values: ['EukBr', '926R']\n",
      "pcr_primer_reference_forward\n",
      "  -> Using assay-specific values:\n",
      "  -> Assigned values: ['10.1371/journal.pone.0006372', '10.1111/1462-2920.13023']\n",
      "pcr_cond\n",
      "  -> Using assay-specific values:\n",
      "  -> Assigned values: ['initial denaturation:94_3;denaturation:94_0.75;annealing:65_0.25;57_0.5;elongation:72_1.5;final elongation:72_10;35', 'initial denaturation:95_2;denaturation:95_0.75;annealing:50_0.75;elongation:68_1.5;final elongation:68_5;25']\n",
      "nucl_acid_amp\n",
      "  -> Using assay-specific values:\n",
      "  -> Assigned values: ['https://doi.org/10.1111/1462-2920.13023']\n",
      "ampliconSize\n",
      "  -> Using assay-specific values:\n",
      "  -> Assigned values: [np.int64(260), np.int64(411)]\n",
      "--------------------------------------------------\n",
      "Finished processing. Final shape: (312214, 104)\n",
      "Columns added/updated: ['lib_layout', 'instrument', 'target_gene', 'target_subfragment', 'pcr_primer_forward', 'pcr_primer_reverse', 'pcr_primer_name_forward', 'pcr_primer_name_reverse', 'pcr_primer_reference_forward', 'pcr_cond', 'nucl_acid_amp', 'ampliconSize']\n"
     ]
    }
   ],
   "source": [
    "# Handle projectMetadata in long format using the now-correct assay_name column\n",
    "\n",
    "print(f\"Current shape: {dna_derived_with_sample.shape}\")\n",
    "print(\"All assays in this submission:\", dna_derived_with_sample['assay_name'].unique())\n",
    "print(\"\\n\")\n",
    "\n",
    "# Fields to populate from projectMetadata\n",
    "project_fields_needed = [\n",
    "    'lib_layout', 'instrument', 'target_gene', 'target_subfragment', \n",
    "    'pcr_primer_forward', 'pcr_primer_reverse', 'pcr_primer_name_forward', \n",
    "    'pcr_primer_name_reverse', 'pcr_primer_reference_forward', 'pcr_cond', \n",
    "    'nucl_acid_amp', 'ampliconSize'\n",
    "]\n",
    "\n",
    "# For each field, add it to our dataframe\n",
    "print(\"Processing projectMetadata fields:\")\n",
    "for field in project_fields_needed:\n",
    "    print(f\"{field}\")\n",
    "    \n",
    "    field_row = data['projectMetadata'][data['projectMetadata']['term_name'] == field]\n",
    "    \n",
    "    if not field_row.empty:\n",
    "        field_row = field_row.iloc[0]\n",
    "        project_level_val = field_row.get('project_level')\n",
    "        \n",
    "        # Check if a valid project_level value exists\n",
    "        if pd.notna(project_level_val) and str(project_level_val).strip() and str(project_level_val).lower() != 'nan':\n",
    "            dna_derived_with_sample[field] = project_level_val\n",
    "            print(f\"  -> Assigned project_level value '{project_level_val}' to all rows.\")\n",
    "        else:\n",
    "            # Use assay-specific values\n",
    "            print(f\"  -> Using assay-specific values:\")\n",
    "            \n",
    "            # Create a dictionary mapping each assay name to its corresponding value for the current field\n",
    "            assay_value_map = {assay: field_row.get(assay) for assay in dna_derived_with_sample['assay_name'].unique()}\n",
    "            \n",
    "            # Use the .map() function for an efficient lookup based on the now-correct 'assay_name' column\n",
    "            dna_derived_with_sample[field] = dna_derived_with_sample['assay_name'].map(assay_value_map)\n",
    "            \n",
    "            # Show the unique values that were assigned to the column for verification\n",
    "            assigned_values = dna_derived_with_sample[field].dropna().unique()\n",
    "            print(f\"  -> Assigned values: {list(assigned_values)}\")\n",
    "    else:\n",
    "        print(f\"WARNING: {field} not found in projectMetadata, column will be empty.\")\n",
    "        dna_derived_with_sample[field] = None\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"Finished processing. Final shape: {dna_derived_with_sample.shape}\")\n",
    "print(\"Columns added/updated:\", [col for col in project_fields_needed if col in dna_derived_with_sample.columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add analysisMetadata fields using occurrence mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available variables to check for analysis mapping:\n",
      "- all_processed_occurrence_dfs: True\n",
      "Number of dataframes: 3\n",
      "Analysis run order: ['gomecc4_18s_p1-6_v2024.10_241122', 'gomecc4_16s_p3-6_v2024.10_241122', 'gomecc4_16s_p1-2_v2024.10_241122']\n",
      "DataFrame 0: 147083 rows -> gomecc4_18s_p1-6_v2024.10_241122\n",
      "  Rows 0 to 147082 (assay: ssu18sv9-emp)\n",
      "    otu_seq_comp_appr: qiime2-2021.2; naive bayes classifier; scikit-learn 0.23.1\n",
      "    otu_db: PR2 v5.0.1; V9 1391f-1510r region; 10.5281/zenodo.8392706\n",
      "    otu_clust_tool: Tourmaline; qiime2-2021.2; dada2\n",
      "DataFrame 1: 118671 rows -> gomecc4_16s_p3-6_v2024.10_241122\n",
      "  Rows 147083 to 265753 (assay: ssu16sv4v5-emp)\n",
      "    otu_seq_comp_appr: qiime2-2021.2; naive bayes classifier; scikit-learn 0.23.1\n",
      "    otu_db: Silva SSU Ref NR 99 v138.1; 515f-926r region; 10.5281/zenodo.8392695\n",
      "    otu_clust_tool: Tourmaline; qiime2-2021.2; dada2\n",
      "DataFrame 2: 46460 rows -> gomecc4_16s_p1-2_v2024.10_241122\n",
      "  Rows 265754 to 312213 (assay: ssu16sv4v5-emp)\n",
      "    otu_seq_comp_appr: Tourmaline; qiime2-2021.2; naive bayes classifier; scikit-learn 0.23.1\n",
      "    otu_db: Silva SSU Ref NR 99 v138.1; 515f-926r region; 10.5281/zenodo.8392695\n",
      "    otu_clust_tool: Tourmaline; qiime2-2021.2; dada2\n",
      "\n",
      "After adding analysis fields - shape: (312214, 107)\n"
     ]
    }
   ],
   "source": [
    "# Use the individual dataframes that were combined to create the occurrence core\n",
    "# Map them to analysis runs using the order from params['datafiles']\n",
    "\n",
    "print(\"Available variables to check for analysis mapping:\")\n",
    "print(\"- all_processed_occurrence_dfs:\", 'all_processed_occurrence_dfs' in locals())\n",
    "\n",
    "if 'all_processed_occurrence_dfs' in locals():\n",
    "    analysis_run_order = list(params['datafiles'].keys())\n",
    "    print(f\"Number of dataframes: {len(all_processed_occurrence_dfs)}\")\n",
    "    print(f\"Analysis run order: {analysis_run_order}\")\n",
    "    \n",
    "    # Create a mapping of row indices to analysis runs\n",
    "    analysis_fields_needed = ['otu_seq_comp_appr', 'otu_db', 'otu_clust_tool']\n",
    "    \n",
    "    # Initialize columns\n",
    "    for field in analysis_fields_needed:\n",
    "        dna_derived_with_sample[field] = None\n",
    "    \n",
    "    current_row_start = 0\n",
    "    for i, df in enumerate(all_processed_occurrence_dfs):\n",
    "        if i >= len(analysis_run_order):\n",
    "            print(f\"Warning: More dataframes than analysis runs defined\")\n",
    "            break\n",
    "            \n",
    "        analysis_run_name = analysis_run_order[i]\n",
    "        print(f\"DataFrame {i}: {len(df)} rows -> {analysis_run_name}\")\n",
    "        \n",
    "        # Find which assay this analysis run belongs to by checking the data structure\n",
    "        assay_key = None\n",
    "        for assay_name, analysis_runs in data['analysis_data_by_assay'].items():\n",
    "            if analysis_run_name in analysis_runs:\n",
    "                assay_key = assay_name\n",
    "                break\n",
    "        \n",
    "        if not assay_key:\n",
    "            print(f\"  Warning: Could not find assay for {analysis_run_name}\")\n",
    "            current_row_start += len(df)\n",
    "            continue\n",
    "            \n",
    "        current_row_end = current_row_start + len(df)\n",
    "        print(f\"  Rows {current_row_start} to {current_row_end-1} (assay: {assay_key})\")\n",
    "        \n",
    "        # Get analysis metadata for this run\n",
    "        if analysis_run_name in data['analysis_data_by_assay'][assay_key]:\n",
    "            analysis_df = data['analysis_data_by_assay'][assay_key][analysis_run_name]\n",
    "            \n",
    "            for field in analysis_fields_needed:\n",
    "                field_row = analysis_df[analysis_df['term_name'] == field]\n",
    "                if not field_row.empty:\n",
    "                    field_value = field_row.iloc[0]['values']\n",
    "                    # Apply to the specific row range\n",
    "                    dna_derived_with_sample.loc[current_row_start:current_row_end-1, field] = field_value\n",
    "                    print(f\"    {field}: {field_value}\")\n",
    "        \n",
    "        current_row_start = current_row_end\n",
    "    \n",
    "    print(f\"\\nAfter adding analysis fields - shape: {dna_derived_with_sample.shape}\")\n",
    "else:\n",
    "    print(\"Required variables not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge DNA sequences using featureID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging DNA sequences from raw taxonomy files...\n",
      "Found taxonomy data for gomecc4_18s_p1-6_v2024.10_241122: (24473, 15)\n",
      "Found taxonomy data for gomecc4_16s_p3-6_v2024.10_241122: (49523, 13)\n",
      "Found taxonomy data for gomecc4_16s_p1-2_v2024.10_241122: (19540, 13)\n",
      "Combined taxonomy data shape: (93536, 16)\n",
      "Using feature column: featureid\n",
      "Using DNA sequence column: dna_sequence\n",
      "Available columns: ['featureid', 'dna_sequence', 'taxonomy', 'verbatimIdentification', 'kingdom', 'supergroup', 'division', 'subdivision', 'class', 'order', 'family', 'genus', 'species', 'Confidence', 'analysis_run', 'phylum']\n",
      "After removing duplicate featureIDs: (89631, 2)\n",
      "Before DNA merge: (312214, 107)\n",
      "After merging DNA sequences - shape: (312214, 109)\n",
      "Rows missing DNA sequences: 0/312214\n"
     ]
    }
   ],
   "source": [
    "# Merge DNA sequences from raw taxonomy files - fix to prevent row explosion\n",
    "print(\"Merging DNA sequences from raw taxonomy files...\")\n",
    "\n",
    "# Use the raw_data_tables that were loaded earlier\n",
    "all_tax_data = []\n",
    "if 'raw_data_tables' in locals() and raw_data_tables:\n",
    "    for analysis_run_name, data_dict in raw_data_tables.items():\n",
    "        if 'taxonomy' in data_dict:\n",
    "            tax_data = data_dict['taxonomy'].copy()\n",
    "            tax_data['analysis_run'] = analysis_run_name  # Track source\n",
    "            all_tax_data.append(tax_data)\n",
    "            print(f\"Found taxonomy data for {analysis_run_name}: {tax_data.shape}\")\n",
    "\n",
    "if all_tax_data:\n",
    "    combined_tax_data = pd.concat(all_tax_data, ignore_index=True)\n",
    "    print(f\"Combined taxonomy data shape: {combined_tax_data.shape}\")\n",
    "    \n",
    "    # Find the column containing DNA sequences and feature IDs\n",
    "    feature_col = None\n",
    "    dna_col = None\n",
    "    \n",
    "    for col in combined_tax_data.columns:\n",
    "        if 'featureid' in col.lower() or col.lower() == 'otu id':\n",
    "            feature_col = col\n",
    "        elif 'dna_sequence' in col.lower() or 'sequence' in col.lower():\n",
    "            dna_col = col\n",
    "    \n",
    "    print(f\"Using feature column: {feature_col}\")\n",
    "    print(f\"Using DNA sequence column: {dna_col}\")\n",
    "    print(f\"Available columns: {list(combined_tax_data.columns)}\")\n",
    "    \n",
    "    if feature_col and dna_col:\n",
    "        # CRITICAL FIX: Remove duplicates from taxonomy data BEFORE merging\n",
    "        seq_data = combined_tax_data[[feature_col, dna_col]].drop_duplicates(subset=[feature_col])\n",
    "        print(f\"After removing duplicate featureIDs: {seq_data.shape}\")\n",
    "        \n",
    "        # Now merge (should be one-to-one)\n",
    "        print(f\"Before DNA merge: {dna_derived_with_sample.shape}\")\n",
    "        dna_derived_with_sequences = dna_derived_with_sample.merge(\n",
    "            seq_data, \n",
    "            left_on='featureID', \n",
    "            right_on=feature_col, \n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        # Rename the DNA sequence column to match our desired output\n",
    "        dna_derived_with_sequences = dna_derived_with_sequences.rename(columns={dna_col: 'DNA_sequence'})\n",
    "        \n",
    "        print(f\"After merging DNA sequences - shape: {dna_derived_with_sequences.shape}\")\n",
    "        print(f\"Rows missing DNA sequences: {dna_derived_with_sequences['DNA_sequence'].isna().sum()}/{len(dna_derived_with_sequences)}\")\n",
    "        \n",
    "        # Drop the extra feature column from the merge if it's different from our featureID\n",
    "        if feature_col in dna_derived_with_sequences.columns and feature_col != 'featureID':\n",
    "            dna_derived_with_sequences = dna_derived_with_sequences.drop(columns=[feature_col])\n",
    "    else:\n",
    "        print(\"Could not find required columns for DNA sequence merging\")\n",
    "        print(\"Available columns in taxonomy data:\", list(combined_tax_data.columns))\n",
    "        dna_derived_with_sequences = dna_derived_with_sample.copy()\n",
    "        dna_derived_with_sequences['DNA_sequence'] = None\n",
    "else:\n",
    "    print(\"No taxonomy data found in raw_data_tables\")\n",
    "    dna_derived_with_sequences = dna_derived_with_sample.copy()\n",
    "    dna_derived_with_sequences['DNA_sequence'] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply final mapping and create output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying Darwin Core mappings and creating final DNA derived extension...\n",
      "Available Darwin Core mappings for dnaDerived: 27\n",
      "\n",
      "\n",
      "Field mappings to apply: 27\n",
      "Mapped env_broad_scale → env_broad_scale\n",
      "Mapped env_local_scale → env_local_scale\n",
      "Mapped env_medium → env_medium\n",
      "Mapped samp_collect_device → samp_collect_device\n",
      "Mapped samp_mat_process → samp_mat_process\n",
      "Mapped size_frac → size_frac\n",
      "Mapped samp_vol_we_dna_ext → samp_vol_we_dna_ext\n",
      "Mapped nucl_acid_ext → nucl_acid_ext\n",
      "Mapped concentration → concentration\n",
      "Mapped concentration_unit → concentrationUnit\n",
      "Mapped target_gene → target_gene\n",
      "Mapped target_subfragment → target_subfragment\n",
      "Mapped ampliconSize → ampliconSize\n",
      "Mapped pcr_primer_forward → pcr_primer_forward\n",
      "Mapped pcr_primer_reverse → pcr_primer_reverse\n",
      "Mapped pcr_primer_name_forward → pcr_primer_name_forward\n",
      "Mapped pcr_primer_name_reverse → pcr_primer_name_reverse\n",
      "Mapped pcr_primer_reference_forward → pcr_primer_reference\n",
      "Mapped nucl_acid_amp → nucl_acid_amp\n",
      "Mapped pcr_cond → pcr_cond\n",
      "Mapped instrument → seq_meth\n",
      "Mapped lib_layout → lib_layout\n",
      "Mapped otu_clust_tool → otu_class_appr\n",
      "Mapped otu_db → otu_db\n",
      "Mapped otu_seq_comp_appr → otu_seq_comp_appr\n",
      "\n",
      "Final DNA derived extension shape: (312214, 29)\n",
      "Expected rows (should match occurrence core): 312214\n",
      "Row count match: True\n",
      "\n",
      "Column order verification:\n",
      " 1. eventID\n",
      " 2. source_mat_id\n",
      " 3. env_broad_scale\n",
      " 4. env_local_scale\n",
      " 5. env_medium\n",
      " 6. samp_vol_we_dna_ext\n",
      " 7. samp_collect_device\n",
      " 8. samp_mat_process\n",
      " 9. size_frac\n",
      "10. concentration\n",
      "11. lib_layout\n",
      "12. seq_meth\n",
      "13. nucl_acid_ext\n",
      "14. target_gene\n",
      "15. target_subfragment\n",
      "16. pcr_primer_forward\n",
      "17. pcr_primer_reverse\n",
      "18. pcr_primer_name_forward\n",
      "19. pcr_primer_name_reverse\n",
      "20. pcr_primer_reference\n",
      "21. pcr_cond\n",
      "22. nucl_acid_amp\n",
      "23. ampliconSize\n",
      "24. otu_seq_comp_appr\n",
      "25. otu_db\n",
      "26. occurrenceID\n",
      "27. DNA_sequence\n",
      "28. concentrationUnit\n",
      "29. otu_class_appr\n",
      "\n",
      "Data completeness summary:\n",
      "  eventID: 312,214 (100.0%)\n",
      "  source_mat_id: 312,214 (100.0%)\n",
      "  env_broad_scale: 312,214 (100.0%)\n",
      "  env_local_scale: 312,214 (100.0%)\n",
      "  env_medium: 312,214 (100.0%)\n",
      "  samp_vol_we_dna_ext: 312,214 (100.0%)\n",
      "  samp_collect_device: 312,214 (100.0%)\n",
      "  samp_mat_process: 312,214 (100.0%)\n",
      "  size_frac: 312,214 (100.0%)\n",
      "  concentration: 312,214 (100.0%)\n",
      "  lib_layout: 312,214 (100.0%)\n",
      "  seq_meth: 312,214 (100.0%)\n",
      "  nucl_acid_ext: 312,214 (100.0%)\n",
      "  target_gene: 312,214 (100.0%)\n",
      "  target_subfragment: 312,214 (100.0%)\n",
      "  pcr_primer_forward: 312,214 (100.0%)\n",
      "  pcr_primer_reverse: 312,214 (100.0%)\n",
      "  pcr_primer_name_forward: 312,214 (100.0%)\n",
      "  pcr_primer_name_reverse: 312,214 (100.0%)\n",
      "  pcr_primer_reference: 312,214 (100.0%)\n",
      "  pcr_cond: 312,214 (100.0%)\n",
      "  nucl_acid_amp: 165,131 (52.9%)\n",
      "  ampliconSize: 312,214 (100.0%)\n",
      "  otu_seq_comp_appr: 312,214 (100.0%)\n",
      "  otu_db: 312,214 (100.0%)\n",
      "  occurrenceID: 312,214 (100.0%)\n",
      "  DNA_sequence: 312,214 (100.0%)\n",
      "  concentrationUnit: 312,214 (100.0%)\n",
      "  otu_class_appr: 312,214 (100.0%)\n",
      "\n",
      "Sample of final DNA derived extension:\n",
      "               eventID         source_mat_id               env_broad_scale  \\\n",
      "0  GOMECC18S_Plate4_53  GOMECC4_27N_Sta1_DCM  marine biome [ENVO:00000447]   \n",
      "1  GOMECC18S_Plate4_53  GOMECC4_27N_Sta1_DCM  marine biome [ENVO:00000447]   \n",
      "2  GOMECC18S_Plate4_53  GOMECC4_27N_Sta1_DCM  marine biome [ENVO:00000447]   \n",
      "3  GOMECC18S_Plate4_53  GOMECC4_27N_Sta1_DCM  marine biome [ENVO:00000447]   \n",
      "4  GOMECC18S_Plate4_53  GOMECC4_27N_Sta1_DCM  marine biome [ENVO:00000447]   \n",
      "\n",
      "                      env_local_scale                 env_medium  \\\n",
      "0  marine photic zone [ENVO:00000209]  sea water [ENVO:00002149]   \n",
      "1  marine photic zone [ENVO:00000209]  sea water [ENVO:00002149]   \n",
      "2  marine photic zone [ENVO:00000209]  sea water [ENVO:00002149]   \n",
      "3  marine photic zone [ENVO:00000209]  sea water [ENVO:00002149]   \n",
      "4  marine photic zone [ENVO:00000209]  sea water [ENVO:00002149]   \n",
      "\n",
      "  samp_vol_we_dna_ext           samp_collect_device  \\\n",
      "0                1540  Niskin bottle on CTD rosette   \n",
      "1                1540  Niskin bottle on CTD rosette   \n",
      "2                1540  Niskin bottle on CTD rosette   \n",
      "3                1540  Niskin bottle on CTD rosette   \n",
      "4                1540  Niskin bottle on CTD rosette   \n",
      "\n",
      "                                                  samp_mat_process size_frac  \\\n",
      "0  Pumped through Sterivex filter (0.22-µm) using peristaltic pump      0.22   \n",
      "1  Pumped through Sterivex filter (0.22-µm) using peristaltic pump      0.22   \n",
      "2  Pumped through Sterivex filter (0.22-µm) using peristaltic pump      0.22   \n",
      "3  Pumped through Sterivex filter (0.22-µm) using peristaltic pump      0.22   \n",
      "4  Pumped through Sterivex filter (0.22-µm) using peristaltic pump      0.22   \n",
      "\n",
      "  concentration  lib_layout                      seq_meth  \\\n",
      "0          1.49  paired end  Illumina MiSeq [OBI_0002003]   \n",
      "1          1.49  paired end  Illumina MiSeq [OBI_0002003]   \n",
      "2          1.49  paired end  Illumina MiSeq [OBI_0002003]   \n",
      "3          1.49  paired end  Illumina MiSeq [OBI_0002003]   \n",
      "4          1.49  paired end  Illumina MiSeq [OBI_0002003]   \n",
      "\n",
      "                                                                             nucl_acid_ext  \\\n",
      "0  https://zenodo.org/records/14224755 (v1.0.0) protocol_extractdna_sterivex_kingfisher.md   \n",
      "1  https://zenodo.org/records/14224755 (v1.0.0) protocol_extractdna_sterivex_kingfisher.md   \n",
      "2  https://zenodo.org/records/14224755 (v1.0.0) protocol_extractdna_sterivex_kingfisher.md   \n",
      "3  https://zenodo.org/records/14224755 (v1.0.0) protocol_extractdna_sterivex_kingfisher.md   \n",
      "4  https://zenodo.org/records/14224755 (v1.0.0) protocol_extractdna_sterivex_kingfisher.md   \n",
      "\n",
      "                target_gene target_subfragment pcr_primer_forward  \\\n",
      "0  18S rRNA (SSU eukaryote)                 V9   GTACACACCGCCCGTC   \n",
      "1  18S rRNA (SSU eukaryote)                 V9   GTACACACCGCCCGTC   \n",
      "2  18S rRNA (SSU eukaryote)                 V9   GTACACACCGCCCGTC   \n",
      "3  18S rRNA (SSU eukaryote)                 V9   GTACACACCGCCCGTC   \n",
      "4  18S rRNA (SSU eukaryote)                 V9   GTACACACCGCCCGTC   \n",
      "\n",
      "         pcr_primer_reverse pcr_primer_name_forward pcr_primer_name_reverse  \\\n",
      "0  TGATCCTTCTGCAGGTTCACCTAC                   1391f                   EukBr   \n",
      "1  TGATCCTTCTGCAGGTTCACCTAC                   1391f                   EukBr   \n",
      "2  TGATCCTTCTGCAGGTTCACCTAC                   1391f                   EukBr   \n",
      "3  TGATCCTTCTGCAGGTTCACCTAC                   1391f                   EukBr   \n",
      "4  TGATCCTTCTGCAGGTTCACCTAC                   1391f                   EukBr   \n",
      "\n",
      "           pcr_primer_reference  \\\n",
      "0  10.1371/journal.pone.0006372   \n",
      "1  10.1371/journal.pone.0006372   \n",
      "2  10.1371/journal.pone.0006372   \n",
      "3  10.1371/journal.pone.0006372   \n",
      "4  10.1371/journal.pone.0006372   \n",
      "\n",
      "                                                                                                              pcr_cond  \\\n",
      "0  initial denaturation:94_3;denaturation:94_0.75;annealing:65_0.25;57_0.5;elongation:72_1.5;final elongation:72_10;35   \n",
      "1  initial denaturation:94_3;denaturation:94_0.75;annealing:65_0.25;57_0.5;elongation:72_1.5;final elongation:72_10;35   \n",
      "2  initial denaturation:94_3;denaturation:94_0.75;annealing:65_0.25;57_0.5;elongation:72_1.5;final elongation:72_10;35   \n",
      "3  initial denaturation:94_3;denaturation:94_0.75;annealing:65_0.25;57_0.5;elongation:72_1.5;final elongation:72_10;35   \n",
      "4  initial denaturation:94_3;denaturation:94_0.75;annealing:65_0.25;57_0.5;elongation:72_1.5;final elongation:72_10;35   \n",
      "\n",
      "  nucl_acid_amp  ampliconSize  \\\n",
      "0           NaN           260   \n",
      "1           NaN           260   \n",
      "2           NaN           260   \n",
      "3           NaN           260   \n",
      "4           NaN           260   \n",
      "\n",
      "                                            otu_seq_comp_appr  \\\n",
      "0  qiime2-2021.2; naive bayes classifier; scikit-learn 0.23.1   \n",
      "1  qiime2-2021.2; naive bayes classifier; scikit-learn 0.23.1   \n",
      "2  qiime2-2021.2; naive bayes classifier; scikit-learn 0.23.1   \n",
      "3  qiime2-2021.2; naive bayes classifier; scikit-learn 0.23.1   \n",
      "4  qiime2-2021.2; naive bayes classifier; scikit-learn 0.23.1   \n",
      "\n",
      "                                                      otu_db  \\\n",
      "0  PR2 v5.0.1; V9 1391f-1510r region; 10.5281/zenodo.8392706   \n",
      "1  PR2 v5.0.1; V9 1391f-1510r region; 10.5281/zenodo.8392706   \n",
      "2  PR2 v5.0.1; V9 1391f-1510r region; 10.5281/zenodo.8392706   \n",
      "3  PR2 v5.0.1; V9 1391f-1510r region; 10.5281/zenodo.8392706   \n",
      "4  PR2 v5.0.1; V9 1391f-1510r region; 10.5281/zenodo.8392706   \n",
      "\n",
      "                                               occurrenceID  \\\n",
      "0  GOMECC18S_Plate4_53_occ_36aa75f9b28f5f831c2d631ba65c2bcb   \n",
      "1  GOMECC18S_Plate4_53_occ_4e38e8ced9070952b314e1880bede1ca   \n",
      "2  GOMECC18S_Plate4_53_occ_2a31e5c01634165da99e7381279baa75   \n",
      "3  GOMECC18S_Plate4_53_occ_ecee60339b2fb88ea6d1c8d18054bed4   \n",
      "4  GOMECC18S_Plate4_53_occ_fa1f1a97dd4ae7c826009186bad26384   \n",
      "\n",
      "                                                                                                                       DNA_sequence  \\\n",
      "0   GCTACTACCGATTGAACGTTTTAGTGAGGTCCTCGGACTGTTTGCCTGGCGGATTACTCTGCCTGGCTGGCGGGAAGACGACCAAACTGTAGCGTTTAGAGGAAGTAAAAGTCGTAACAAGGTTTCC   \n",
      "1   GCTACTACCGATTGAACGTTTTAGTGAGGTCCTCGGACTGTTTGGTAGTCGGATCACTCTGACTGCCTGGCGGGAAGACGACCAAACTGTAGCGTTTAGAGGAAGTAAAAGTCGTAACAAGGTTTCC   \n",
      "2  GCTACTACCGATTGGACGTTTTAGTGAGACATTTGGACTGGGTTAAGATAGTCGCAAGACTACCTTTTCTCCGGAAAGACTTTCAAACTTGAGCGTCTAGAGGAAGTAAAAGTCGTAACAAGGTTTCC   \n",
      "3    GCTCCTACCGATTGAGTGATCCGGTGAATAATTCGGACTGCAGCAGTGTTCAGTTCCTGAACGTTGCAGCGGAAAGTTTAGTGAACCTTATCACTTAGAGGAAGGAGAAGTCGTAACAAGGTTTCC   \n",
      "4    GCTCCTACCGATTGAGTGATCCGGTGAATAATTCGGACTGCAGCAATGTTTGGATCCCGAACGTTGCAGCGGAAAGTTTAGTGAACCTTATCACTTAGAGGAAGGAGAAGTCGTAACAAGGTTTCC   \n",
      "\n",
      "  concentrationUnit                    otu_class_appr  \n",
      "0             ng/µl  Tourmaline; qiime2-2021.2; dada2  \n",
      "1             ng/µl  Tourmaline; qiime2-2021.2; dada2  \n",
      "2             ng/µl  Tourmaline; qiime2-2021.2; dada2  \n",
      "3             ng/µl  Tourmaline; qiime2-2021.2; dada2  \n",
      "4             ng/µl  Tourmaline; qiime2-2021.2; dada2  \n"
     ]
    }
   ],
   "source": [
    "# Apply Darwin Core field mappings and create final output\n",
    "print(\"Applying Darwin Core mappings and creating final DNA derived extension...\")\n",
    "\n",
    "# The dwc_data is a dictionary containing DataFrames\n",
    "dna_derived_mapping = dwc_data['dnaDerived']\n",
    "print(f\"Available Darwin Core mappings for dnaDerived: {len(dna_derived_mapping)}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# Create mapping dictionary from FAIRe names to Darwin Core names\n",
    "# Note: Darwin Core terms are the INDEX, FAIRe terms are in 'FAIRe_term' column\n",
    "field_mapping = {}\n",
    "for dwc_name, row in dna_derived_mapping.iterrows():\n",
    "    faire_name = row['FAIRe_term']  # Changed from 'FAIRe_name' to 'FAIRe_term'\n",
    "    if pd.notna(faire_name) and pd.notna(dwc_name):\n",
    "        field_mapping[faire_name] = dwc_name\n",
    "\n",
    "print(f\"Field mappings to apply: {len(field_mapping)}\")\n",
    "\n",
    "# Apply the mappings\n",
    "dna_derived_df_final = dna_derived_with_sequences.copy()\n",
    "for faire_name, dwc_name in field_mapping.items():\n",
    "    if faire_name in dna_derived_df_final.columns:\n",
    "        dna_derived_df_final = dna_derived_df_final.rename(columns={faire_name: dwc_name})\n",
    "        print(f\"Mapped {faire_name} → {dwc_name}\")\n",
    "\n",
    "# Ensure we have all required columns in the correct order\n",
    "final_columns = []\n",
    "for col in DESIRED_DNA_DERIVED_COLUMNS:\n",
    "    if col in dna_derived_df_final.columns:\n",
    "        final_columns.append(col)\n",
    "    else:\n",
    "        print(f\"WARNING: Missing required column: {col}\")\n",
    "        dna_derived_df_final[col] = pd.NA\n",
    "        final_columns.append(col)\n",
    "\n",
    "# Select only the required columns in the correct order\n",
    "dna_derived_extension = dna_derived_df_final[final_columns]\n",
    "\n",
    "print(f\"\\nFinal DNA derived extension shape: {dna_derived_extension.shape}\")\n",
    "print(f\"Expected rows (should match occurrence core): {len(occ_all_final_output)}\")\n",
    "print(f\"Row count match: {len(dna_derived_extension) == len(occ_all_final_output)}\")\n",
    "\n",
    "print(f\"\\nColumn order verification:\")\n",
    "for i, col in enumerate(dna_derived_extension.columns):\n",
    "    print(f\"{i+1:2d}. {col}\")\n",
    "\n",
    "print(f\"\\nData completeness summary:\")\n",
    "for col in dna_derived_extension.columns:\n",
    "    non_null = dna_derived_extension[col].notna().sum()\n",
    "    pct = (non_null / len(dna_derived_extension)) * 100\n",
    "    print(f\"  {col}: {non_null:,} ({pct:.1f}%)\")\n",
    "\n",
    "# Display sample of final data\n",
    "print(f\"\\nSample of final DNA derived extension:\")\n",
    "print(dna_derived_extension.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save DNA Derived Extension to a CSV. You are now ready to submit to OBIS/GBIF!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving DNA derived extension to CSV...\n",
      "DNA derived extension saved to: ../processed-v3/dna_derived_extension.csv\n",
      "File contains 312214 rows and 29 columns\n",
      "File size: 329.31 MB\n",
      "✓ DNA derived extension file successfully created!\n"
     ]
    }
   ],
   "source": [
    "# Save DNA derived extension to CSV file\n",
    "print(\"Saving DNA derived extension to CSV...\")\n",
    "\n",
    "# Save to the processed directory (same as occurrence.csv)\n",
    "output_path = \"../processed-v3/dna_derived_extension.csv\"\n",
    "dna_derived_extension.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"DNA derived extension saved to: {output_path}\")\n",
    "print(f\"File contains {len(dna_derived_extension)} rows and {len(dna_derived_extension.columns)} columns\")\n",
    "\n",
    "# Verify the file was created\n",
    "import os\n",
    "if os.path.exists(output_path):\n",
    "    file_size = os.path.getsize(output_path) / (1024*1024)  # Size in MB\n",
    "    print(f\"File size: {file_size:.2f} MB\")\n",
    "    print(\"✓ DNA derived extension file successfully created!\")\n",
    "else:\n",
    "    print(\"❌ Error: File was not created\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edna2obis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
